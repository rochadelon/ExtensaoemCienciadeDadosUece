{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3d4f5b0",
   "metadata": {},
   "source": [
    "# An√°lise de Artigos Cient√≠ficos com Gemini AI\n",
    "\n",
    "## Escopo: Nanorevestimentos e Tintas (Nanocoatings and Paints)\n",
    "\n",
    "Este notebook realiza an√°lise automatizada de artigos cient√≠ficos para identificar trabalhos relevantes ao escopo de nanorevestimentos e tintas usando a API do Google Gemini.\n",
    "\n",
    "### Funcionalidades:\n",
    "1. **Instala√ß√£o de depend√™ncias**\n",
    "2. **Configura√ß√£o da API Gemini**\n",
    "3. **Defini√ß√£o de palavras-chave espec√≠ficas**\n",
    "4. **Busca e filtro de artigos**\n",
    "5. **An√°lise individual de abstracts**\n",
    "6. **Processamento em lote com rate limiting**\n",
    "7. **Sistema de checkpoint para recupera√ß√£o**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a61dabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-generativeai in /home/delon/miniconda3/envs/env_del/lib/python3.12/site-packages (0.8.5)\n",
      "Requirement already satisfied: pandas in /home/delon/miniconda3/envs/env_del/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /home/delon/miniconda3/envs/env_del/lib/python3.12/site-packages (from google-generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in /home/delon/miniconda3/envs/env_del/lib/python3.12/site-packages (from google-generativeai) (2.25.0rc1)\n",
      "Requirement already satisfied: google-api-python-client in /home/delon/miniconda3/envs/env_del/lib/python3.12/site-packages (from google-generativeai) (2.170.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in /home/delon/miniconda3/envs/env_del/lib/python3.12/site-packages (from google-generativeai) (2.40.2)\n",
      "Requirement already satisfied: protobuf in /home/delon/miniconda3/envs/env_del/lib/python3.12/site-packages (from google-generativeai) (5.29.4)\n",
      "Requirement already satisfied: pydantic in /home/delon/miniconda3/envs/env_del/lib/python3.12/site-packages (from google-generativeai) (2.11.4)\n",
      "Requirement already satisfied: tqdm in /home/delon/miniconda3/envs/env_del/lib/python3.12/site-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in /home/delon/miniconda3/envs/env_del/lib/python3.12/site-packages (from google-generativeai) (4.13.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /home/delon/miniconda3/envs/env_del/lib/python3.12/site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/delon/miniconda3/envs/env_del/lib/python3.12/site-packages (from pandas) (1.26.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/delon/miniconda3/envs/env_del/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/delon/miniconda3/envs/env_del/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/delon/miniconda3/envs/env_del/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /home/delon/miniconda3/envs/env_del/lib/python3.12/site-packages (from google-api-core->google-generativeai) (1.70.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /home/delon/miniconda3/envs/env_del/lib/python3.12/site-packages (from google-api-core->google-generativeai) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/delon/miniconda3/envs/env_del/lib/python3.12/site-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/delon/miniconda3/envs/env_del/lib/python3.12/site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/delon/miniconda3/envs/env_del/lib/python3.12/site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/delon/miniconda3/envs/env_del/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /home/delon/miniconda3/envs/env_del/lib/python3.12/site-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /home/delon/miniconda3/envs/env_del/lib/python3.12/site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /home/delon/miniconda3/envs/env_del/lib/python3.12/site-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/delon/miniconda3/envs/env_del/lib/python3.12/site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/delon/miniconda3/envs/env_del/lib/python3.12/site-packages (from pydantic->google-generativeai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/delon/miniconda3/envs/env_del/lib/python3.12/site-packages (from pydantic->google-generativeai) (0.4.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /home/delon/miniconda3/envs/env_del/lib/python3.12/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /home/delon/miniconda3/envs/env_del/lib/python3.12/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /home/delon/miniconda3/envs/env_del/lib/python3.12/site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /home/delon/miniconda3/envs/env_del/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/delon/miniconda3/envs/env_del/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/delon/miniconda3/envs/env_del/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/delon/miniconda3/envs/env_del/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/delon/miniconda3/envs/env_del/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.4.26)\n"
     ]
    }
   ],
   "source": [
    "# Instala√ß√£o das depend√™ncias necess√°rias\n",
    "!pip install google-generativeai pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9706301d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîê Carregando configura√ß√µes da API...\n",
      "‚ùå Arquivo 'api_key.txt' n√£o encontrado!\n",
      "üí° Crie o arquivo 'api_key.txt' e coloque sua API key nele\n",
      "‚ùå N√£o foi poss√≠vel configurar a API\n",
      "\n",
      "üìã Para configurar a API:\n",
      "1. Crie um arquivo chamado 'api_key.txt' no mesmo diret√≥rio deste notebook\n",
      "2. Cole sua API key do Google Gemini no arquivo (apenas a key, sem aspas)\n",
      "3. Salve o arquivo e execute novamente esta c√©lula\n",
      "\n",
      "üîó Como obter uma API key:\n",
      "   Acesse: https://makersuite.google.com/app/apikey\n",
      "\n",
      "‚úÖ Depend√™ncias carregadas\n"
     ]
    }
   ],
   "source": [
    "# Imports necess√°rios\n",
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "from typing import List, Optional\n",
    "\n",
    "# Fun√ß√£o para carregar API key do arquivo\n",
    "def carregar_api_key(arquivo_api='api_key.txt'):\n",
    "    \"\"\"\n",
    "    Carrega a API key do arquivo especificado\n",
    "    \n",
    "    Args:\n",
    "        arquivo_api: Caminho para o arquivo contendo a API key\n",
    "    \n",
    "    Returns:\n",
    "        String com a API key ou None se houver erro\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Verificar se o arquivo existe\n",
    "        if not os.path.exists(arquivo_api):\n",
    "            print(f\"‚ùå Arquivo '{arquivo_api}' n√£o encontrado!\")\n",
    "            print(f\"üí° Crie o arquivo '{arquivo_api}' e coloque sua API key nele\")\n",
    "            return None\n",
    "        \n",
    "        # Ler a API key do arquivo\n",
    "        with open(arquivo_api, 'r', encoding='utf-8') as file:\n",
    "            api_key = file.read().strip()\n",
    "        \n",
    "        # Verificar se a API key n√£o est√° vazia\n",
    "        if not api_key:\n",
    "            print(f\"‚ùå Arquivo '{arquivo_api}' est√° vazio!\")\n",
    "            return None\n",
    "        \n",
    "        # Verificar formato b√°sico da API key (deve come√ßar com 'AIza')\n",
    "        if not api_key.startswith('AIza'):\n",
    "            print(f\"‚ö†Ô∏è API key pode estar no formato incorreto\")\n",
    "            print(f\"   APIs do Google Gemini geralmente come√ßam com 'AIza'\")\n",
    "        \n",
    "        print(f\"‚úÖ API key carregada com sucesso de '{arquivo_api}'\")\n",
    "        print(f\"üîë API key: {api_key[:10]}...{api_key[-4:]} (ocultada por seguran√ßa)\")\n",
    "        \n",
    "        return api_key\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Arquivo '{arquivo_api}' n√£o encontrado!\")\n",
    "        print(f\"üí° Crie o arquivo '{arquivo_api}' e coloque sua API key nele\")\n",
    "        return None\n",
    "    except PermissionError:\n",
    "        print(f\"‚ùå Sem permiss√£o para ler o arquivo '{arquivo_api}'\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao carregar API key: {e}\")\n",
    "        return None\n",
    "\n",
    "# Carregar e configurar a API do Gemini\n",
    "print(\"üîê Carregando configura√ß√µes da API...\")\n",
    "\n",
    "api_key = carregar_api_key('api_key.txt')\n",
    "\n",
    "if api_key:\n",
    "    # Configurar a API do Gemini\n",
    "    genai.configure(api_key=api_key)\n",
    "    print(\"‚úÖ API do Gemini configurada com sucesso\")\n",
    "    \n",
    "    # Testar a conex√£o (opcional)\n",
    "    try:\n",
    "        # Fazer um teste simples para verificar se a API est√° funcionando\n",
    "        model = genai.GenerativeModel('gemini-2.0-flash-exp')\n",
    "        test_response = model.generate_content(\"Hello\")\n",
    "        print(\"üîó Conex√£o com a API testada com sucesso\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erro ao testar a API: {e}\")\n",
    "        print(\"   A API key pode estar inv√°lida ou sem cr√©ditos\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå N√£o foi poss√≠vel configurar a API\")\n",
    "    print(\"\\nüìã Para configurar a API:\")\n",
    "    print(\"1. Crie um arquivo chamado 'api_key.txt' no mesmo diret√≥rio deste notebook\")\n",
    "    print(\"2. Cole sua API key do Google Gemini no arquivo (apenas a key, sem aspas)\")\n",
    "    print(\"3. Salve o arquivo e execute novamente esta c√©lula\")\n",
    "    print(\"\\nüîó Como obter uma API key:\")\n",
    "    print(\"   Acesse: https://makersuite.google.com/app/apikey\")\n",
    "\n",
    "print(\"\\n‚úÖ Depend√™ncias carregadas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4279c16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Arquivo .gitignore criado/atualizado com sucesso\n",
      "üîí Seus arquivos sens√≠veis est√£o protegidos do versionamento\n",
      "\n",
      "üìù API key n√£o encontrada. Criando arquivo de exemplo...\n",
      "üìù Arquivo 'api_key_exemplo.txt' criado\n",
      "   Renomeie para 'api_key.txt' e adicione sua key real\n"
     ]
    }
   ],
   "source": [
    "# Criar arquivo .gitignore para proteger a API key\n",
    "def criar_gitignore():\n",
    "    \"\"\"\n",
    "    Cria ou atualiza arquivo .gitignore para proteger arquivos sens√≠veis\n",
    "    \"\"\"\n",
    "    gitignore_content = \"\"\"# API Keys e arquivos sens√≠veis\n",
    "api_key.txt\n",
    "*.key\n",
    ".env\n",
    "\n",
    "# Arquivos de checkpoint\n",
    "*checkpoint*.csv\n",
    "\n",
    "# Arquivos tempor√°rios\n",
    "*.tmp\n",
    "*.temp\n",
    "\n",
    "# Resultados de an√°lises (opcional - remova se quiser versionar)\n",
    "*_resultado.csv\n",
    "*_analise.csv\n",
    "nanomateriais_identificados*.csv\n",
    "\n",
    "# Jupyter Notebook checkpoints\n",
    ".ipynb_checkpoints/\n",
    "\n",
    "# Python cache\n",
    "__pycache__/\n",
    "*.pyc\n",
    "*.pyo\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        with open('.gitignore', 'w', encoding='utf-8') as file:\n",
    "            file.write(gitignore_content)\n",
    "        print(\"‚úÖ Arquivo .gitignore criado/atualizado com sucesso\")\n",
    "        print(\"üîí Seus arquivos sens√≠veis est√£o protegidos do versionamento\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erro ao criar .gitignore: {e}\")\n",
    "\n",
    "# Executar a cria√ß√£o do .gitignore\n",
    "criar_gitignore()\n",
    "\n",
    "# Fun√ß√£o auxiliar para criar arquivo de exemplo da API key\n",
    "def criar_exemplo_api_key():\n",
    "    \"\"\"\n",
    "    Cria um arquivo de exemplo mostrando como configurar a API key\n",
    "    \"\"\"\n",
    "    exemplo_content = \"\"\"AIzaSyA_exemplo_da_sua_api_key_aqui_substitua_por_sua_key_real\n",
    "\n",
    "INSTRU√á√ïES:\n",
    "1. Substitua o texto acima pela sua API key real do Google Gemini\n",
    "2. Mantenha apenas a API key (sem aspas, sem espa√ßos extras)\n",
    "3. Salve este arquivo como 'api_key.txt'\n",
    "4. Delete esta linha de instru√ß√µes\n",
    "\n",
    "Como obter sua API key:\n",
    "- Acesse: https://makersuite.google.com/app/apikey\n",
    "- Fa√ßa login com sua conta Google\n",
    "- Clique em \"Create API Key\"\n",
    "- Copie a key gerada e cole aqui\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        if not os.path.exists('api_key.txt'):\n",
    "            with open('api_key_exemplo.txt', 'w', encoding='utf-8') as file:\n",
    "                file.write(exemplo_content)\n",
    "            print(\"üìù Arquivo 'api_key_exemplo.txt' criado\")\n",
    "            print(\"   Renomeie para 'api_key.txt' e adicione sua key real\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erro ao criar exemplo: {e}\")\n",
    "\n",
    "# Criar exemplo se n√£o existir api_key.txt\n",
    "if not os.path.exists('api_key.txt'):\n",
    "    print(\"\\nüìù API key n√£o encontrada. Criando arquivo de exemplo...\")\n",
    "    criar_exemplo_api_key()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2bb9a4",
   "metadata": {},
   "source": [
    "## 1. Configura√ß√£o de Par√¢metros\n",
    "\n",
    "Defini√ß√£o dos par√¢metros principais para an√°lise e rate limiting da API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8338a29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Configura√ß√µes carregadas:\n",
      "  modelo_gemini: gemini-2.0-flash-exp\n",
      "  max_requests_per_minute: 20\n",
      "  max_requests_per_day: 1000\n",
      "  delay_between_requests: 5\n",
      "  arquivo_entrada: /home/delon/Modelos/cenanoink/Projeto-CENanoInk/Alan Delon.csv\n",
      "  checkpoint_file: analise_escopo_checkpoint.csv\n",
      "  arquivo_saida: df_com_analise_escopo_completo.csv\n"
     ]
    }
   ],
   "source": [
    "# Configura√ß√µes principais\n",
    "CONFIG = {\n",
    "    'modelo_gemini': 'gemini-2.0-flash-exp',  # Flash-Lite para rate limits melhores\n",
    "    'max_requests_per_minute': 20,              # Limite de requisi√ß√µes por minuto\n",
    "    'max_requests_per_day': 1000,               # Limite di√°rio recomendado\n",
    "    'delay_between_requests': 5,              # Delay entre requisi√ß√µes (segundos)\n",
    "    'arquivo_entrada': '/home/delon/Modelos/cenanoink/Projeto-CENanoInk/Alan Delon.csv',\n",
    "    'checkpoint_file': 'analise_escopo_checkpoint.csv',\n",
    "    'arquivo_saida': 'df_com_analise_escopo_completo.csv'\n",
    "}\n",
    "\n",
    "print(\"üìã Configura√ß√µes carregadas:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ae5ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ VERS√ÉO OTIMIZADA - IDENTIFICA√á√ÉO DE NANOMATERIAIS\n",
      "============================================================\n",
      "‚úÖ 525 artigos carregados\n",
      "üìÑ 525 abstracts dispon√≠veis\n",
      "\n",
      "Op√ß√µes:\n",
      "1. Apenas busca r√°pida (regex) - Recomendado\n",
      "2. Busca r√°pida + Gemini para casos n√£o identificados\n",
      "3. Teste com 50 abstracts\n",
      "\n",
      "üöÄ PROCESSAMENTO COMPLETO (s√≥ regex)\n",
      "üöÄ Processando 525 abstracts (modo otimizado)\n",
      "üìç Fase 1: Busca r√°pida por palavras-chave...\n",
      "  Processado: 100/525\n",
      "  Processado: 200/525\n",
      "  Processado: 300/525\n",
      "  Processado: 400/525\n",
      "  Processado: 500/525\n",
      "‚úÖ Fase 1 conclu√≠da em 0.7s\n",
      "üíæ Salvo em: nanomateriais_identificados_rapido.csv\n",
      "\n",
      "üìä ESTAT√çSTICAS R√ÅPIDAS\n",
      "Total de artigos: 525\n",
      "Artigos com nanomateriais: 179\n",
      "Materiais √∫nicos identificados: 39\n",
      "\n",
      "üèÜ Top 5 nanomateriais:\n",
      "  1. TiO2: 42 men√ß√µes\n",
      "  2. ZnO: 25 men√ß√µes\n",
      "  3. Al2O3: 24 men√ß√µes\n",
      "  4. carbon nanotube: 20 men√ß√µes\n",
      "  5. alumina: 19 men√ß√µes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# VERS√ÉO OTIMIZADA - Identifica√ß√£o de Nanomateriais\n",
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "import time\n",
    "import re\n",
    "from collections import Counter\n",
    "from typing import Set, Dict, List\n",
    "\n",
    "# Configura√ß√£o otimizada\n",
    "CONFIG_OTIMIZADO = {\n",
    "    'modelo_gemini': 'gemini-2.0-flash-exp',\n",
    "    'max_requests_per_minute': 25,\n",
    "    'delay_between_requests': 2.5,\n",
    "    'batch_size': 50,  # Processar em lotes maiores\n",
    "    'verbose': False   # Reduzir verbosidade\n",
    "}\n",
    "\n",
    "# Nanomateriais organizados e otimizados\n",
    "NANOMATERIAIS_OTIMIZADO = {\n",
    "    'oxidos_metalicos': [\n",
    "        'TiO2', 'titanium dioxide', 'ZnO', 'zinc oxide',\n",
    "        'Al2O3', 'alumina', 'SiO2', 'silica', 'Fe2O3', 'iron oxide',\n",
    "        'CeO2', 'cerium oxide', 'SnO2', 'tin oxide'\n",
    "    ],\n",
    "    'nanoparticulas_metalicas': [\n",
    "        'silver nanoparticle', 'AgNP', 'gold nanoparticle', 'AuNP',\n",
    "        'copper nanoparticle', 'CuNP', 'iron nanoparticle', 'FeNP'\n",
    "    ],\n",
    "    'nanomateriais_carbono': [\n",
    "        'carbon nanotube', 'CNT', 'graphene', 'graphene oxide',\n",
    "        'fullerene', 'carbon black', 'carbon nanofiber'\n",
    "    ],\n",
    "    'outros': [\n",
    "        'quantum dot', 'nanoclay', 'nanocellulose', 'polymer nanocomposite'\n",
    "    ]\n",
    "}\n",
    "\n",
    "def criar_patterns_regex() -> Dict[str, re.Pattern]:\n",
    "    \"\"\"Cria patterns regex otimizados para busca r√°pida\"\"\"\n",
    "    patterns = {}\n",
    "    for categoria, materiais in NANOMATERIAIS_OTIMIZADO.items():\n",
    "        # Criar pattern regex case-insensitive\n",
    "        pattern_str = '|'.join(re.escape(mat) for mat in materiais)\n",
    "        patterns[categoria] = re.compile(f'({pattern_str})', re.IGNORECASE)\n",
    "    return patterns\n",
    "\n",
    "def identificar_nanomateriais_rapido(abstract_text: str, patterns: Dict[str, re.Pattern]) -> Set[str]:\n",
    "    \"\"\"Identifica√ß√£o r√°pida usando regex - sem Gemini\"\"\"\n",
    "    if pd.isna(abstract_text) or not str(abstract_text).strip():\n",
    "        return set()\n",
    "    \n",
    "    materiais_encontrados = set()\n",
    "    text = str(abstract_text)\n",
    "    \n",
    "    for categoria, pattern in patterns.items():\n",
    "        matches = pattern.findall(text)\n",
    "        materiais_encontrados.update(matches)\n",
    "    \n",
    "    return materiais_encontrados\n",
    "\n",
    "def identificar_nanomateriais_gemini_batch(abstracts: List[str]) -> List[str]:\n",
    "    \"\"\"Processamento em lote com Gemini para maior efici√™ncia\"\"\"\n",
    "    if not abstracts:\n",
    "        return []\n",
    "    \n",
    "    # Combinar m√∫ltiplos abstracts em uma √∫nica requisi√ß√£o\n",
    "    combined_text = \"\\n\\n---ABSTRACT---\\n\\n\".join(abstracts[:5])  # M√°ximo 5 por requisi√ß√£o\n",
    "    \n",
    "    model = genai.GenerativeModel(CONFIG_OTIMIZADO['modelo_gemini'])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Analise os abstracts cient√≠ficos separados por \"---ABSTRACT---\" e identifique nanomateriais em cada um.\n",
    "    \n",
    "    {combined_text}\n",
    "    \n",
    "    Para cada abstract, liste os nanomateriais encontrados ou \"NENHUM\" se n√£o houver.\n",
    "    Formato de resposta:\n",
    "    ABSTRACT 1: material1, material2\n",
    "    ABSTRACT 2: NENHUM\n",
    "    ABSTRACT 3: material3\n",
    "    \n",
    "    Use nomenclatura padr√£o (TiO2, ZnO, graphene, etc.)\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        return processar_resposta_batch(response.text, len(abstracts[:5]))\n",
    "    except Exception as e:\n",
    "        print(f\"Erro Gemini batch: {e}\")\n",
    "        return [\"Erro\"] * len(abstracts[:5])\n",
    "\n",
    "def processar_resposta_batch(response_text: str, num_abstracts: int) -> List[str]:\n",
    "    \"\"\"Processa resposta em lote do Gemini\"\"\"\n",
    "    linhas = response_text.strip().split('\\n')\n",
    "    resultados = []\n",
    "    \n",
    "    for i in range(num_abstracts):\n",
    "        encontrado = False\n",
    "        for linha in linhas:\n",
    "            if f\"ABSTRACT {i+1}:\" in linha:\n",
    "                material_part = linha.split(':', 1)[1].strip()\n",
    "                if material_part.upper() == \"NENHUM\":\n",
    "                    resultados.append(\"Nenhum nanomaterial identificado\")\n",
    "                else:\n",
    "                    resultados.append(material_part)\n",
    "                encontrado = True\n",
    "                break\n",
    "        \n",
    "        if not encontrado:\n",
    "            resultados.append(\"Nenhum nanomaterial identificado\")\n",
    "    \n",
    "    return resultados\n",
    "\n",
    "def processar_dataframe_otimizado(df: pd.DataFrame, \n",
    "                                use_gemini: bool = False,\n",
    "                                max_linhas: int = None) -> pd.DataFrame:\n",
    "    \"\"\"Vers√£o otimizada do processamento\"\"\"\n",
    "    \n",
    "    if 'Abstract' not in df.columns:\n",
    "        print(\"‚ùå Coluna 'Abstract' n√£o encontrada\")\n",
    "        return df\n",
    "    \n",
    "    # Limitar processamento se especificado\n",
    "    if max_linhas:\n",
    "        df_work = df.head(max_linhas).copy()\n",
    "    else:\n",
    "        df_work = df.copy()\n",
    "    \n",
    "    print(f\"üöÄ Processando {len(df_work)} abstracts (modo otimizado)\")\n",
    "    \n",
    "    # Criar patterns regex uma vez\n",
    "    patterns = criar_patterns_regex()\n",
    "    \n",
    "    # Inicializar coluna\n",
    "    df_work['Nanomaterial Citado'] = ''\n",
    "    \n",
    "    # FASE 1: Busca r√°pida com regex (todos os abstracts)\n",
    "    print(\"üìç Fase 1: Busca r√°pida por palavras-chave...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for idx in range(len(df_work)):\n",
    "        abstract = df_work.iloc[idx]['Abstract']\n",
    "        materiais = identificar_nanomateriais_rapido(abstract, patterns)\n",
    "        \n",
    "        if materiais:\n",
    "            resultado = ', '.join(sorted(materiais))\n",
    "        else:\n",
    "            resultado = \"Nenhum nanomaterial identificado\"\n",
    "        \n",
    "        df_work.iloc[idx, df_work.columns.get_loc('Nanomaterial Citado')] = resultado\n",
    "        \n",
    "        # Progresso a cada 100 linhas\n",
    "        if (idx + 1) % 100 == 0:\n",
    "            print(f\"  Processado: {idx + 1}/{len(df_work)}\")\n",
    "    \n",
    "    fase1_time = time.time() - start_time\n",
    "    print(f\"‚úÖ Fase 1 conclu√≠da em {fase1_time:.1f}s\")\n",
    "    \n",
    "    # FASE 2: Gemini apenas para abstracts sem nanomateriais (opcional)\n",
    "    if use_gemini:\n",
    "        abstracts_sem_materiais = df_work[\n",
    "            df_work['Nanomaterial Citado'] == \"Nenhum nanomaterial identificado\"\n",
    "        ]\n",
    "        \n",
    "        if len(abstracts_sem_materiais) > 0:\n",
    "            print(f\"üìç Fase 2: An√°lise Gemini para {len(abstracts_sem_materiais)} abstracts sem materiais...\")\n",
    "            \n",
    "            abstracts_list = abstracts_sem_materiais['Abstract'].tolist()\n",
    "            indices_list = abstracts_sem_materiais.index.tolist()\n",
    "            \n",
    "            # Processar em lotes\n",
    "            for i in range(0, len(abstracts_list), 5):\n",
    "                batch_abstracts = abstracts_list[i:i+5]\n",
    "                batch_indices = indices_list[i:i+5]\n",
    "                \n",
    "                resultados = identificar_nanomateriais_gemini_batch(batch_abstracts)\n",
    "                \n",
    "                # Atualizar DataFrame\n",
    "                for idx, resultado in zip(batch_indices, resultados):\n",
    "                    df_work.iloc[\n",
    "                        df_work.index.get_loc(idx), \n",
    "                        df_work.columns.get_loc('Nanomaterial Citado')\n",
    "                    ] = resultado\n",
    "                \n",
    "                # Rate limiting\n",
    "                time.sleep(CONFIG_OTIMIZADO['delay_between_requests'])\n",
    "                \n",
    "                if (i + 5) % 25 == 0:\n",
    "                    print(f\"  Gemini: {min(i + 5, len(abstracts_list))}/{len(abstracts_list)}\")\n",
    "    \n",
    "    return df_work\n",
    "\n",
    "def gerar_estatisticas_rapidas(df: pd.DataFrame) -> None:\n",
    "    \"\"\"Gera estat√≠sticas de forma otimizada\"\"\"\n",
    "    if 'Nanomaterial Citado' not in df.columns:\n",
    "        print(\"‚ùå Coluna n√£o encontrada\")\n",
    "        return\n",
    "    \n",
    "    # Contar materiais\n",
    "    todos_materiais = []\n",
    "    for materiais_str in df['Nanomaterial Citado'].dropna():\n",
    "        if materiais_str and \"nenhum\" not in materiais_str.lower() and \"erro\" not in materiais_str.lower():\n",
    "            materiais = [m.strip() for m in materiais_str.split(',')]\n",
    "            todos_materiais.extend(materiais)\n",
    "    \n",
    "    contador = Counter(todos_materiais)\n",
    "    \n",
    "    print(f\"\\nüìä ESTAT√çSTICAS R√ÅPIDAS\")\n",
    "    print(f\"Total de artigos: {len(df):,}\")\n",
    "    print(f\"Artigos com nanomateriais: {len(df[df['Nanomaterial Citado'] != 'Nenhum nanomaterial identificado']):,}\")\n",
    "    print(f\"Materiais √∫nicos identificados: {len(contador)}\")\n",
    "    \n",
    "    if contador:\n",
    "        print(f\"\\nüèÜ Top 5 nanomateriais:\")\n",
    "        for i, (material, count) in enumerate(contador.most_common(5), 1):\n",
    "            print(f\"  {i}. {material}: {count} men√ß√µes\")\n",
    "\n",
    "# EXECU√á√ÉO OTIMIZADA\n",
    "print(\"üöÄ VERS√ÉO OTIMIZADA - IDENTIFICA√á√ÉO DE NANOMATERIAIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Carregar dados\n",
    "try:\n",
    "    df = pd.read_csv(CONFIG['arquivo_entrada'])\n",
    "    print(f\"‚úÖ {len(df):,} artigos carregados\")\n",
    "    \n",
    "    if 'Abstract' in df.columns:\n",
    "        abstracts_validos = df['Abstract'].notna().sum()\n",
    "        print(f\"üìÑ {abstracts_validos:,} abstracts dispon√≠veis\")\n",
    "        \n",
    "        # Op√ß√µes de processamento\n",
    "        print(\"\\nOp√ß√µes:\")\n",
    "        print(\"1. Apenas busca r√°pida (regex) - Recomendado\")\n",
    "        print(\"2. Busca r√°pida + Gemini para casos n√£o identificados\")\n",
    "        print(\"3. Teste com 50 abstracts\")\n",
    "        \n",
    "        opcao = input(\"Escolha (1/2/3): \")\n",
    "        \n",
    "        if opcao == \"3\":\n",
    "            # Teste r√°pido\n",
    "            print(\"\\nüß™ TESTE R√ÅPIDO (50 abstracts)\")\n",
    "            df_result = processar_dataframe_otimizado(df, use_gemini=False, max_linhas=50)\n",
    "            gerar_estatisticas_rapidas(df_result)\n",
    "            \n",
    "        elif opcao == \"1\":\n",
    "            # Processamento r√°pido completo\n",
    "            print(\"\\nüöÄ PROCESSAMENTO COMPLETO (s√≥ regex)\")\n",
    "            df_result = processar_dataframe_otimizado(df, use_gemini=False)\n",
    "            \n",
    "            # Salvar\n",
    "            arquivo_saida = 'nanomateriais_identificados_rapido.csv'\n",
    "            df_result.to_csv(arquivo_saida, index=False)\n",
    "            print(f\"üíæ Salvo em: {arquivo_saida}\")\n",
    "            \n",
    "            gerar_estatisticas_rapidas(df_result)\n",
    "            \n",
    "        elif opcao == \"2\":\n",
    "            # Processamento h√≠brido\n",
    "            print(\"\\nü§ñ PROCESSAMENTO H√çBRIDO (regex + Gemini)\")\n",
    "            df_result = processar_dataframe_otimizado(df, use_gemini=True)\n",
    "            \n",
    "            # Salvar\n",
    "            arquivo_saida = 'nanomateriais_identificados_hibrido.csv'\n",
    "            df_result.to_csv(arquivo_saida, index=False)\n",
    "            print(f\"üíæ Salvo em: {arquivo_saida}\")\n",
    "            \n",
    "            gerar_estatisticas_rapidas(df_result)\n",
    "    \n",
    "    else:\n",
    "        print(\"‚ùå Coluna 'Abstract' n√£o encontrada\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1bee93",
   "metadata": {},
   "source": [
    "## 2. Defini√ß√£o de Palavras-Chave\n",
    "\n",
    "Conjunto abrangente de termos relacionados ao escopo de nanorevestimentos e tintas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d9945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Palavras-chave organizadas por categoria\n",
    "PALAVRAS_CHAVE = {\n",
    "    'termos_principais': [\n",
    "        \"nanocoating\", \"nanocoatings\", \"nano coating\", \"nano coatings\",\n",
    "        \"nanorevestimento\", \"nanorevestimentos\", \"nanocomposite coating\"\n",
    "    ],\n",
    "    \n",
    "    'materiais_nano': [\n",
    "        \"TiO2\", \"titanium dioxide\", \"zinc oxide\", \"ZnO\", \n",
    "        \"silica nanoparticle\", \"alumina nanoparticle\", \"Al2O3\",\n",
    "        \"carbon nanotube\", \"CNT\", \"graphene\", \"iron oxide\", \"Fe2O3\",\n",
    "        \"cerium oxide\", \"CeO2\", \"clay nanoparticles\"\n",
    "    ],\n",
    "    \n",
    "    'propriedades_funcionais': [\n",
    "        \"anticorrosive coating\", \"anti-corrosion coating\",\n",
    "        \"antimicrobial coating\", \"antibacterial coating\",\n",
    "        \"self-cleaning coating\", \"superhydrophobic coating\",\n",
    "        \"photocatalytic coating\", \"UV resistant coating\",\n",
    "        \"scratch resistant coating\", \"wear resistant coating\"\n",
    "    ],\n",
    "    \n",
    "    'tecnicas_preparacao': [\n",
    "        \"sol-gel coating\", \"layer-by-layer\", \"dip coating\",\n",
    "        \"spin coating\", \"spray coating\", \"electrodeposition\",\n",
    "        \"CVD coating\", \"PVD coating\", \"plasma treatment\"\n",
    "    ],\n",
    "    \n",
    "    'aplicacoes': [\n",
    "        \"protective coating\", \"functional coating\",\n",
    "        \"smart coating\", \"intelligent coating\", \"automotive coating\",\n",
    "        \"marine coating\", \"architectural coating\", \"biomedical coating\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Criar lista √∫nica de todas as palavras-chave\n",
    "todas_palavras_chave = []\n",
    "for categoria, palavras in PALAVRAS_CHAVE.items():\n",
    "    todas_palavras_chave.extend(palavras)\n",
    "\n",
    "print(f\"üîç Total de palavras-chave definidas: {len(todas_palavras_chave)}\")\n",
    "print(f\"üìÇ Categorias: {list(PALAVRAS_CHAVE.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1f9d12",
   "metadata": {},
   "source": [
    "## 3. Fun√ß√µes Utilit√°rias\n",
    "\n",
    "Fun√ß√µes para filtro, busca e manipula√ß√£o de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c5fe8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Informa√ß√µes do DataFrame:\n",
      "Total de linhas: 525\n",
      "Colunas dispon√≠veis: ['Publication Type', 'Authors', 'Book Authors', 'Book Editors', 'Book Group Authors', 'Author Full Names', 'Book Author Full Names', 'Group Authors', 'Article Title', 'Source Title', 'Book Series Title', 'Book Series Subtitle', 'Language', 'Document Type', 'Conference Title', 'Conference Date', 'Conference Location', 'Conference Sponsor', 'Conference Host', 'Author Keywords', 'Keywords Plus', 'Abstract', 'Addresses', 'Affiliations', 'Reprint Addresses', 'Email Addresses', 'Researcher Ids', 'ORCIDs', 'Funding Orgs', 'Funding Name Preferred', 'Funding Text', 'Cited References', 'Cited Reference Count', 'Times Cited, WoS Core', 'Times Cited, All Databases', '180 Day Usage Count', 'Since 2013 Usage Count', 'Publisher', 'Publisher City', 'Publisher Address', 'ISSN', 'eISSN', 'ISBN', 'Journal Abbreviation', 'Journal ISO Abbreviation', 'Publication Date', 'Publication Year', 'Volume', 'Issue', 'Part Number', 'Supplement', 'Special Issue', 'Meeting Abstract', 'Start Page', 'End Page', 'Article Number', 'DOI', 'DOI Link', 'Book DOI', 'Early Access Date', 'Number of Pages', 'WoS Categories', 'Web of Science Index', 'Research Areas', 'IDS Number', 'Pubmed Id', 'Open Access Designations', 'Highly Cited Status', 'Hot Paper Status', 'Date of Export', 'UT (Unique WOS ID)', 'Web of Science Record', 'Unnamed: 72']\n",
      "Buscando por palavras-chave espec√≠ficas...\n",
      "Buscando: 'nanocoating'\n",
      "  Encontrados 6 resultados em 'Article Title'\n",
      "  Encontrados 5 resultados em 'Abstract'\n",
      "Buscando: 'nanocoatings'\n",
      "  Encontrados 4 resultados em 'Article Title'\n",
      "  Encontrados 4 resultados em 'Abstract'\n",
      "Buscando: 'nano coating'\n",
      "Buscando: 'nano coatings'\n",
      "Buscando: 'nanorevestimento'\n",
      "Buscando: 'nanorevestimentos'\n",
      "Buscando: 'TiO2'\n",
      "  Encontrados 27 resultados em 'Article Title'\n",
      "  Encontrados 42 resultados em 'Abstract'\n",
      "Buscando: 'titanium dioxide'\n",
      "  Encontrados 7 resultados em 'Abstract'\n",
      "Buscando: 'zinc oxide'\n",
      "  Encontrados 4 resultados em 'Article Title'\n",
      "  Encontrados 13 resultados em 'Abstract'\n",
      "Buscando: 'ZnO'\n",
      "  Encontrados 17 resultados em 'Article Title'\n",
      "  Encontrados 25 resultados em 'Abstract'\n",
      "Buscando: 'silica nanoparticle'\n",
      "  Encontrados 1 resultados em 'Abstract'\n",
      "Buscando: 'alumina nanoparticle'\n",
      "Buscando: 'carbon nanotube'\n",
      "  Encontrados 11 resultados em 'Article Title'\n",
      "  Encontrados 20 resultados em 'Abstract'\n",
      "Buscando: 'CNT'\n",
      "  Encontrados 3 resultados em 'Article Title'\n",
      "  Encontrados 12 resultados em 'Abstract'\n",
      "Buscando: 'graphene'\n",
      "  Encontrados 16 resultados em 'Article Title'\n",
      "  Encontrados 20 resultados em 'Abstract'\n",
      "Buscando: 'anticorrosive coating'\n",
      "  Encontrados 1 resultados em 'Article Title'\n",
      "Buscando: 'anti-corrosion coating'\n",
      "Buscando: 'antimicrobial coating'\n",
      "  Encontrados 1 resultados em 'Article Title'\n",
      "  Encontrados 1 resultados em 'Abstract'\n",
      "Buscando: 'antibacterial coating'\n",
      "  Encontrados 1 resultados em 'Article Title'\n",
      "  Encontrados 2 resultados em 'Abstract'\n",
      "Buscando: 'self-cleaning coating'\n",
      "Buscando: 'superhydrophobic coating'\n",
      "  Encontrados 5 resultados em 'Article Title'\n",
      "  Encontrados 3 resultados em 'Abstract'\n",
      "Buscando: 'photocatalytic coating'\n",
      "  Encontrados 1 resultados em 'Article Title'\n",
      "  Encontrados 2 resultados em 'Abstract'\n",
      "Buscando: 'sol-gel coating'\n",
      "  Encontrados 1 resultados em 'Article Title'\n",
      "  Encontrados 2 resultados em 'Abstract'\n",
      "Buscando: 'layer-by-layer'\n",
      "  Encontrados 1 resultados em 'Article Title'\n",
      "  Encontrados 5 resultados em 'Abstract'\n",
      "Buscando: 'dip coating'\n",
      "  Encontrados 2 resultados em 'Article Title'\n",
      "  Encontrados 11 resultados em 'Abstract'\n",
      "Buscando: 'spin coating'\n",
      "  Encontrados 4 resultados em 'Article Title'\n",
      "  Encontrados 21 resultados em 'Abstract'\n",
      "Buscando: 'spray coating'\n",
      "  Encontrados 1 resultados em 'Article Title'\n",
      "  Encontrados 4 resultados em 'Abstract'\n",
      "Buscando: 'protective coating'\n",
      "  Encontrados 6 resultados em 'Article Title'\n",
      "  Encontrados 10 resultados em 'Abstract'\n",
      "Buscando: 'functional coating'\n",
      "  Encontrados 2 resultados em 'Abstract'\n",
      "Buscando: 'smart coating'\n",
      "Buscando: 'intelligent coating'\n",
      "\n",
      "Total de artigos √∫nicos encontrados: 145\n",
      "\n",
      "=== ARTIGOS ENCONTRADOS (145) ===\n",
      "\n",
      "Primeiros 10 t√≠tulos encontrados:\n",
      "1. Impact of Layer-by-Layer Self-Assembly Clay-Based Nanocoating on Flame Retardant Properties of Sisal Fiber Cellulose Microcrystals\n",
      "2. Stress-Strain Analysis in TiN Nanocoating Deposited on Polymer with respect to Au Nanointerlayer\n",
      "3. Morphology and Electrical Conductivity of Carbon Nanocoatings Prepared from Pyrolysed Polymers\n",
      "4. Comparative performance of a panel of commercially available antimicrobial nanocoatings in Europe\n",
      "5. Evaluation of the Particle Aerosolization from n-TiO2 Photocatalytic Nanocoatings under Abrasion\n",
      "6. Cyclic Nanoindentation and Finite Element Analysis of Ti/TiN and CrN Nanocoatings on Zr-Based Metallic Glasses Mechanical Performance\n",
      "7. Ab Initio Study of the Atomic Level Structure of the Rutile TiO2(110) -Titanium Nitride (TiN) Interface\n",
      "8. Self-Cleaning and Photocatalytic Performance of TiO2 Coating Films Prepared by Peroxo Titanic Acid\n",
      "9. Photocatalytic Performance of Carbon Monolith/TiO2 Composite\n",
      "10. Optical, Photocatalytical and Structural Properties of TiO2-SiO2 Sol-Gel Coatings on High Content SiO2 Enamel Surface\n",
      "\n",
      "=== AN√ÅLISE COM GEMINI (primeiros 10 artigos) ===\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31m_InactiveRpcError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env_del/lib/python3.12/site-packages/google/api_core/grpc_helpers.py:76\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env_del/lib/python3.12/site-packages/grpc/_interceptor.py:277\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable.__call__\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m    269\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    270\u001b[39m     request: Any,\n\u001b[32m   (...)\u001b[39m\u001b[32m    275\u001b[39m     compression: Optional[grpc.Compression] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    276\u001b[39m ) -> Any:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     response, ignored_call = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_with_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env_del/lib/python3.12/site-packages/grpc/_interceptor.py:332\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable._with_call\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m    329\u001b[39m call = \u001b[38;5;28mself\u001b[39m._interceptor.intercept_unary_unary(\n\u001b[32m    330\u001b[39m     continuation, client_call_details, request\n\u001b[32m    331\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, call\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env_del/lib/python3.12/site-packages/grpc/_channel.py:440\u001b[39m, in \u001b[36m_InactiveRpcError.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    439\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"See grpc.Future.result.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env_del/lib/python3.12/site-packages/grpc/_interceptor.py:315\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable._with_call.<locals>.continuation\u001b[39m\u001b[34m(new_details, request)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m     response, call = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_thunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_method\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_credentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_wait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_compression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _UnaryOutcome(response, call)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env_del/lib/python3.12/site-packages/grpc/_channel.py:1198\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable.with_call\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m   1192\u001b[39m (\n\u001b[32m   1193\u001b[39m     state,\n\u001b[32m   1194\u001b[39m     call,\n\u001b[32m   1195\u001b[39m ) = \u001b[38;5;28mself\u001b[39m._blocking(\n\u001b[32m   1196\u001b[39m     request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[32m   1197\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1198\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env_del/lib/python3.12/site-packages/grpc/_channel.py:1006\u001b[39m, in \u001b[36m_end_unary_response_blocking\u001b[39m\u001b[34m(state, call, with_call, deadline)\u001b[39m\n\u001b[32m   1005\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1006\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[31m_InactiveRpcError\u001b[39m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"The model is overloaded. Please try again later.\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv6:%5B2800:3f0:4004:808::200a%5D:443 {created_time:\"2025-05-27T16:29:24.757874371-03:00\", grpc_status:14, grpc_message:\"The model is overloaded. Please try again later.\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mServiceUnavailable\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env_del/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:147\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     result = \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env_del/lib/python3.12/site-packages/google/api_core/timeout.py:130\u001b[39m, in \u001b[36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m] = remaining_timeout\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env_del/lib/python3.12/site-packages/google/api_core/grpc_helpers.py:78\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mServiceUnavailable\u001b[39m: 503 The model is overloaded. Please try again later.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 81\u001b[39m\n\u001b[32m     78\u001b[39m amostra_analise = artigos_encontrados.head(\u001b[32m10\u001b[39m)\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     analise = \u001b[43manalisar_com_gemini\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamostra_analise\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAn√°lise do Gemini:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     83\u001b[39m     \u001b[38;5;28mprint\u001b[39m(analise)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 46\u001b[39m, in \u001b[36manalisar_com_gemini\u001b[39m\u001b[34m(dados_filtrados)\u001b[39m\n\u001b[32m     36\u001b[39m model = genai.GenerativeModel(\u001b[33m'\u001b[39m\u001b[33mgemini-2.0-flash\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     38\u001b[39m prompt = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[33mAnalise estes dados sobre \u001b[39m\u001b[33m'\u001b[39m\u001b[33mwords scope of paints and nanocoatings\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\n\u001b[32m     40\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     43\u001b[39m \u001b[33mForne√ßa insights sobre os padr√µes encontrados.\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[33m\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m response = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response.text\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env_del/lib/python3.12/site-packages/google/generativeai/generative_models.py:331\u001b[39m, in \u001b[36mGenerativeModel.generate_content\u001b[39m\u001b[34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[39m\n\u001b[32m    329\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types.GenerateContentResponse.from_iterator(iterator)\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m         response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    335\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types.GenerateContentResponse.from_response(response)\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m google.api_core.exceptions.InvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env_del/lib/python3.12/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:835\u001b[39m, in \u001b[36mGenerativeServiceClient.generate_content\u001b[39m\u001b[34m(self, request, model, contents, retry, timeout, metadata)\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_universe_domain()\n\u001b[32m    834\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m835\u001b[39m response = \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    837\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[32m    843\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env_del/lib/python3.12/site-packages/google/api_core/gapic_v1/method.py:131\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, compression, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m] = compression\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env_del/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:294\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    290\u001b[39m target = functools.partial(func, *args, **kwargs)\n\u001b[32m    291\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    293\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env_del/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:167\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    156\u001b[39m next_sleep = _retry_error_helper(\n\u001b[32m    157\u001b[39m     exc,\n\u001b[32m    158\u001b[39m     deadline,\n\u001b[32m   (...)\u001b[39m\u001b[32m    164\u001b[39m     timeout,\n\u001b[32m    165\u001b[39m )\n\u001b[32m    166\u001b[39m \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_sleep\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ...existing code...\n",
    "\n",
    "# Conjunto de palavras-chave mais espec√≠ficas para nanorevestimentos e tintas\n",
    "palavras_chave_especificas = [\n",
    "    # Termos principais\n",
    "    \"nanocoating\", \"nanocoatings\", \"nano coating\", \"nano coatings\",\n",
    "    \"nanorevestimento\", \"nanorevestimentos\",\n",
    "    \n",
    "    # Materiais nanoestruturados comuns\n",
    "    \"TiO2\", \"titanium dioxide\", \"zinc oxide\", \"ZnO\", \n",
    "    \"silica nanoparticle\", \"alumina nanoparticle\",\n",
    "    \"carbon nanotube\", \"CNT\", \"graphene\",\n",
    "    \n",
    "    # Propriedades funcionais\n",
    "    \"anticorrosive coating\", \"anti-corrosion coating\",\n",
    "    \"antimicrobial coating\", \"antibacterial coating\",\n",
    "    \"self-cleaning coating\", \"superhydrophobic coating\",\n",
    "    \"photocatalytic coating\",\n",
    "    \n",
    "    # T√©cnicas\n",
    "    \"sol-gel coating\", \"layer-by-layer\", \"dip coating\",\n",
    "    \"spin coating\", \"spray coating\",\n",
    "    \n",
    "    # Aplica√ß√µes\n",
    "    \"protective coating\", \"functional coating\",\n",
    "    \"smart coating\", \"intelligent coating\"\n",
    "]\n",
    "\n",
    "def buscar_artigos_nanorevestimentos(df, palavras_chave, colunas=['Article Title', 'Abstract']):\n",
    "    \"\"\"\n",
    "    Busca artigos usando m√∫ltiplas palavras-chave em m√∫ltiplas colunas\n",
    "    \"\"\"\n",
    "    resultados_totais = pd.DataFrame()\n",
    "    \n",
    "    print(\"Buscando por palavras-chave espec√≠ficas...\")\n",
    "    \n",
    "    for palavra in palavras_chave:\n",
    "        print(f\"Buscando: '{palavra}'\")\n",
    "        \n",
    "        for coluna in colunas:\n",
    "            if coluna in df.columns:\n",
    "                # Buscar na coluna\n",
    "                temp_resultados = filtrar_coluna(df, coluna, palavra, tipo='contem')\n",
    "                \n",
    "                if len(temp_resultados) > 0:\n",
    "                    print(f\"  Encontrados {len(temp_resultados)} resultados em '{coluna}'\")\n",
    "                    resultados_totais = pd.concat([resultados_totais, temp_resultados])\n",
    "    \n",
    "    # Remover duplicatas\n",
    "    if len(resultados_totais) > 0:\n",
    "        resultados_totais = resultados_totais.drop_duplicates()\n",
    "        print(f\"\\nTotal de artigos √∫nicos encontrados: {len(resultados_totais)}\")\n",
    "    else:\n",
    "        print(\"\\nNenhum artigo encontrado com as palavras-chave especificadas.\")\n",
    "    \n",
    "    return resultados_totais\n",
    "\n",
    "# Carregar e examinar o DataFrame\n",
    "df = pd.read_csv('/home/delon/Modelos/cenanoink/Projeto-CENanoInk/Alan Delon.csv')\n",
    "\n",
    "print(\"Informa√ß√µes do DataFrame:\")\n",
    "print(f\"Total de linhas: {len(df)}\")\n",
    "print(f\"Colunas dispon√≠veis: {list(df.columns)}\")\n",
    "\n",
    "# Buscar artigos relacionados a nanorevestimentos\n",
    "artigos_encontrados = buscar_artigos_nanorevestimentos(df, palavras_chave_especificas)\n",
    "\n",
    "if len(artigos_encontrados) > 0:\n",
    "    print(f\"\\n=== ARTIGOS ENCONTRADOS ({len(artigos_encontrados)}) ===\")\n",
    "    \n",
    "    # Mostrar primeiros resultados\n",
    "    print(\"\\nPrimeiros 10 t√≠tulos encontrados:\")\n",
    "    for i, titulo in enumerate(artigos_encontrados['Article Title'].head(10)):\n",
    "        print(f\"{i+1}. {titulo}\")\n",
    "    \n",
    "    # Analisar com Gemini (limitado a 10 artigos para evitar problemas de API)\n",
    "    print(f\"\\n=== AN√ÅLISE COM GEMINI (primeiros 10 artigos) ===\")\n",
    "    amostra_analise = artigos_encontrados.head(10)\n",
    "    \n",
    "    try:\n",
    "        analise = analisar_com_gemini(amostra_analise)\n",
    "        print(\"An√°lise do Gemini:\")\n",
    "        print(analise)\n",
    "    except Exception as e:\n",
    "        print(f\"Erro na an√°lise do Gemini: {e}\")\n",
    "    \n",
    "    # Salvar resultados\n",
    "    arquivo_saida = 'artigos_nanorevestimentos_encontrados.csv'\n",
    "    artigos_encontrados.to_csv(arquivo_saida, index=False)\n",
    "    print(f\"\\nArtigos salvos em: {arquivo_saida}\")\n",
    "    \n",
    "    # Estat√≠sticas por palavra-chave (opcional)\n",
    "    print(f\"\\n=== ESTAT√çSTICAS ===\")\n",
    "    print(f\"Total de artigos encontrados: {len(artigos_encontrados)}\")\n",
    "    \n",
    "    # Verificar se existe coluna Abstract para an√°lise detalhada\n",
    "    if 'Abstract' in artigos_encontrados.columns:\n",
    "        abstracts_validos = artigos_encontrados['Abstract'].notna().sum()\n",
    "        print(f\"Artigos com abstract dispon√≠vel: {abstracts_validos}\")\n",
    "        \n",
    "        if abstracts_validos > 0:\n",
    "            resposta = input(\"\\nDeseja analisar os abstracts para adequa√ß√£o ao escopo? (s/n): \")\n",
    "            if resposta.lower() == 's':\n",
    "                print(\"Processando an√°lise de escopo...\")\n",
    "                # Usar apenas uma amostra pequena para teste\n",
    "                amostra_escopo = artigos_encontrados.head(5)\n",
    "                resultado_escopo = processar_dataframe_escopo(amostra_escopo, batch_size=1)\n",
    "                \n",
    "                print(\"\\nResultados da an√°lise de escopo:\")\n",
    "                for idx, row in resultado_escopo.iterrows():\n",
    "                    print(f\"\\nArtigo {idx + 1}:\")\n",
    "                    print(f\"T√≠tulo: {row.get('Article Title', 'N/A')[:80]}...\")\n",
    "                    print(f\"Adequa√ß√£o: {row['Se adequa ao escopo?']}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nNenhum artigo encontrado. Vamos tentar com termos mais b√°sicos:\")\n",
    "    \n",
    "    # Termos mais b√°sicos se n√£o encontrar nada\n",
    "    termos_basicos = [\"coating\", \"paint\", \"nano\", \"nanoparticle\", \"surface\"]\n",
    "    \n",
    "    for termo in termos_basicos:\n",
    "        print(f\"\\nBuscando pelo termo b√°sico: '{termo}'\")\n",
    "        resultado_basico = filtrar_coluna(df, 'Article Title', termo, tipo='contem')\n",
    "        print(f\"Encontrados {len(resultado_basico)} artigos com '{termo}' no t√≠tulo\")\n",
    "        \n",
    "        if len(resultado_basico) > 0:\n",
    "            print(\"Primeiros 5 t√≠tulos:\")\n",
    "            for titulo in resultado_basico['Article Title'].head(5):\n",
    "                print(f\"  - {titulo}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49260797",
   "metadata": {},
   "source": [
    "## 4. An√°lise com Gemini AI\n",
    "\n",
    "Fun√ß√µes para an√°lise autom√°tica de abstracts usando a API do Gemini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91f2020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Informa√ß√µes do DataFrame:\n",
      "Total de linhas: 525\n",
      "Colunas dispon√≠veis: ['Publication Type', 'Authors', 'Book Authors', 'Book Editors', 'Book Group Authors', 'Author Full Names', 'Book Author Full Names', 'Group Authors', 'Article Title', 'Source Title', 'Book Series Title', 'Book Series Subtitle', 'Language', 'Document Type', 'Conference Title', 'Conference Date', 'Conference Location', 'Conference Sponsor', 'Conference Host', 'Author Keywords', 'Keywords Plus', 'Abstract', 'Addresses', 'Affiliations', 'Reprint Addresses', 'Email Addresses', 'Researcher Ids', 'ORCIDs', 'Funding Orgs', 'Funding Name Preferred', 'Funding Text', 'Cited References', 'Cited Reference Count', 'Times Cited, WoS Core', 'Times Cited, All Databases', '180 Day Usage Count', 'Since 2013 Usage Count', 'Publisher', 'Publisher City', 'Publisher Address', 'ISSN', 'eISSN', 'ISBN', 'Journal Abbreviation', 'Journal ISO Abbreviation', 'Publication Date', 'Publication Year', 'Volume', 'Issue', 'Part Number', 'Supplement', 'Special Issue', 'Meeting Abstract', 'Start Page', 'End Page', 'Article Number', 'DOI', 'DOI Link', 'Book DOI', 'Early Access Date', 'Number of Pages', 'WoS Categories', 'Web of Science Index', 'Research Areas', 'IDS Number', 'Pubmed Id', 'Open Access Designations', 'Highly Cited Status', 'Hot Paper Status', 'Date of Export', 'UT (Unique WOS ID)', 'Web of Science Record', 'Unnamed: 72']\n",
      "Abstracts n√£o vazios: 525\n",
      "\n",
      "=== PROCESSAMENTO DE TESTE (primeiras 5 linhas) ===\n",
      "Processando 5 abstracts...\n",
      "Processando linhas 1 a 1...\n",
      "Processando linhas 2 a 2...\n",
      "Processando linhas 3 a 3...\n",
      "Processando linhas 4 a 4...\n",
      "Processando linhas 5 a 5...\n",
      "  Processado: 5/5\n",
      "\n",
      "Resultados do teste:\n",
      "\n",
      "Linha 1:\n",
      "T√≠tulo: Ab Initio Study of the Atomic Level Structure of the Rutile TiO2(110) -Titanium Nitride (TiN) Interf...\n",
      "Adequa√ß√£o: Sim - O abstract descreve um estudo te√≥rico (DFT) das propriedades estruturais e eletr√¥nicas de uma interface TiO2/TiN, que se forma quando o nitreto de tit√¢nio (TiN), um material usado como revestimento protetor, √© oxidado. Embora seja um estudo fundamental, a compreens√£o da forma√ß√£o e propriedades dessa interface √© relevante para otimizar o desempenho de nanorevestimentos protetores baseados em TiN.\n",
      "\n",
      "Linha 2:\n",
      "T√≠tulo: Polymer and ceramic nanocomposites for aerospace applications...\n",
      "Adequa√ß√£o: Sim - O abstract menciona explicitamente nanocomp√≥sitos de matriz polim√©rica utilizados em revestimentos para aplica√ß√µes aeroespaciais e a capacidade de nanocomp√≥sitos de matriz cer√¢mica para fornecer blindagem eletromagn√©tica, indicando uma aplica√ß√£o em prote√ß√£o de superf√≠cies, alinhando-se com o escopo.\n",
      "\n",
      "Linha 3:\n",
      "T√≠tulo: Recycling of yttria-stabilized zirconia waste powders in glazes suitable for ceramic tiles...\n",
      "Adequa√ß√£o: 2.  \"N√£o - O abstract descreve a reciclagem de res√≠duos de zirc√¥nia estabilizada com √≠trio para a produ√ß√£o de fritas para esmaltes cer√¢micos e azulejos. Embora a zirc√¥nia estabilizada com √≠trio possa ser utilizada em nanorrevestimentos, o foco do abstract √© a aplica√ß√£o em esmaltes cer√¢micos, n√£o abordando especificamente as propriedades, t√©cnicas ou aplica√ß√µes relacionadas a nanorrevestimentos e tintas como definido nos crit√©rios.\"\n",
      "\n",
      "Linha 4:\n",
      "T√≠tulo: Properties of Polysiloxane Coated Borosilicate Lining Blocks...\n",
      "Adequa√ß√£o: 1. \"Sim - O abstract descreve a prepara√ß√£o e aplica√ß√£o de um revestimento contendo s√≠lica fumada (que pode ter dimens√µes nanom√©tricas) sobre um bloco de borossilicato, visando melhorar a resist√™ncia t√©rmica. A caracteriza√ß√£o do material e a an√°lise da sua funcionalidade (resist√™ncia t√©rmica) est√£o alinhadas com o escopo.\"\n",
      "\n",
      "Linha 5:\n",
      "T√≠tulo: Solution Processed Porous Fe2O3 Thin Films for Solar-Driven Water Splitting...\n",
      "Adequa√ß√£o: 2.  N√£o - O abstract descreve a prepara√ß√£o e caracteriza√ß√£o de filmes finos de hematita (Œ±-Fe2O3) mesoporosa para aplica√ß√£o em divis√£o de √°gua impulsionada por energia solar. Embora envolva nanomateriais e filmes finos, o foco principal √© na fotoeletroqu√≠mica e n√£o na aplica√ß√£o como revestimento ou tinta para prote√ß√£o de superf√≠cies ou propriedades funcionais t√≠picas de nanorrevestimentos (anticorros√£o, etc.).\n",
      "\n",
      "=== PROCESSANDO TODO O DATAFRAME ===\n",
      "Processando 525 abstracts...\n",
      "Processando linhas 1 a 10...\n",
      "  Processado: 5/525\n",
      "  Processado: 10/525\n",
      "Processando linhas 11 a 20...\n",
      "  Processado: 15/525\n",
      "  Processado: 20/525\n",
      "Processando linhas 21 a 30...\n",
      "  Processado: 25/525\n",
      "  Processado: 30/525\n",
      "Processando linhas 31 a 40...\n",
      "  Processado: 35/525\n",
      "  Processado: 40/525\n",
      "Processando linhas 41 a 50...\n",
      "  Processado: 45/525\n",
      "  Processado: 50/525\n",
      "Processando linhas 51 a 60...\n",
      "  Processado: 55/525\n",
      "  Processado: 60/525\n",
      "Processando linhas 61 a 70...\n",
      "  Processado: 65/525\n",
      "  Processado: 70/525\n",
      "Processando linhas 71 a 80...\n",
      "  Processado: 75/525\n",
      "  Processado: 80/525\n",
      "Processando linhas 81 a 90...\n",
      "  Processado: 85/525\n",
      "  Processado: 90/525\n",
      "Processando linhas 91 a 100...\n",
      "  Processado: 95/525\n",
      "  Processado: 100/525\n",
      "Processando linhas 101 a 110...\n",
      "  Processado: 105/525\n",
      "  Processado: 110/525\n",
      "Processando linhas 111 a 120...\n",
      "  Processado: 115/525\n",
      "  Processado: 120/525\n",
      "Processando linhas 121 a 130...\n",
      "  Processado: 125/525\n",
      "  Processado: 130/525\n",
      "Processando linhas 131 a 140...\n",
      "  Processado: 135/525\n",
      "  Processado: 140/525\n",
      "Processando linhas 141 a 150...\n",
      "  Processado: 145/525\n",
      "  Processado: 150/525\n",
      "Processando linhas 151 a 160...\n",
      "  Processado: 155/525\n",
      "  Processado: 160/525\n",
      "Processando linhas 161 a 170...\n",
      "  Processado: 165/525\n",
      "  Processado: 170/525\n",
      "Processando linhas 171 a 180...\n",
      "  Processado: 175/525\n",
      "  Processado: 180/525\n",
      "Processando linhas 181 a 190...\n",
      "  Processado: 185/525\n",
      "  Processado: 190/525\n",
      "Processando linhas 191 a 200...\n",
      "  Processado: 195/525\n",
      "  Processado: 200/525\n",
      "Processando linhas 201 a 210...\n",
      "  Processado: 205/525\n",
      "  Processado: 210/525\n",
      "Processando linhas 211 a 220...\n",
      "  Processado: 215/525\n",
      "  Processado: 220/525\n",
      "Processando linhas 221 a 230...\n",
      "  Processado: 225/525\n",
      "  Processado: 230/525\n",
      "Processando linhas 231 a 240...\n",
      "  Processado: 235/525\n",
      "  Processado: 240/525\n",
      "Processando linhas 241 a 250...\n",
      "  Processado: 245/525\n",
      "  Processado: 250/525\n",
      "Processando linhas 251 a 260...\n",
      "  Processado: 255/525\n",
      "  Processado: 260/525\n",
      "Processando linhas 261 a 270...\n",
      "  Processado: 265/525\n",
      "  Processado: 270/525\n",
      "Processando linhas 271 a 280...\n",
      "  Processado: 275/525\n",
      "  Processado: 280/525\n",
      "Processando linhas 281 a 290...\n",
      "  Processado: 285/525\n",
      "  Processado: 290/525\n",
      "Processando linhas 291 a 300...\n",
      "  Processado: 295/525\n",
      "  Processado: 300/525\n",
      "Processando linhas 301 a 310...\n",
      "  Processado: 305/525\n",
      "  Processado: 310/525\n",
      "Processando linhas 311 a 320...\n",
      "  Processado: 315/525\n",
      "  Processado: 320/525\n",
      "Processando linhas 321 a 330...\n",
      "  Processado: 325/525\n",
      "  Processado: 330/525\n",
      "Processando linhas 331 a 340...\n",
      "  Processado: 335/525\n",
      "  Processado: 340/525\n",
      "Processando linhas 341 a 350...\n",
      "  Processado: 345/525\n",
      "  Processado: 350/525\n",
      "Processando linhas 351 a 360...\n",
      "  Processado: 355/525\n",
      "  Processado: 360/525\n",
      "Processando linhas 361 a 370...\n",
      "  Processado: 365/525\n",
      "  Processado: 370/525\n",
      "Processando linhas 371 a 380...\n",
      "  Processado: 375/525\n",
      "  Processado: 380/525\n",
      "Processando linhas 381 a 390...\n",
      "  Processado: 385/525\n",
      "  Processado: 390/525\n",
      "Processando linhas 391 a 400...\n",
      "  Processado: 395/525\n",
      "  Processado: 400/525\n",
      "Processando linhas 401 a 410...\n",
      "  Processado: 405/525\n",
      "  Processado: 410/525\n",
      "Processando linhas 411 a 420...\n",
      "  Processado: 415/525\n",
      "  Processado: 420/525\n",
      "Processando linhas 421 a 430...\n",
      "  Processado: 425/525\n",
      "  Processado: 430/525\n",
      "Processando linhas 431 a 440...\n",
      "  Processado: 435/525\n",
      "  Processado: 440/525\n",
      "Processando linhas 441 a 450...\n",
      "  Processado: 445/525\n",
      "  Processado: 450/525\n",
      "Processando linhas 451 a 460...\n",
      "  Processado: 455/525\n",
      "  Processado: 460/525\n",
      "Processando linhas 461 a 470...\n",
      "  Processado: 465/525\n",
      "  Processado: 470/525\n",
      "Processando linhas 471 a 480...\n",
      "  Processado: 475/525\n",
      "  Processado: 480/525\n",
      "Processando linhas 481 a 490...\n",
      "  Processado: 485/525\n",
      "  Processado: 490/525\n",
      "Processando linhas 491 a 500...\n",
      "  Processado: 495/525\n",
      "  Processado: 500/525\n",
      "Processando linhas 501 a 510...\n",
      "  Processado: 505/525\n",
      "  Processado: 510/525\n",
      "Processando linhas 511 a 520...\n",
      "  Processado: 515/525\n",
      "  Processado: 520/525\n",
      "Processando linhas 521 a 525...\n",
      "  Processado: 525/525\n",
      "\n",
      "Resultados salvos em: df_com_analise_escopo.csv\n",
      "\n",
      "=== ESTAT√çSTICAS FINAIS ===\n",
      "Adequados ao escopo: 63\n",
      "N√£o adequados ao escopo: 114\n",
      "Total processado: 177\n"
     ]
    }
   ],
   "source": [
    "# ...existing code...\n",
    "\n",
    "def analisar_escopo_abstract(abstract_text):\n",
    "    \"\"\"\n",
    "    Analisa um abstract individual para determinar se se adequa ao escopo de nanorevestimentos e tintas\n",
    "    \"\"\"\n",
    "    if pd.isna(abstract_text) or str(abstract_text).strip() == '':\n",
    "        return \"N√£o se adequa - Abstract vazio ou n√£o dispon√≠vel\"\n",
    "    \n",
    "    model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Analise o seguinte abstract cient√≠fico e determine se ele se adequa ao escopo de pesquisa sobre \"nanorevestimentos e tintas (nanocoatings and paints)\".\n",
    "\n",
    "    Abstract: {abstract_text}\n",
    "\n",
    "    Crit√©rios para adequa√ß√£o ao escopo:\n",
    "    - Estudos sobre nanomateriais aplicados em revestimentos ou tintas\n",
    "    - Propriedades funcionais de nanocoatings (anticorros√£o, antimicrobiano, autolimpante, etc.)\n",
    "    - T√©cnicas de prepara√ß√£o ou caracteriza√ß√£o de nanorevestimentos\n",
    "    - Aplica√ß√µes industriais de tintas nanoestruturadas\n",
    "    - Materiais nanoestruturados para prote√ß√£o de superf√≠cies\n",
    "\n",
    "    Responda APENAS uma das op√ß√µes:\n",
    "    1. \"Sim - [breve justificativa]\"\n",
    "    2. \"N√£o - [breve justificativa explicando por que n√£o se adequa]\"\n",
    "\n",
    "    Mantenha a justificativa concisa (m√°ximo 100 palavras).\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Erro na an√°lise - {str(e)}\"\n",
    "\n",
    "def processar_dataframe_escopo(df, coluna_abstract='Abstract', batch_size=10):\n",
    "    \"\"\"\n",
    "    Processa todo o DataFrame adicionando a coluna de adequa√ß√£o ao escopo\n",
    "    \"\"\"\n",
    "    # Verificar se a coluna Abstract existe\n",
    "    if coluna_abstract not in df.columns:\n",
    "        print(f\"Coluna '{coluna_abstract}' n√£o encontrada no DataFrame\")\n",
    "        return df\n",
    "    \n",
    "    # Criar uma c√≥pia do DataFrame\n",
    "    df_processado = df.copy()\n",
    "    \n",
    "    # Inicializar a nova coluna\n",
    "    df_processado['Se adequa ao escopo?'] = ''\n",
    "    \n",
    "    total_linhas = len(df_processado)\n",
    "    print(f\"Processando {total_linhas} abstracts...\")\n",
    "    \n",
    "    # Processar em lotes para evitar sobrecarga da API\n",
    "    for i in range(0, total_linhas, batch_size):\n",
    "        batch_end = min(i + batch_size, total_linhas)\n",
    "        print(f\"Processando linhas {i+1} a {batch_end}...\")\n",
    "        \n",
    "        for idx in range(i, batch_end):\n",
    "            abstract = df_processado.iloc[idx][coluna_abstract]\n",
    "            resultado = analisar_escopo_abstract(abstract)\n",
    "            df_processado.iloc[idx, df_processado.columns.get_loc('Se adequa ao escopo?')] = resultado\n",
    "            \n",
    "            # Mostrar progresso\n",
    "            if (idx + 1) % 5 == 0:\n",
    "                print(f\"  Processado: {idx + 1}/{total_linhas}\")\n",
    "        \n",
    "        # Pequena pausa entre lotes para n√£o sobrecarregar a API\n",
    "        import time\n",
    "        time.sleep(2)\n",
    "    \n",
    "    return df_processado\n",
    "\n",
    "# Carregar o DataFrame\n",
    "df = pd.read_csv('/home/delon/Modelos/cenanoink/Projeto-CENanoInk/Alan Delon.csv')\n",
    "\n",
    "print(\"Informa√ß√µes do DataFrame:\")\n",
    "print(f\"Total de linhas: {len(df)}\")\n",
    "print(f\"Colunas dispon√≠veis: {list(df.columns)}\")\n",
    "\n",
    "# Verificar se existe coluna Abstract\n",
    "if 'Abstract' in df.columns:\n",
    "    print(f\"Abstracts n√£o vazios: {df['Abstract'].notna().sum()}\")\n",
    "    \n",
    "    # Processar apenas uma amostra primeiro (para teste)\n",
    "    print(\"\\n=== PROCESSAMENTO DE TESTE (primeiras 5 linhas) ===\")\n",
    "    df_teste = df.head(5).copy()\n",
    "    df_teste_processado = processar_dataframe_escopo(df_teste, batch_size=1)\n",
    "    \n",
    "    print(\"\\nResultados do teste:\")\n",
    "    for idx, row in df_teste_processado.iterrows():\n",
    "        print(f\"\\nLinha {idx + 1}:\")\n",
    "        print(f\"T√≠tulo: {row.get('Article Title', 'N/A')[:100]}...\")\n",
    "        print(f\"Adequa√ß√£o: {row['Se adequa ao escopo?']}\")\n",
    "    \n",
    "    # Perguntar se deseja processar todo o DataFrame\n",
    "    resposta = input(\"\\nDeseja processar todo o DataFrame? (s/n): \")\n",
    "    if resposta.lower() == 's':\n",
    "        print(\"\\n=== PROCESSANDO TODO O DATAFRAME ===\")\n",
    "        df_completo = processar_dataframe_escopo(df)\n",
    "        \n",
    "        # Salvar resultado\n",
    "        arquivo_saida = 'df_com_analise_escopo.csv'\n",
    "        df_completo.to_csv(arquivo_saida, index=False)\n",
    "        print(f\"\\nResultados salvos em: {arquivo_saida}\")\n",
    "        \n",
    "        # Mostrar estat√≠sticas\n",
    "        adequados = df_completo['Se adequa ao escopo?'].str.contains('Sim', case=False, na=False).sum()\n",
    "        nao_adequados = df_completo['Se adequa ao escopo?'].str.contains('N√£o', case=False, na=False).sum()\n",
    "        \n",
    "        print(f\"\\n=== ESTAT√çSTICAS FINAIS ===\")\n",
    "        print(f\"Adequados ao escopo: {adequados}\")\n",
    "        print(f\"N√£o adequados ao escopo: {nao_adequados}\")\n",
    "        print(f\"Total processado: {adequados + nao_adequados}\")\n",
    "        \n",
    "else:\n",
    "    print(\"Coluna 'Abstract' n√£o encontrada no DataFrame\")\n",
    "    print(\"Colunas dispon√≠veis:\", list(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d8d896",
   "metadata": {},
   "source": [
    "## 5. Processamento em Lote\n",
    "\n",
    "Fun√ß√µes para processar grandes volumes de dados respeitando rate limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227d6b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import genaiimport random\n",
    "as as pd\n",
    "import time\n",
    "import randomfrom typing import List\n",
    "\n",
    "def analisar_escopo_abstract(abstract_text: str, retry_count: int = 3) -> str:lisar_escopo_abstract(abstract_text, retry_count=3):\n",
    "    \"\"\"\n",
    "    Analisa um abstract individual para adequa√ß√£o ao escopoal para determinar se se adequa ao escopo de nanorevestimentos e tintas\n",
    "    ndo Gemini 2.0 Flash-Lite\n",
    "    Args:\n",
    "        abstract_text: Texto do abstract:\n",
    "        retry_count: N√∫mero de tentativas em caso de erro    return \"N√£o se adequa - Abstract vazio ou n√£o dispon√≠vel\"\n",
    "    \n",
    "    Returns:\n",
    "        String com resultado da an√°lisemodel = genai.GenerativeModel('gemini-2.0-flash-exp')\n",
    "    \"\"\"\n",
    "    if pd.isna(abstract_text) or str(abstract_text).strip() == '':\n",
    "        return \"N√£o se adequa - Abstract vazio ou n√£o dispon√≠vel\"    Analise o seguinte abstract cient√≠fico e determine se ele se adequa ao escopo de pesquisa sobre \"nanorevestimentos e tintas (nanocoatings and paints)\".\n",
    "    \n",
    "    model = genai.GenerativeModel(CONFIG['modelo_gemini'])    Abstract: {abstract_text}\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Analise o seguinte abstract cient√≠fico e determine se ele se adequa ao escopo de pesquisa sobre \"nanorevestimentos e tintas (nanocoatings and paints)\".\n",
    "crobiano, autolimpante, etc.)\n",
    "    Abstract: {abstract_text}evestimentos\n",
    "\n",
    "    Crit√©rios para adequa√ß√£o ao escopo:    - Materiais nanoestruturados para prote√ß√£o de superf√≠cies\n",
    "    - Estudos sobre nanomateriais aplicados em revestimentos ou tintas\n",
    "    - Propriedades funcionais de nanocoatings (anticorros√£o, antimicrobiano, autolimpante, etc.)\n",
    "    - T√©cnicas de prepara√ß√£o ou caracteriza√ß√£o de nanorevestimentos\n",
    "    - Aplica√ß√µes industriais de tintas nanoestruturadas    2. \"N√£o - [breve justificativa explicando por que n√£o se adequa]\"\n",
    "    - Materiais nanoestruturados para prote√ß√£o de superf√≠cies\n",
    "tenha a justificativa concisa (m√°ximo 100 palavras).\n",
    "    Responda APENAS uma das op√ß√µes:\"\"\"\n",
    "    1. \"Sim - [breve justificativa]\"\n",
    "    2. \"N√£o - [breve justificativa explicando por que n√£o se adequa]\"mpt in range(retry_count):\n",
    "\n",
    "    Mantenha a justificativa concisa (m√°ximo 100 palavras).ntent(prompt)\n",
    "    \"\"\"xt.strip()\n",
    "    \n",
    "    for attempt in range(retry_count):\n",
    "        try:in error_msg.lower():\n",
    "            response = model.generate_content(prompt)\n",
    "            return response.text.strip()ar rate limit\n",
    "        except Exception as e:\n",
    "            error_msg = str(e)- aguardando {wait_time}s...\")\n",
    "            if \"429\" in error_msg or \"quota\" in error_msg.lower():ep(wait_time)\n",
    "                if attempt < retry_count - 1:ontinue\n",
    "                    wait_time = 3 + random.randint(1, 2)\n",
    "                    print(f\"  ‚è≥ Rate limit - aguardando {wait_time}s...\")   return f\"Erro - Rate limit excedido ap√≥s {retry_count} tentativas\"\n",
    "                    time.sleep(wait_time)\n",
    "                    continue            return f\"Erro na an√°lise - {error_msg}\"\n",
    "                else:\n",
    "                    return f\"Erro - Rate limit excedido ap√≥s {retry_count} tentativas\"    return \"Erro - Falha ap√≥s m√∫ltiplas tentativas\"\n",
    "            else:\n",
    "                              coluna_abstract: str = 'Abstract') -> pd.DataFrame:         return f\"Erro na an√°lise - {error_msg}\"cessar_dataframe_escopo(df, coluna_abstract='Abstract', max_requests_per_day=1400):\n",
    "    \"\"\"\n",
    "    Processa DataFrame completo adicionando an√°lise de escopo\" de adequa√ß√£o ao escopo\n",
    "    Respeitando limites de API: 30 req/min, 1.500 req/diao limites: 30 req/min, 1.500 req/dia\n",
    "    bstract='Abstract', max_requests_per_day=1400):\n",
    "    Args:\n",
    "        df: DataFrame a ser processadoo\n",
    "        coluna_abstract: Nome da coluna com abstractsimites: 30 req/min, 1.500 req/diaoluna '{coluna_abstract}' n√£o encontrada no DataFrame\")\n",
    "    \"\"\"    return df\n",
    "    Returns:ct existe\n",
    "        DataFrame com nova coluna 'Se adequa ao escopo?' df.columns:Frame\n",
    "    \"\"\"    print(f\"Coluna '{coluna_abstract}' n√£o encontrada no DataFrame\")df_processado = df.copy()\n",
    "    if coluna_abstract not in df.columns:\n",
    "        print(f\"‚ùå Coluna '{coluna_abstract}' n√£o encontrada\")\n",
    "        return df# Criar uma c√≥pia do DataFramedf_processado['Se adequa ao escopo?'] = ''\n",
    "    \n",
    "    df_processado = df.copy()\n",
    "    df_processado['Se adequa ao escopo?'] = ''# Inicializar a nova colunaprint(f\"Processando {total_linhas} abstracts...\")\n",
    "    '\n",
    "    total_linhas = len(df_processado)\n",
    "    print(f\"üìã Processando {total_linhas} abstracts...\")\n",
    "    \n",
    "    # Verificar limite di√°rioax_requests_per_day} linhas? (s/n): \")\n",
    "    if total_linhas > CONFIG['max_requests_per_day']:\n",
    "        print(f\"‚ö†Ô∏è Dataset tem {total_linhas} linhas, limite di√°rio: {CONFIG['max_requests_per_day']}\")linhas > max_requests_per_day:otal_linhas = max_requests_per_day\n",
    "        resposta = input(f\"Processar apenas as primeiras {CONFIG['max_requests_per_day']} linhas? (s/n): \")inhas} linhas, mas limite di√°rio √© {max_requests_per_day}\")\n",
    "        if resposta.lower() == 's':a processar apenas as primeiras {max_requests_per_day} linhas? (s/n): \") cancelado.\")\n",
    "            total_linhas = CONFIG['max_requests_per_day']    if resposta.lower() == 's':        return df_processado\n",
    "        else:\n",
    "            print(\"‚ùå Processamento cancelado\")\n",
    "            return df_processado        print(\"Processamento cancelado.\")print(f\"Tempo estimado: {(total_linhas * 2.5) / 60:.1f} minutos\")\n",
    "    _processado\n",
    "    tempo_estimado = (total_linhas * CONFIG['delay_between_requests']) / 60\n",
    "    print(f\"‚è±Ô∏è Tempo estimado: {tempo_estimado:.1f} minutos\")print(f\"Rate limiting: 30 req/min, processando {total_linhas} abstracts\")start_time = time.time()\n",
    "    print(f\"üîÑ Rate limiting: {CONFIG['max_requests_per_minute']} req/min\")_linhas * 2.5) / 60:.1f} minutos\")\n",
    "    \n",
    "    requests_made = 0\n",
    "    start_time = time.time()\n",
    "    adequados_count = 0\n",
    "    \n",
    "    for idx in range(total_linhas): 30 req/min aguardando {wait_time:.1f}s...\")\n",
    "        # Controle de rate limitime() - start_timeime)\n",
    "        elapsed_time = time.time() - start_timelapsed_time < 60:\n",
    "        if requests_made >= (CONFIG['max_requests_per_minute'] - 1) and elapsed_time < 60:    wait_time = 60 - elapsed_time + 2  # +2 segundos de margem    start_time = time.time()\n",
    "            wait_time = 60 - elapsed_time + 2}s...\")\n",
    "            print(f\"\\n‚è≥ Rate limit: aguardando {wait_time:.1f}s...\")    time.sleep(wait_time)abstract = df_processado.iloc[idx][coluna_abstract]\n",
    "            time.sleep(wait_time) 0\n",
    "            requests_made = 0\n",
    "            start_time = time.time()\n",
    "        abstract = df_processado.iloc[idx][coluna_abstract]    print(f\"Processando {idx + 1}/{total_linhas} ({((idx + 1)/total_linhas)*100:.1f}%)\")\n",
    "        abstract = df_processado.iloc[idx][coluna_abstract]\n",
    "        \n",
    "        # Progressoif idx % 10 == 0 or idx == total_linhas - 1:df_processado.iloc[idx, df_processado.columns.get_loc('Se adequa ao escopo?')] = resultado\n",
    "        if idx % 10 == 0 or idx == total_linhas - 1:sando {idx + 1}/{total_linhas} ({((idx + 1)/total_linhas)*100:.1f}%)\")\n",
    "            print(f\"üìä Processando {idx + 1}/{total_linhas} ({((idx + 1)/total_linhas)*100:.1f}%)\")requests_made += 1\n",
    "        \n",
    "        resultado = analisar_escopo_abstract(abstract)entre requisi√ß√µes (2 segundos para 30 req/min)loc[idx, df_processado.columns.get_loc('Se adequa ao escopo?')] = resultado\n",
    "        df_processado.iloc[idx, df_processado.columns.get_loc('Se adequa ao escopo?')] = resultadotime.sleep(2.1)\n",
    "        \n",
    "        if 'Sim' in resultado:ada 25 processamentos\n",
    "            adequados_count += 1\n",
    "        +1].str.contains('Sim', case=False, na=False).sum()\n",
    "        requests_made += 1        print(f\"  Adequados at√© agora: {adequados_parcial}/{idx + 1}\")    \n",
    "        time.sleep(CONFIG['delay_between_requests'])samentos\n",
    "            return df_processado        if (idx + 1) % 25 == 0:\n",
    "        # Estat√≠sticas parciais\n",
    "        if (idx + 1) % 25 == 0:     print(f\"  Adequados at√© agora: {adequados_parcial}/{idx + 1}\")cessar_com_checkpoint(df, coluna_abstract='Abstract', checkpoint_file='checkpoint_analise.csv', start_from=0):\n",
    "            taxa_atual = (adequados_count / (idx + 1)) * 100\n",
    "            print(f\"  ‚úÖ Adequados at√© agora: {adequados_count}/{idx + 1} ({taxa_atual:.1f}%)\")urn df_processados√£o com checkpoint para poder retomar o processamento\n",
    "    \n",
    "    return df_processadotract='Abstract', checkpoint_file='checkpoint_analise.csv', start_from=0):\n",
    "_file and start_from == 0:\n",
    "def processar_com_checkpoint(df: pd.DataFrame, ento\n",
    "                           coluna_abstract: str = 'Abstract') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Processamento com sistema de checkpoint para recupera√ß√£o\n",
    "    )\n",
    "    Args:checkpoint_file)s':\n",
    "        df: DataFrame a ser processadopo?' in df_checkpoint.columns:ckpoint\n",
    "        coluna_abstract: Nome da coluna com abstractslinhas j√° processadas\")\n",
    "                resposta = input(\"Deseja continuar do checkpoint? (s/n): \")        print(\"Nenhum checkpoint encontrado, iniciando do zero\")\n",
    "    Returns:.lower() == 's':\n",
    "        DataFrame processado\n",
    "    \"\"\"    except FileNotFoundError:df_processado = processar_dataframe_escopo(df, coluna_abstract)\n",
    "    checkpoint_file = CONFIG['checkpoint_file']um checkpoint encontrado, iniciando do zero\")\n",
    "    \n",
    "    # Verificar checkpoint existente\n",
    "    try:a_abstract)e)\n",
    "        df_checkpoint = pd.read_csv(checkpoint_file)    print(f\"Checkpoint salvo em: {checkpoint_file}\")\n",
    "        if 'Se adequa ao escopo?' in df_checkpoint.columns:\n",
    "            processed_count = df_checkpoint['Se adequa ao escopo?'].ne('').sum()    if checkpoint_file:    return df_processado\n",
    "            print(f\"üìÇ Checkpoint encontrado: {processed_count} linhas j√° processadas\")to_csv(checkpoint_file, index=False)\n",
    "            \n",
    "            resposta = input(\"Continuar do checkpoint? (s/n): \")        \"\"\"\n",
    "            if resposta.lower() == 's': espec√≠fica\n",
    "                return df_checkpoint\n",
    "    except FileNotFoundError:.DataFrame) -> str:\n",
    "        print(\"üìù Nenhum checkpoint encontrado, iniciando do zero\")    \"\"\"        df: DataFrame a ser filtrado\n",
    "    rtigos com Gemini\n",
    "    # Processar normalmente\n",
    "    df_processado = processar_dataframe_escopo(df, coluna_abstract)\n",
    "    s\n",
    "    # Salvar checkpointReturns:\n",
    "    df_processado.to_csv(checkpoint_file, index=False)\n",
    "    print(f\"üíæ Checkpoint salvo em: {checkpoint_file}\")\n",
    "    \n",
    "    return df_processadoif len(dados_filtrados) == 0:    print(f\"‚ö†Ô∏è Coluna '{coluna}' n√£o encontrada\")\n",
    "aFrame()m dado dispon√≠vel para an√°lise.\"\n",
    "# Carregar o DataFrame\n",
    "df = pd.read_csv('/home/delon/Modelos/cenanoink/Projeto-CENanoInk/Alan Delon.csv')\n",
    "str.contains(termo, case=False, na=False)filtrados.head(10)\n",
    "print(\"Informa√ß√µes do DataFrame:\")elif tipo == 'exato':texto_dados = dados_amostra[['Article Title', 'Abstract']].to_string()\n",
    "print(f\"Total de linhas: {len(df)}\")\n",
    "print(f\"Colunas dispon√≠veis: {list(df.columns)}\")delo_gemini'])\n",
    "h(termo.lower())\n",
    "# Verificar se existe coluna Abstract\n",
    "if 'Abstract' in df.columns:s e tintas:\n",
    "    abstracts_validos = df['Abstract'].notna().sum()\n",
    "    print(f\"Abstracts n√£o vazios: {abstracts_validos}\")return df[mask]{texto_dados}\n",
    "    \n",
    "    # Teste com 3 abstracts primeiroataFrame, \n",
    "    print(\"\\n=== TESTE INICIAL (3 abstracts) ===\")\n",
    "    df_teste = df.head(3).copy()['Article Title', 'Abstract']) -> pd.DataFrame:\n",
    "    \n",
    "    # Processar testeBusca artigos usando m√∫ltiplas palavras-chave em m√∫ltiplas colunas4. Aplica√ß√µes industriais identificadas\n",
    "    start_test = time.time()\n",
    "    df_teste_processado = processar_dataframe_escopo(df_teste)\n",
    "    end_test = time.time()\n",
    "    \n",
    "    print(f\"\\nTempo do teste: {end_test - start_test:.1f} segundos\")colunas: Colunas onde buscar\n",
    "    print(\"\\nResultados do teste:\")\n",
    "    for idx, row in df_teste_processado.iterrows():\n",
    "        print(f\"\\nLinha {idx + 1}:\")\n",
    "        print(f\"T√≠tulo: {row.get('Article Title', 'N/A')[:80]}...\")\n",
    "        print(f\"Adequa√ß√£o: {row['Se adequa ao escopo?']}\")ltados = pd.DataFrame()return f\"Erro na an√°lise geral: {str(e)}\"\n",
    "    \n",
    "    # Perguntar sobre processamento completo\n",
    "    print(f\"\\n=== PROCESSAMENTO COMPLETO ===\")\n",
    "    print(f\"Total de abstracts para processar: {abstracts_validos}\")\n",
    "    print(f\"Limite di√°rio recomendado: 1.400 requisi√ß√µes\")palavra in palavras_chave:nforma√ß√µes do DataFrame:\")\n",
    "    print(f\"Tempo estimado: {(abstracts_validos * 2.5) / 60:.1f} minutos\")\n",
    "    \n",
    "    resposta = input(\"\\nDeseja processar todo o DataFrame? (s/n): \")\n",
    "    if resposta.lower() == 's':oluna(df, coluna, palavra, tipo='contem')tract\n",
    "        print(\"\\nIniciando processamento completo com checkpoint...\")\n",
    "        df_completo = processar_com_checkpoint(df, checkpoint_file='analise_escopo_checkpoint.csv')            count_palavra += len(temp)racts_validos = df['Abstract'].notna().sum()\n",
    "        tados = pd.concat([resultados, temp]) vazios: {abstracts_validos}\")\n",
    "        # Salvar resultado final\n",
    "        arquivo_saida = 'df_com_analise_escopo_completo.csv'if count_palavra > 0:ste com 3 abstracts primeiro\n",
    "        df_completo.to_csv(arquivo_saida, index=False)       estatisticas[palavra] = count_palavrarint(\"\\n=== TESTE INICIAL (3 abstracts) ===\")\n",
    "        print(f\"\\nResultados finais salvos em: {arquivo_saida}\")dos\")\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Colunas dispon√≠veis:\", list(df.columns))    print(\"Coluna 'Abstract' n√£o encontrada no DataFrame\")else:                    print(f\"Taxa de adequa√ß√£o: {(adequados/(adequados + nao_adequados))*100:.1f}%\")        if adequados > 0:                print(f\"Total processado: {adequados + nao_adequados + erros}\")        print(f\"Erros: {erros}\")        print(f\"N√£o adequados ao escopo: {nao_adequados}\")        print(f\"Adequados ao escopo: {adequados}\")        print(f\"\\n=== ESTAT√çSTICAS FINAIS ===\")                erros = df_completo['Se adequa ao escopo?'].str.contains('Erro', case=False, na=False).sum()        nao_adequados = df_completo['Se adequa ao escopo?'].str.contains('N√£o', case=False, na=False).sum()        adequados = df_completo['Se adequa ao escopo?'].str.contains('Sim', case=False, na=False).sum()        # Mostrar estat√≠sticas\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Colunas dispon√≠veis:\", list(df.columns))    print(\"Coluna 'Abstract' n√£o encontrada no DataFrame\")else:                    print(f\"Taxa de adequa√ß√£o: {(adequados/(adequados + nao_adequados))*100:.1f}%\")        if adequados > 0:                print(f\"Total processado: {adequados + nao_adequados + erros}\")        print(f\"Erros: {erros}\")        print(f\"N√£o adequados ao escopo: {nao_adequados}\")        print(f\"Adequados ao escopo: {adequados}\")        print(f\"\\n=== ESTAT√çSTICAS FINAIS ===\")                erros = df_completo['Se adequa ao escopo?'].str.contains('Erro', case=False, na=False).sum()        nao_adequados = df_completo['Se adequa ao escopo?'].str.contains('N√£o', case=False, na=False).sum()        adequados = df_completo['Se adequa ao escopo?'].str.contains('Sim', case=False, na=False).sum()        # Mostrar estat√≠sticas                print(f\"\\nResultados finais salvos em: {arquivo_saida}\")        df_completo.to_csv(arquivo_saida, index=False)        arquivo_saida = 'df_com_analise_escopo_completo.csv'        # Salvar resultado final                df_completo = processar_com_checkpoint(df, checkpoint_file='analise_escopo_checkpoint.csv')        print(\"\\nIniciando processamento completo com checkpoint...\")    if resposta.lower() == 's':    resposta = input(\"\\nDeseja processar todo o DataFrame? (s/n): \")        print(f\"Tempo estimado: {(abstracts_validos * 2.5) / 60:.1f} minutos\")    print(f\"Limite di√°rio recomendado: 1.400 requisi√ß√µes\")    print(f\"Total de abstracts para processar: {abstracts_validos}\")    print(f\"\\n=== PROCESSAMENTO COMPLETO ===\")    # Perguntar sobre processamento completo            print(f\"Adequa√ß√£o: {row['Se adequa ao escopo?']}\")        print(f\"T√≠tulo: {row.get('Article Title', 'N/A')[:80]}...\")        print(f\"\\nLinha {idx + 1}:\")    for idx, row in df_teste_processado.iterrows():    print(\"\\nResultados do teste:\")    print(f\"\\nTempo do teste: {end_test - start_test:.1f} segundos\")        end_test = time.time()    df_teste_processado = processar_dataframe_escopo(df_teste)    start_test = time.time()    # Processar teste        df_teste = df.head(3).copy()    print(\"\\n=== TESTE INICIAL (3 abstracts) ===\")    # Teste com 3 abstracts primeiro        print(f\"Abstracts n√£o vazios: {abstracts_validos}\")    abstracts_validos = df['Abstract'].notna().sum()if 'Abstract' in df.columns:# Verificar se existe coluna Abstractprint(f\"Colunas dispon√≠veis: {list(df.columns)}\")print(f\"Total de linhas: {len(df)}\")print(\"Informa√ß√µes do DataFrame:\")df = pd.read_csv('/home/delon/Modelos/cenanoink/Projeto-CENanoInk/Alan Delon.csv')# Carregar o DataFrame    return resultados            print(\"\\n‚ùå Nenhum artigo encontrado\")    else:        print(f\"üìä Palavras-chave com resultados: {len(estatisticas)}\")        print(f\"\\n‚úÖ Total de artigos √∫nicos encontrados: {len(resultados)}\")        resultados = resultados.drop_duplicates()    if len(resultados) > 0:    # Remover duplicatas    # Processar teste\n",
    "    start_test = time.time()\n",
    "    df_teste_processado = processar_dataframe_escopo(df_teste)\n",
    "    end_test = time.time()\n",
    "    \n",
    "    print(f\"\\nTempo do teste: {end_test - start_test:.1f} segundos\")\n",
    "    print(\"\\nResultados do teste:\")\n",
    "    for idx, row in df_teste_processado.iterrows():\n",
    "        print(f\"\\nLinha {idx + 1}:\")\n",
    "        print(f\"T√≠tulo: {row.get('Article Title', 'N/A')[:80]}...\")\n",
    "        print(f\"Adequa√ß√£o: {row['Se adequa ao escopo?']}\")\n",
    "    \n",
    "    # Perguntar sobre processamento completo\n",
    "    print(f\"\\n=== PROCESSAMENTO COMPLETO ===\")\n",
    "    print(f\"Total de abstracts para processar: {abstracts_validos}\")\n",
    "    print(f\"Limite di√°rio recomendado: 1.400 requisi√ß√µes\")\n",
    "    print(f\"Tempo estimado: {(abstracts_validos * 2.5) / 60:.1f} minutos\")\n",
    "    \n",
    "    resposta = input(\"\\nDeseja processar todo o DataFrame? (s/n): \")\n",
    "    if resposta.lower() == 's':\n",
    "        print(\"\\nIniciando processamento completo com checkpoint...\")\n",
    "        df_completo = processar_com_checkpoint(df, checkpoint_file='analise_escopo_checkpoint.csv')\n",
    "        \n",
    "        # Salvar resultado final\n",
    "        arquivo_saida = 'df_com_analise_escopo_completo.csv'\n",
    "        df_completo.to_csv(arquivo_saida, index=False)\n",
    "        print(f\"\\nResultados finais salvos em: {arquivo_saida}\")\n",
    "        \n",
    "        # Mostrar estat√≠sticas\n",
    "        adequados = df_completo['Se adequa ao escopo?'].str.contains('Sim', case=False, na=False).sum()\n",
    "        nao_adequados = df_completo['Se adequa ao escopo?'].str.contains('N√£o', case=False, na=False).sum()\n",
    "        erros = df_completo['Se adequa ao escopo?'].str.contains('Erro', case=False, na=False).sum()\n",
    "        \n",
    "        print(f\"\\n=== ESTAT√çSTICAS FINAIS ===\")\n",
    "        print(f\"Adequados ao escopo: {adequados}\")\n",
    "        print(f\"N√£o adequados ao escopo: {nao_adequados}\")\n",
    "        print(f\"Erros: {erros}\")\n",
    "        print(f\"Total processado: {adequados + nao_adequados + erros}\")\n",
    "        \n",
    "        if adequados > 0:\n",
    "            print(f\"Taxa de adequa√ß√£o: {(adequados/(adequados + nao_adequados))*100:.1f}%\")\n",
    "        \n",
    "else:\n",
    "    print(\"Coluna 'Abstract' n√£o encontrada no DataFrame\")\n",
    "    print(\"Colunas dispon√≠veis:\", list(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6776909f",
   "metadata": {},
   "source": [
    "## 6. Execu√ß√£o Principal\n",
    "\n",
    "Script principal para carregar dados e executar an√°lises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d006991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar e explorar os dados\n",
    "print(\"üìÇ Carregando dados...\")\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(CONFIG['arquivo_entrada'])\n",
    "    print(f\"‚úÖ Dados carregados com sucesso\")\n",
    "    print(f\"üìä Total de linhas: {len(df):,}\")\n",
    "    print(f\"üìä Total de colunas: {len(df.columns)}\")\n",
    "    print(f\"üìã Colunas dispon√≠veis: {list(df.columns)}\")\n",
    "    \n",
    "    # Verificar colunas essenciais\n",
    "    colunas_essenciais = ['Article Title', 'Abstract']\n",
    "    colunas_encontradas = [col for col in colunas_essenciais if col in df.columns]\n",
    "    colunas_faltantes = [col for col in colunas_essenciais if col not in df.columns]\n",
    "    \n",
    "    print(f\"\\n‚úÖ Colunas encontradas: {colunas_encontradas}\")\n",
    "    if colunas_faltantes:\n",
    "        print(f\"‚ö†Ô∏è Colunas faltantes: {colunas_faltantes}\")\n",
    "    \n",
    "    # Estat√≠sticas dos abstracts\n",
    "    if 'Abstract' in df.columns:\n",
    "        abstracts_validos = df['Abstract'].notna().sum()\n",
    "        abstracts_vazios = df['Abstract'].isna().sum()\n",
    "        print(f\"\\nüìÑ Abstracts dispon√≠veis: {abstracts_validos:,}\")\n",
    "        print(f\"üìÑ Abstracts vazios: {abstracts_vazios:,}\")\n",
    "        print(f\"üìÑ Taxa de completude: {(abstracts_validos/len(df))*100:.1f}%\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Arquivo n√£o encontrado: {CONFIG['arquivo_entrada']}\")\n",
    "    df = None\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro ao carregar dados: {e}\")\n",
    "    df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59f7858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscar artigos relevantes por palavras-chave\n",
    "if df is not None:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üîç BUSCA POR PALAVRAS-CHAVE\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Buscar artigos usando todas as palavras-chave\n",
    "    artigos_encontrados = buscar_artigos_por_palavras_chave(df, todas_palavras_chave)\n",
    "    \n",
    "    if len(artigos_encontrados) > 0:\n",
    "        print(f\"\\nüìã RESULTADOS DA BUSCA:\")\n",
    "        print(f\"  Total de artigos encontrados: {len(artigos_encontrados):,}\")\n",
    "        print(f\"  Percentual do dataset: {(len(artigos_encontrados)/len(df))*100:.2f}%\")\n",
    "        \n",
    "        # Mostrar amostra dos t√≠tulos\n",
    "        print(f\"\\nüìë Primeiros 10 t√≠tulos encontrados:\")\n",
    "        for i, titulo in enumerate(artigos_encontrados['Article Title'].head(10), 1):\n",
    "            print(f\"  {i:2d}. {titulo[:80]}{'...' if len(titulo) > 80 else ''}\")\n",
    "        \n",
    "        # Salvar artigos encontrados\n",
    "        arquivo_filtrados = 'artigos_nanorevestimentos_encontrados.csv'\n",
    "        artigos_encontrados.to_csv(arquivo_filtrados, index=False)\n",
    "        print(f\"\\nüíæ Artigos filtrados salvos em: {arquivo_filtrados}\")\n",
    "        \n",
    "        # An√°lise geral com Gemini\n",
    "        print(f\"\\nü§ñ Realizando an√°lise geral com Gemini...\")\n",
    "        analise_geral = analisar_com_gemini_geral(artigos_encontrados)\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"üìä AN√ÅLISE GERAL - GEMINI AI\")\n",
    "        print(\"=\"*50)\n",
    "        print(analise_geral)\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n‚ùå Nenhum artigo encontrado com as palavras-chave especificadas\")\n",
    "        print(\"üí° Sugest√£o: Verificar se o dataset cont√©m artigos da √°rea\")\n",
    "else:\n",
    "    print(\"‚ùå N√£o foi poss√≠vel carregar os dados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92012426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise detalhada de escopo (opcional)\n",
    "if df is not None and len(artigos_encontrados) > 0:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üî¨ AN√ÅLISE DETALHADA DE ESCOPO\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    if 'Abstract' in artigos_encontrados.columns:\n",
    "        abstracts_disponiveis = artigos_encontrados['Abstract'].notna().sum()\n",
    "        print(f\"üìÑ Artigos com abstract: {abstracts_disponiveis:,}\")\n",
    "        \n",
    "        if abstracts_disponiveis > 0:\n",
    "            print(f\"‚ö†Ô∏è ATEN√á√ÉO: Esta an√°lise usa a API do Gemini\")\n",
    "            print(f\"   Limite: {CONFIG['max_requests_per_day']} requisi√ß√µes/dia\")\n",
    "            print(f\"   Tempo estimado: {(abstracts_disponiveis * CONFIG['delay_between_requests'])/60:.1f} minutos\")\n",
    "            \n",
    "            resposta = input(\"\\nRealizar an√°lise detalhada de todos os abstracts? (s/n): \")\n",
    "            \n",
    "            if resposta.lower() == 's':\n",
    "                print(\"\\nüöÄ Iniciando an√°lise detalhada com checkpoint...\")\n",
    "                \n",
    "                # Usar apenas artigos encontrados para an√°lise\n",
    "                df_analisado = processar_com_checkpoint(artigos_encontrados)\n",
    "                \n",
    "                # Salvar resultado final\n",
    "                df_analisado.to_csv(CONFIG['arquivo_saida'], index=False)\n",
    "                print(f\"\\nüíæ An√°lise completa salva em: {CONFIG['arquivo_saida']}\")\n",
    "                \n",
    "                # Estat√≠sticas finais\n",
    "                print(\"\\n\" + \"=\"*50)\n",
    "                print(\"üìä ESTAT√çSTICAS FINAIS\")\n",
    "                print(\"=\"*50)\n",
    "                \n",
    "                adequados = df_analisado['Se adequa ao escopo?'].str.contains('Sim', case=False, na=False).sum()\n",
    "                nao_adequados = df_analisado['Se adequa ao escopo?'].str.contains('N√£o', case=False, na=False).sum()\n",
    "                erros = df_analisado['Se adequa ao escopo?'].str.contains('Erro', case=False, na=False).sum()\n",
    "                total_analisado = adequados + nao_adequados + erros\n",
    "                \n",
    "                print(f\"‚úÖ Adequados ao escopo: {adequados:,}\")\n",
    "                print(f\"‚ùå N√£o adequados: {nao_adequados:,}\")\n",
    "                print(f\"‚ö†Ô∏è Erros: {erros:,}\")\n",
    "                print(f\"üìä Total analisado: {total_analisado:,}\")\n",
    "                \n",
    "                if adequados + nao_adequados > 0:\n",
    "                    taxa_adequacao = (adequados / (adequados + nao_adequados)) * 100\n",
    "                    print(f\"üìà Taxa de adequa√ß√£o: {taxa_adequacao:.1f}%\")\n",
    "                \n",
    "                # Mostrar alguns exemplos\n",
    "                if adequados > 0:\n",
    "                    print(f\"\\nüìë Exemplos de artigos adequados:\")\n",
    "                    adequados_df = df_analisado[df_analisado['Se adequa ao escopo?'].str.contains('Sim', case=False, na=False)]\n",
    "                    for i, row in adequados_df.head(3).iterrows():\n",
    "                        print(f\"\\n{i+1}. {row['Article Title'][:80]}...\")\n",
    "                        print(f\"   Adequa√ß√£o: {row['Se adequa ao escopo?']}\")\n",
    "            \n",
    "            else:\n",
    "                print(\"\\n‚è≠Ô∏è An√°lise detalhada pulada\")\n",
    "                \n",
    "                # Fazer teste com 3 abstracts\n",
    "                print(\"\\nüß™ Realizando teste com 3 abstracts...\")\n",
    "                amostra_teste = artigos_encontrados.head(3)\n",
    "                df_teste = processar_dataframe_escopo(amostra_teste)\n",
    "                \n",
    "                print(\"\\nüìã Resultados do teste:\")\n",
    "                for idx, row in df_teste.iterrows():\n",
    "                    print(f\"\\n{idx+1}. {row['Article Title'][:60]}...\")\n",
    "                    print(f\"   Adequa√ß√£o: {row['Se adequa ao escopo?']}\")\n",
    "        else:\n",
    "            print(\"‚ùå Nenhum abstract dispon√≠vel para an√°lise\")\n",
    "    else:\n",
    "        print(\"‚ùå Coluna 'Abstract' n√£o encontrada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746b4a56",
   "metadata": {},
   "source": [
    "## 7. Conclus√£o\n",
    "\n",
    "### Funcionalidades implementadas:\n",
    "\n",
    "1. ‚úÖ **Busca por palavras-chave**: Sistema robusto de filtro com m√∫ltiplas categorias\n",
    "2. ‚úÖ **An√°lise com Gemini AI**: An√°lise autom√°tica de relev√¢ncia dos abstracts\n",
    "3. ‚úÖ **Rate limiting**: Respeita limites da API (30 req/min, 1.500 req/dia)\n",
    "4. ‚úÖ **Sistema de checkpoint**: Permite retomar an√°lises interrompidas\n",
    "5. ‚úÖ **Estat√≠sticas detalhadas**: M√©tricas de adequa√ß√£o e performance\n",
    "6. ‚úÖ **Exporta√ß√£o de dados**: Salva resultados em formato CSV\n",
    "\n",
    "### Arquivos gerados:\n",
    "- `artigos_nanorevestimentos_encontrados.csv`: Artigos filtrados por palavras-chave\n",
    "- `analise_escopo_checkpoint.csv`: Checkpoint do processamento\n",
    "- `df_com_analise_escopo_completo.csv`: An√°lise completa com Gemini\n",
    "\n",
    "### Pr√≥ximos passos:\n",
    "- Refinar palavras-chave com base nos resultados\n",
    "- Implementar an√°lise de trends temporais\n",
    "- Adicionar an√°lise de colabora√ß√µes e redes\n",
    "- Desenvolver dashboard interativo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_del",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
