{"cells":[{"source":"import pandas as pd\nimport zipfile\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom gensim.models import Word2Vec\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')","metadata":{"executionCancelledAt":null,"executionTime":56,"lastExecutedAt":1738536410526,"lastExecutedByKernel":"73922ebd-74a8-4a1d-930d-bffd738645a2","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import pandas as pd\nimport zipfile\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom gensim.models import Word2Vec\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')","outputsMetadata":{"0":{"height":149,"type":"stream"}}},"cell_type":"code","id":"3ee261f3-424c-4289-90fa-ae6ea5304696","outputs":[{"output_type":"stream","name":"stderr","text":"[nltk_data] Downloading package punkt to /home/repl/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /home/repl/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /home/repl/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"},{"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{},"execution_count":11}],"execution_count":11},{"source":"## Pipeline de Processamento :\n\n- **Entrada do Usuário:** A mensagem do usuário é capturada e pré-processada (remoção de ruídos, tokenização).\n- **Word2Vec:** Gera embeddings das palavras para capturar o significado semântico.\n- **Análise de Sentimentos:** Avalia o tom emocional da mensagem.\n- **RNN:** Processa os embeddings e o histórico da conversa para gerar uma resposta contextualizada.\n- **Saída:** O chatbot gera uma resposta personalizada com base nas intenções detectadas, no contexto e no sentimento do usuário.","metadata":{},"cell_type":"markdown","id":"df231a8a-8ab5-4aba-a572-9190ef3d360f"},{"source":"zip_file_path = 'sentiment+labelled+sentences.zip'\nwith zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n    zip_ref.extractall()\n\n# \namazon = pd.read_csv(\"sentiment labelled sentences/amazon_cells_labelled.txt\", delimiter=\"\\t\", header=None, names=[\"sentence\", \"sentiment\"])\nimdb = pd.read_csv(\"sentiment labelled sentences/imdb_labelled.txt\", delimiter=\"\\t\", header=None, names=[\"sentence\", \"sentiment\"])\nyelp = pd.read_csv(\"sentiment labelled sentences/yelp_labelled.txt\", delimiter=\"\\t\", header=None, names=[\"sentence\", \"sentiment\"])\n\ndf = pd.concat([imdb, yelp, amazon], ignore_index=True)\ndf.info()\ndf.head()","metadata":{"executionCancelledAt":null,"executionTime":243,"lastExecutedAt":1738535618366,"lastExecutedByKernel":"73922ebd-74a8-4a1d-930d-bffd738645a2","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"zip_file_path = 'sentiment+labelled+sentences.zip'\nwith zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n    zip_ref.extractall()\n\n# \namazon = pd.read_csv(\"sentiment labelled sentences/amazon_cells_labelled.txt\", delimiter=\"\\t\", header=None, names=[\"sentence\", \"sentiment\"])\nimdb = pd.read_csv(\"sentiment labelled sentences/imdb_labelled.txt\", delimiter=\"\\t\", header=None, names=[\"sentence\", \"sentiment\"])\nyelp = pd.read_csv(\"sentiment labelled sentences/yelp_labelled.txt\", delimiter=\"\\t\", header=None, names=[\"sentence\", \"sentiment\"])\n\ndf = pd.concat([imdb, yelp, amazon], ignore_index=True)\ndf.info()\ndf.head()","outputsMetadata":{"0":{"height":215,"type":"stream"},"1":{"height":501,"type":"dataFrame","tableState":{"quickFilterText":""}}}},"cell_type":"code","id":"54fa2434-b1c8-442c-900b-c992d24ebf59","outputs":[{"output_type":"stream","name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2748 entries, 0 to 2747\nData columns (total 2 columns):\n #   Column     Non-Null Count  Dtype \n---  ------     --------------  ----- \n 0   sentence   2748 non-null   object\n 1   sentiment  2748 non-null   int64 \ndtypes: int64(1), object(1)\nmemory usage: 43.1+ KB\n"},{"output_type":"execute_result","data":{"application/com.datacamp.data-table.v2+json":{"table":{"schema":{"fields":[{"name":"index","type":"integer"},{"name":"sentence","type":"string"},{"name":"sentiment","type":"integer"}],"primaryKey":["index"],"pandas_version":"1.4.0"},"data":{"index":[0,1,2,3,4],"sentence":["A very, very, very slow-moving, aimless movie about a distressed, drifting young man.  ","Not sure who was more lost - the flat characters or the audience, nearly half of whom walked out.  ","Attempting artiness with black & white and clever camera angles, the movie disappointed - became even more ridiculous - as the acting was poor and the plot and lines almost non-existent.  ","Very little music or anything to speak of.  ","The best scene in the movie was when Gerardo is trying to find a song that keeps running through his head.  "],"sentiment":[0,0,0,0,1]}},"total_rows":5,"truncation_type":null},"text/plain":"                                            sentence  sentiment\n0  A very, very, very slow-moving, aimless movie ...          0\n1  Not sure who was more lost - the flat characte...          0\n2  Attempting artiness with black & white and cle...          0\n3       Very little music or anything to speak of.            0\n4  The best scene in the movie was when Gerardo i...          1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A very, very, very slow-moving, aimless movie ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Not sure who was more lost - the flat characte...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Attempting artiness with black &amp; white and cle...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Very little music or anything to speak of.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The best scene in the movie was when Gerardo i...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{"application/com.datacamp.data-table.v2+json":{"status":"success"}},"execution_count":3}],"execution_count":3},{"source":"## Uso do Word2Vec\nO Word2Vec transforma o texto em vetores numéricos de alta dimensão capaz de capturar as relações semânticas e sintaáticas entre as palavras. Então ela ajuda ao chatbot a identificar as diferenças variações da pergunta e contexto da situação. ","metadata":{},"cell_type":"markdown","id":"1386c45b-2e87-497a-ad91-05dc7b3e06d3"},{"source":"def preprocess(text):\n    # Tokenização\n    tokens = nltk.word_tokenize(text.lower())\n    # Remoção de pontuação e stopwords\n    tokens = [word for word in tokens if word.isalpha() and word not in stopwords.words('english')]\n    # Lematização\n    lemmatizer = WordNetLemmatizer()\n    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n    return ' '.join(tokens)","metadata":{"executionCancelledAt":null,"executionTime":11,"lastExecutedAt":1738535622165,"lastExecutedByKernel":"73922ebd-74a8-4a1d-930d-bffd738645a2","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"def preprocess(text):\n    # Tokenização\n    tokens = nltk.word_tokenize(text.lower())\n    # Remoção de pontuação e stopwords\n    tokens = [word for word in tokens if word.isalpha() and word not in stopwords.words('english')]\n    # Lematização\n    lemmatizer = WordNetLemmatizer()\n    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n    return ' '.join(tokens)"},"cell_type":"code","id":"6c33b67a-0606-4c74-bc2b-2966eab06684","outputs":[],"execution_count":4},{"source":"# Pré-processamento das frases\ndf['processed_sentence'] = df['sentence'].apply(preprocess)","metadata":{"executionCancelledAt":null,"executionTime":5560,"lastExecutedAt":1738535630806,"lastExecutedByKernel":"73922ebd-74a8-4a1d-930d-bffd738645a2","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Pré-processamento das frases\ndf['processed_sentence'] = df['sentence'].apply(preprocess)"},"cell_type":"code","id":"49a8be05-8052-4156-9bf7-8116194c0677","outputs":[],"execution_count":5},{"source":"model_word2vec = Word2Vec(sentences=[sentence.split() for sentence in df['processed_sentence']], vector_size=100, window=5, min_count=1, workers=4)","metadata":{"executionCancelledAt":null,"executionTime":150,"lastExecutedAt":1738535633419,"lastExecutedByKernel":"73922ebd-74a8-4a1d-930d-bffd738645a2","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"model_word2vec = Word2Vec(sentences=[sentence.split() for sentence in df['processed_sentence']], vector_size=100, window=5, min_count=1, workers=4)"},"cell_type":"code","id":"c785c272-cbad-4816-a6c6-9f1b1ed206c6","outputs":[],"execution_count":6},{"source":"def sentence_vector(sentence):\n    # Calcular a média dos vetores de palavras na frase\n    words = sentence.split()\n    word_vectors = [model_word2vec.wv[word] for word in words if word in model_word2vec.wv]\n    if word_vectors:\n        return np.mean(word_vectors, axis=0)\n    else:\n        return np.zeros(model_word2vec.vector_size)\n\nimport numpy as np\n\n# Criar vetores de frases\nX_vectors = np.array([sentence_vector(sentence) for sentence in df['processed_sentence']])","metadata":{"executionCancelledAt":null,"executionTime":89,"lastExecutedAt":1738535755584,"lastExecutedByKernel":"73922ebd-74a8-4a1d-930d-bffd738645a2","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"def sentence_vector(sentence):\n    # Calcular a média dos vetores de palavras na frase\n    words = sentence.split()\n    word_vectors = [model_word2vec.wv[word] for word in words if word in model_word2vec.wv]\n    if word_vectors:\n        return np.mean(word_vectors, axis=0)\n    else:\n        return np.zeros(model_word2vec.vector_size)\n\nimport numpy as np\n\n# Criar vetores de frases\nX_vectors = np.array([sentence_vector(sentence) for sentence in df['processed_sentence']])"},"cell_type":"code","id":"8020217e-056c-4a6d-a28e-cb831a1938e5","outputs":[],"execution_count":7},{"source":"X_train, X_test, y_train, y_test = train_test_split(X_vectors, df['sentiment'], test_size=0.2, random_state=42)","metadata":{"executionCancelledAt":null,"executionTime":52,"lastExecutedAt":1738535792642,"lastExecutedByKernel":"73922ebd-74a8-4a1d-930d-bffd738645a2","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"X_train, X_test, y_train, y_test = train_test_split(X_vectors, df['sentiment'], test_size=0.2, random_state=42)"},"cell_type":"code","id":"a2a2f24e-277a-4a2d-9bcf-0576175c553d","outputs":[],"execution_count":8},{"source":"## Impacto das RNN no chatbot","metadata":{},"cell_type":"markdown","id":"d9ce0495-6ce6-4ee0-88ce-caa9a8e60e22"},{"source":"O modelos RNNs, especialmente arquiteturas como LSTM (Long Short-Term Memory) e GRU (Gated Recurrent Unit), são capazes de processar sequências de dados e capturar dependências de longo prazo. Isso permite que o chat bot mantenha uma meméria ao longo da conversa, dê respostas mais coerentes e contextuais, além de identificar sentimentos.No cenário de atendimento ao cliente isso é importante para evitar atritos entre o cliente e a empresa, já que o atendimento irá se aproximar mais do humano. ","metadata":{},"cell_type":"markdown","id":"4d20013d-c9bc-49cf-9cdc-37d90a6fcc0d"},{"source":"# Parâmetros do modelo\nvocab_size = 1000\nembedding_dim = 100\nlstm_units = 128\n\nmodel_rnn = Sequential([\n    Embedding(vocab_size, embedding_dim, input_length=10),\n    LSTM(lstm_units),\n    Dense(1, activation='sigmoid')\n])\n\nmodel_rnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","metadata":{"executionCancelledAt":null,"executionTime":42,"lastExecutedAt":1738535795169,"lastExecutedByKernel":"73922ebd-74a8-4a1d-930d-bffd738645a2","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Parâmetros do modelo\nvocab_size = 1000\nembedding_dim = 100\nlstm_units = 128\n\nmodel_rnn = Sequential([\n    Embedding(vocab_size, embedding_dim, input_length=10),\n    LSTM(lstm_units),\n    Dense(1, activation='sigmoid')\n])\n\nmodel_rnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"},"cell_type":"code","id":"1d30103f-fe3b-44cd-a55e-f6f97be6a567","outputs":[],"execution_count":9},{"source":"# Treinamento do modelo RNN\nmodel_rnn.fit(X_train, y_train, epochs=10, batch_size=4, validation_data=(X_test, y_test))","metadata":{"executionCancelledAt":null,"executionTime":254554,"lastExecutedAt":1738536054560,"lastExecutedByKernel":"73922ebd-74a8-4a1d-930d-bffd738645a2","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Treinamento do modelo RNN\nmodel_rnn.fit(X_train, y_train, epochs=10, batch_size=4, validation_data=(X_test, y_test))","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"cell_type":"code","id":"eed1aec3-4a85-4302-a83d-46aaafba1117","outputs":[{"output_type":"stream","name":"stdout","text":"Epoch 1/10\n\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 46ms/step - accuracy: 0.4803 - loss: 0.6945 - val_accuracy: 0.4564 - val_loss: 0.6990\nEpoch 2/10\n\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 46ms/step - accuracy: 0.5144 - loss: 0.6934 - val_accuracy: 0.4564 - val_loss: 0.6962\nEpoch 3/10\n\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 46ms/step - accuracy: 0.5101 - loss: 0.6932 - val_accuracy: 0.4564 - val_loss: 0.6979\nEpoch 4/10\n\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 46ms/step - accuracy: 0.5092 - loss: 0.6936 - val_accuracy: 0.4564 - val_loss: 0.6963\nEpoch 5/10\n\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 46ms/step - accuracy: 0.5268 - loss: 0.6920 - val_accuracy: 0.4564 - val_loss: 0.6933\nEpoch 6/10\n\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 46ms/step - accuracy: 0.4993 - loss: 0.6933 - val_accuracy: 0.4564 - val_loss: 0.6949\nEpoch 7/10\n\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 46ms/step - accuracy: 0.5156 - loss: 0.6929 - val_accuracy: 0.4564 - val_loss: 0.6959\nEpoch 8/10\n\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 46ms/step - accuracy: 0.5175 - loss: 0.6928 - val_accuracy: 0.4564 - val_loss: 0.6959\nEpoch 9/10\n\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 46ms/step - accuracy: 0.5269 - loss: 0.6923 - val_accuracy: 0.4564 - val_loss: 0.6955\nEpoch 10/10\n\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 46ms/step - accuracy: 0.5203 - loss: 0.6925 - val_accuracy: 0.4564 - val_loss: 0.6955\n"},{"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7f2117f44670>"},"metadata":{},"execution_count":10}],"execution_count":10},{"source":"## Exemplo de interação","metadata":{},"cell_type":"markdown","id":"6d3bfe77-12c1-4001-8721-6ca717663eb1"},{"source":"def chatbot_response(user_input):\n    processed_input = preprocess(user_input)\n  \n    sentiment = model_rnn.predict([user_vector])[0][0] if 'user_vector' in locals() else 0  \n    \n    sentiment_label = 1 if sentiment >= 0.5 else 0\n    \n  \n    keywords = ['change password', 'reset password', 'password', 'ajuda', 'suporte']\n    has_keyword = any(keyword in processed_input for keyword in keywords)\n    \n    # Geração de resposta baseada no sentimento e palavras-chave\n    if has_keyword:\n        if 'change password' in processed_input or 'reset password' in processed_input:\n            response = \"Claro, posso ajudar você a redefinir sua senha. Por favor, siga estas etapas...\"\n        elif 'password' in processed_input:\n            response = \"Parece que você está tendo problemas com sua senha. Como posso ajudar?\"\n        else:\n            response = \"Claro, como posso ajudar você hoje?\"\n    else:\n        if sentiment_label == 1:\n            response = \"Que ótimo saber que você está feliz!\"\n        else:\n            response = \"Lamento que você esteja se sentindo assim. Como posso ajudar?\"\n    \n    return response","metadata":{"executionCancelledAt":null,"executionTime":14,"lastExecutedAt":1738536747055,"lastExecutedByKernel":"73922ebd-74a8-4a1d-930d-bffd738645a2","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"def chatbot_response(user_input):\n    # Pré-processamento da entrada do usuário\n    processed_input = preprocess(user_input)\n    \n    # Criar vetor para a entrada do usuário usando Word2Vec (se estiver usando)\n    # user_vector = sentence_vector(processed_input)\n    \n    # Previsão do sentimento\n    sentiment = model_rnn.predict([user_vector])[0][0] if 'user_vector' in locals() else 0  # Exemplo simplificado\n    \n    # Mapeamento da previsão para rótulo\n    sentiment_label = 1 if sentiment >= 0.5 else 0\n    \n    # Verificação de palavras-chave\n    keywords = ['change password', 'reset password', 'password', 'ajuda', 'suporte']\n    has_keyword = any(keyword in processed_input for keyword in keywords)\n    \n    # Geração de resposta baseada no sentimento e palavras-chave\n    if has_keyword:\n        if 'change password' in processed_input or 'reset password' in processed_input:\n            response = \"Claro, posso ajudar você a redefinir sua senha. Por favor, siga estas etapas...\"\n        elif 'password' in processed_input:\n            response = \"Parece que você está tendo problemas com sua senha. Como posso ajudar?\"\n        else:\n            response = \"Claro, como posso ajudar você hoje?\"\n    else:\n        if sentiment_label == 1:\n            response = \"Que ótimo saber que você está feliz!\"\n        else:\n            response = \"Lamento que você esteja se sentindo assim. Como posso ajudar?\"\n    \n    return response"},"cell_type":"code","id":"683eb2f3-5a62-49ed-aade-5ac6c6f42635","outputs":[],"execution_count":12},{"source":"user_input = \"I wanna change password\"\nprint(chatbot_response(user_input))\nuser_input = \"I so angry\"\nprint(chatbot_response(user_input))","metadata":{"executionCancelledAt":null,"executionTime":52,"lastExecutedAt":1738537034366,"lastExecutedByKernel":"73922ebd-74a8-4a1d-930d-bffd738645a2","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"user_input = \"I wanna change password\"\nprint(chatbot_response(user_input))\nuser_input = \"I so angry\"\nprint(chatbot_response(user_input))","outputsMetadata":{"0":{"height":83,"type":"stream"}}},"cell_type":"code","id":"f4a2bf74-6275-45a4-bb95-d5005e2d15e0","outputs":[{"output_type":"stream","name":"stdout","text":"Claro, posso ajudar você a redefinir sua senha. Por favor, siga estas etapas...\nLamento que você esteja se sentindo assim. Como posso ajudar?\n"}],"execution_count":17}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":5}