{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eead57c9",
   "metadata": {},
   "source": [
    "# ğŸ“Š CenanoInk Data Loader - Web of Science Excel Files\n",
    "\n",
    "## ğŸ¯ Purpose\n",
    "This notebook handles the loading, consolidation, and initial validation of Web of Science Excel files for the CenanoInk nanotechnology research project.\n",
    "\n",
    "## ğŸ“ Expected Data Structure\n",
    "- Input: 39 Excel files (.xls format) from Web of Science\n",
    "- Each file contains ~1000 scientific article records\n",
    "- Total: ~39,000 articles about nanomaterials and nanocoatings\n",
    "\n",
    "## ğŸ”„ Workflow\n",
    "1. **Load** all Excel files from directory\n",
    "2. **Consolidate** into single DataFrame\n",
    "3. **Map** WoS columns to standard format\n",
    "4. **Validate** data quality and completeness\n",
    "5. **Export** consolidated dataset for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31357368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š CenanoInk Data Loader - Web of Science Excel\n",
      "==================================================\n",
      "ğŸ“ Data directory: /home/delon/Modelos/modeloCenanoInk/data/raw/arquivos_excel_artigo_cienciometrico/\n",
      "ğŸ’¾ Output directory: /home/delon/Modelos/modeloCenanoInk/data/processed/\n",
      "ğŸ“‹ Expected files: 39\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“¦ DEPENDENCIES AND CONFIGURATION\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'data_directory': '/home/delon/Modelos/modeloCenanoInk/data/raw/arquivos_excel_artigo_cienciometrico/',\n",
    "    'output_directory': '/home/delon/Modelos/modeloCenanoInk/data/processed/',\n",
    "    'expected_files': 39,\n",
    "    'file_pattern': '*.xls'\n",
    "}\n",
    "\n",
    "print(\"ğŸ“Š CenanoInk Data Loader - Web of Science Excel\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"ğŸ“ Data directory: {CONFIG['data_directory']}\")\n",
    "print(f\"ğŸ’¾ Output directory: {CONFIG['output_directory']}\")\n",
    "print(f\"ğŸ“‹ Expected files: {CONFIG['expected_files']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e977222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Executing the refactored 'discover_excel_files' function using CONFIG values...\n",
      "ğŸ”„ [INFO] Initializing Excel file discovery and validation process...\n",
      "===================================================================\n",
      "ğŸ“ [VALIDATION] Checking existence of directory: /home/delon/Modelos/modeloCenanoInk/data/raw/arquivos_excel_artigo_cienciometrico/\n",
      "âœ… [SUCCESS] Directory confirmed: /home/delon/Modelos/modeloCenanoInk/data/raw/arquivos_excel_artigo_cienciometrico/\n",
      "ğŸ” [SEARCH] Using search pattern: /home/delon/Modelos/modeloCenanoInk/data/raw/arquivos_excel_artigo_cienciometrico/*.xls\n",
      "ğŸ’¡ [RESULT] Glob search completed. Found 39 potential item(s) matching pattern.\n",
      "ğŸ“Š [COUNT] Successfully identified 39 Excel file(s).\n",
      "âœ… [VALIDATION SUCCESS] File count (39) perfectly matches the expectation (39 files).\n",
      "\n",
      "ğŸ“‹ [INFO] Displaying a sample of discovered files (up to 5 files, sorted alphabetically):\n",
      "   1. ğŸ“„ File: savedrecs1.xls | Size: 4.29 MB | Modified: 2025-04-23 15:47:08\n",
      "   2. ğŸ“„ File: savedrecs10.xls | Size: 4.25 MB | Modified: 2025-04-23 15:59:24\n",
      "   3. ğŸ“„ File: savedrecs11.xls | Size: 4.30 MB | Modified: 2025-04-23 15:59:49\n",
      "   4. ğŸ“„ File: savedrecs12.xls | Size: 4.32 MB | Modified: 2025-04-23 16:00:16\n",
      "   5. ğŸ“„ File: savedrecs13.xls | Size: 4.30 MB | Modified: 2025-04-23 16:00:39\n",
      "     ... and 34 more file(s) not listed in this sample.\n",
      "\n",
      "ğŸ‘ [COMPLETE] Excel file discovery and validation process finished successfully.\n",
      "===================================================================\n",
      "\n",
      "âœ… [SUMMARY] Successfully discovered and validated 39 Excel file(s).\n",
      "ğŸ“ The variable 'excel_files' is now populated with their paths and ready for use in subsequent cells.\n"
     ]
    }
   ],
   "source": [
    "# ğŸ” EXCEL FILE DISCOVERY AND VALIDATION\n",
    "import os \n",
    "import glob \n",
    "from typing import List\n",
    "from datetime import datetime # Added for file modification timestamp\n",
    "\n",
    "# CONFIG is assumed to be defined in a previous cell (e.g., cell 'ba70942e') and accessible here.\n",
    "\n",
    "def discover_excel_files(directory: str, file_pattern: str, expected_files_count: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Discovers, validates, and lists Excel files in a specified directory. ğŸ“‚âœ¨\n",
    "\n",
    "    This function meticulously searches for files matching a given pattern (e.g., '*.xls')\n",
    "    within the provided directory. It then validates the number of files found against\n",
    "    an expected count, providing clear feedback. Finally, it prints a summary of the\n",
    "    findings, including a sample of the discovered files with their names, sizes,\n",
    "    and last modification dates.\n",
    "\n",
    "    Args:\n",
    "        directory (str): The absolute path to the directory where Excel files are located.\n",
    "                         Example: '/path/to/your/data/excel_files'\n",
    "        file_pattern (str): The glob pattern used to identify Excel files.\n",
    "                            Example: '*.xls', 'data_*.xlsx'\n",
    "        expected_files_count (int): The anticipated number of Excel files to be found.\n",
    "                                    This helps in quick verification of data integrity.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A sorted list of absolute paths to the discovered Excel files.\n",
    "                   Returns an empty list if no files are found, if the directory\n",
    "                   does not exist, or if an error occurs during the search.\n",
    "    \n",
    "    Side Effects:\n",
    "        Prints detailed logs, warnings, and error messages to the console.\n",
    "    \"\"\"\n",
    "    print(\"ğŸ”„ [INFO] Initializing Excel file discovery and validation process...\")\n",
    "    print(\"===================================================================\")\n",
    "\n",
    "    # Step 1: Validate the input directory path\n",
    "    print(f\"ğŸ“ [VALIDATION] Checking existence of directory: {directory}\")\n",
    "    if not os.path.isdir(directory):\n",
    "        print(f\"âŒ [ERROR] Directory not found: {directory}. Please verify the path in your CONFIG.\")\n",
    "        print(\"===================================================================\")\n",
    "        return []\n",
    "    print(f\"âœ… [SUCCESS] Directory confirmed: {directory}\")\n",
    "\n",
    "    # Step 2: Construct the full search pattern\n",
    "    search_pattern = os.path.join(directory, file_pattern)\n",
    "    print(f\"ğŸ” [SEARCH] Using search pattern: {search_pattern}\")\n",
    "\n",
    "    # Step 3: Discover files using the glob module\n",
    "    excel_files: List[str] = []\n",
    "    try:\n",
    "        excel_files = glob.glob(search_pattern)\n",
    "        print(f\"ğŸ’¡ [RESULT] Glob search completed. Found {len(excel_files)} potential item(s) matching pattern.\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ [ERROR] A critical error occurred during file search with glob: {e}\")\n",
    "        print(\"This could be due to an invalid pattern or system issues. Aborting discovery.\")\n",
    "        print(\"===================================================================\")\n",
    "        return []\n",
    "\n",
    "    # Step 4: File Validation and Reporting\n",
    "    if not excel_files:\n",
    "        print(f\"âš ï¸ [WARN] No Excel files found matching the pattern '{file_pattern}' in '{directory}'.\")\n",
    "        print(\"Possible reasons: Incorrect directory, wrong file pattern, or no such files exist there.\")\n",
    "        print(\"Please double-check the 'data_directory' and 'file_pattern' in your CONFIG settings.\")\n",
    "        print(\"===================================================================\")\n",
    "        return []\n",
    "\n",
    "    found_files_count = len(excel_files)\n",
    "    print(f\"ğŸ“Š [COUNT] Successfully identified {found_files_count} Excel file(s).\")\n",
    "\n",
    "    # Step 5: Validate against the expected file count\n",
    "    if found_files_count != expected_files_count:\n",
    "        print(f\"âš ï¸ [VALIDATION WARN] Expected {expected_files_count} file(s), but actually found {found_files_count}.\")\n",
    "        print(\"This discrepancy might indicate missing data, extra unprocessed files, or an outdated 'expected_files' count in CONFIG.\")\n",
    "    else:\n",
    "        print(f\"âœ… [VALIDATION SUCCESS] File count ({found_files_count}) perfectly matches the expectation ({expected_files_count} files).\")\n",
    "\n",
    "    # Step 6: File Sample Display (with enhanced details)\n",
    "    print(\"\\nğŸ“‹ [INFO] Displaying a sample of discovered files (up to 5 files, sorted alphabetically):\")\n",
    "    \n",
    "    sorted_excel_files = sorted(excel_files) # Sort for consistent output\n",
    "    \n",
    "    MAX_SAMPLES_TO_SHOW = 5 \n",
    "    for i, file_path in enumerate(sorted_excel_files[:MAX_SAMPLES_TO_SHOW]):\n",
    "        try:\n",
    "            file_name = os.path.basename(file_path)\n",
    "            file_size_bytes = os.path.getsize(file_path)\n",
    "            file_size_mb = file_size_bytes / (1024 * 1024)\n",
    "            last_modified_timestamp = os.path.getmtime(file_path)\n",
    "            # Format timestamp to a readable string\n",
    "            last_modified_date = datetime.fromtimestamp(last_modified_timestamp).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            \n",
    "            print(f\"  {i+1:2d}. ğŸ“„ File: {file_name} | Size: {file_size_mb:.2f} MB | Modified: {last_modified_date}\")\n",
    "        except OSError as e:\n",
    "            file_name_fallback = os.path.basename(file_path) \n",
    "            print(f\"  {i+1:2d}. â“ Error accessing metadata for: {file_name_fallback}. Reason: {e}\")\n",
    "\n",
    "    if found_files_count > MAX_SAMPLES_TO_SHOW:\n",
    "        print(f\"     ... and {found_files_count - MAX_SAMPLES_TO_SHOW} more file(s) not listed in this sample.\")\n",
    "    \n",
    "    print(\"\\nğŸ‘ [COMPLETE] Excel file discovery and validation process finished successfully.\")\n",
    "    print(\"===================================================================\")\n",
    "    return sorted_excel_files\n",
    "\n",
    "# --- FUNCTION EXECUTION ---\n",
    "# This part calls the function defined above.\n",
    "# It relies on 'CONFIG' being available from a previous cell.\n",
    "# Imports like 'datetime' are handled at the top of this cell's code.\n",
    "\n",
    "print(\"ğŸš€ Executing the refactored 'discover_excel_files' function using CONFIG values...\")\n",
    "excel_files = discover_excel_files(\n",
    "    directory=CONFIG['data_directory'],\n",
    "    file_pattern=CONFIG['file_pattern'],\n",
    "    expected_files_count=CONFIG['expected_files']\n",
    ")\n",
    "\n",
    "# --- POST-EXECUTION SUMMARY ---\n",
    "# This provides a quick summary of what happened.\n",
    "if excel_files:\n",
    "    print(f\"\\nâœ… [SUMMARY] Successfully discovered and validated {len(excel_files)} Excel file(s).\")\n",
    "    print(f\"ğŸ“ The variable 'excel_files' is now populated with their paths and ready for use in subsequent cells.\")\n",
    "else:\n",
    "    print(\"\\nâŒ [SUMMARY] File discovery did not yield any results or encountered an error during the process.\")\n",
    "    print(\"Please review the detailed logs printed above to diagnose the issue. Check CONFIG and file locations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c90b71f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š ANALYZING FILE STRUCTURES\n",
      "===================================\n",
      "ğŸ“„ Analyzing 1/39: savedrecs1.xls\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ… 1000 rows, 72 columns\n",
      "ğŸ“„ Analyzing 2/39: savedrecs10.xls\n",
      "  âœ… 1000 rows, 72 columns\n",
      "ğŸ“„ Analyzing 3/39: savedrecs11.xls\n",
      "  âœ… 1000 rows, 72 columns\n",
      "ğŸ“„ Analyzing 4/39: savedrecs12.xls\n",
      "  âœ… 1000 rows, 72 columns\n",
      "ğŸ“„ Analyzing 5/39: savedrecs13.xls\n",
      "  âœ… 1000 rows, 72 columns\n",
      "ğŸ“„ Analyzing 6/39: savedrecs14.xls\n",
      "  âœ… 1000 rows, 72 columns\n",
      "ğŸ“„ Analyzing 7/39: savedrecs15.xls\n",
      "  âœ… 1000 rows, 72 columns\n",
      "ğŸ“„ Analyzing 8/39: savedrecs16.xls\n",
      "  âœ… 1000 rows, 72 columns\n",
      "ğŸ“„ Analyzing 9/39: savedrecs17.xls\n",
      "  âœ… 1000 rows, 72 columns\n",
      "ğŸ“„ Analyzing 10/39: savedrecs18.xls\n",
      "  âœ… 1000 rows, 72 columns\n",
      "ğŸ“„ Analyzing 11/39: savedrecs19.xls\n",
      "  âœ… 1000 rows, 72 columns\n",
      "ğŸ“„ Analyzing 12/39: savedrecs2.xls\n",
      "  âœ… 1000 rows, 72 columns\n",
      "ğŸ“„ Analyzing 13/39: savedrecs20.xls\n",
      "  âœ… 1000 rows, 72 columns\n",
      "ğŸ“„ Analyzing 14/39: savedrecs21.xls\n",
      "  âœ… 1000 rows, 72 columns\n",
      "ğŸ“„ Analyzing 15/39: savedrecs22.xls\n",
      "  âœ… 1000 rows, 72 columns\n",
      "ğŸ“„ Analyzing 16/39: savedrecs23.xls\n",
      "  âœ… 1000 rows, 72 columns\n",
      "ğŸ“„ Analyzing 17/39: savedrecs24.xls\n",
      "  âœ… 1000 rows, 72 columns\n",
      "ğŸ“„ Analyzing 18/39: savedrecs25.xls\n",
      "  âœ… 1000 rows, 72 columns\n",
      "ğŸ“„ Analyzing 19/39: savedrecs26.xls\n",
      "  âœ… 1000 rows, 72 columns\n",
      "ğŸ“„ Analyzing 20/39: savedrecs27.xls\n",
      "  âœ… 1000 rows, 72 columns\n",
      "ğŸ“„ Analyzing 21/39: savedrecs28.xls\n",
      "  âœ… 1000 rows, 72 columns\n",
      "ğŸ“„ Analyzing 22/39: savedrecs29.xls\n",
      "  âœ… 1000 rows, 72 columns\n",
      "ğŸ“„ Analyzing 23/39: savedrecs3.xls\n",
      "  âœ… 1000 rows, 72 columns\n",
      "ğŸ“„ Analyzing 24/39: savedrecs30.xls\n",
      "  âœ… 1000 rows, 72 columns\n",
      "ğŸ“„ Analyzing 25/39: savedrecs31.xls\n",
      "  âœ… 1000 rows, 72 columns\n",
      "ğŸ“„ Analyzing 26/39: savedrecs32.xls\n",
      "  âœ… 1000 rows, 72 columns\n",
      "ğŸ“„ Analyzing 27/39: savedrecs33.xls\n",
      "  âœ… 1000 rows, 72 columns\n",
      "ğŸ“„ Analyzing 28/39: savedrecs34.xls\n",
      "  âœ… 1000 rows, 72 columns\n",
      "ğŸ“„ Analyzing 29/39: savedrecs35.xls\n",
      "  âœ… 1000 rows, 72 columns\n",
      "ğŸ“„ Analyzing 30/39: savedrecs36.xls\n",
      "  âœ… 1000 rows, 72 columns\n",
      "ğŸ“„ Analyzing 31/39: savedrecs37.xls\n",
      "  âœ… 1000 rows, 72 columns\n",
      "ğŸ“„ Analyzing 32/39: savedrecs38.xls\n",
      "  âœ… 1000 rows, 72 columns\n",
      "ğŸ“„ Analyzing 33/39: savedrecs39.xls\n",
      "  âœ… 323 rows, 72 columns\n",
      "ğŸ“„ Analyzing 34/39: savedrecs4.xls\n",
      "  âœ… 1000 rows, 72 columns\n",
      "ğŸ“„ Analyzing 35/39: savedrecs5.xls\n",
      "  âœ… 1000 rows, 72 columns\n",
      "ğŸ“„ Analyzing 36/39: savedrecs6.xls\n",
      "  âœ… 1000 rows, 72 columns\n",
      "ğŸ“„ Analyzing 37/39: savedrecs7.xls\n",
      "  âœ… 1000 rows, 72 columns\n",
      "ğŸ“„ Analyzing 38/39: savedrecs8.xls\n",
      "  âœ… 1000 rows, 72 columns\n",
      "ğŸ“„ Analyzing 39/39: savedrecs9.xls\n",
      "  âœ… 1000 rows, 72 columns\n",
      "\n",
      "ğŸ“Š ANALYSIS SUMMARY\n",
      "âœ… Successful files: 39/39\n",
      "ğŸ“‹ Total estimated rows: 38,323\n",
      "âœ… All files have consistent structure\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“Š EXCEL FILE STRUCTURE ANALYSIS\n",
    "def analyze_excel_structure(file_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze the structure of a single Excel file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_excel(file_path, engine='xlrd')\n",
    "        \n",
    "        analysis = {\n",
    "            'file_name': os.path.basename(file_path),\n",
    "            'rows': len(df),\n",
    "            'columns': len(df.columns),\n",
    "            'column_names': list(df.columns),\n",
    "            'success': True,\n",
    "            'error': None\n",
    "        }\n",
    "        \n",
    "        # Check for key WoS columns\n",
    "        key_columns = ['Article Title', 'Abstract', 'Authors', 'Publication Year']\n",
    "        analysis['key_columns_present'] = {col: col in df.columns for col in key_columns}\n",
    "        \n",
    "        return analysis\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'file_name': os.path.basename(file_path),\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "def analyze_all_files(file_paths: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze structure of all Excel files\n",
    "    \"\"\"\n",
    "    print(\"\\nğŸ“Š ANALYZING FILE STRUCTURES\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    analyses = []\n",
    "    successful_files = 0\n",
    "    total_rows = 0\n",
    "    \n",
    "    for i, file_path in enumerate(file_paths):\n",
    "        print(f\"ğŸ“„ Analyzing {i+1}/{len(file_paths)}: {os.path.basename(file_path)}\")\n",
    "        \n",
    "        analysis = analyze_excel_structure(file_path)\n",
    "        analyses.append(analysis)\n",
    "        \n",
    "        if analysis['success']:\n",
    "            successful_files += 1\n",
    "            total_rows += analysis['rows']\n",
    "            print(f\"  âœ… {analysis['rows']} rows, {analysis['columns']} columns\")\n",
    "        else:\n",
    "            print(f\"  âŒ Error: {analysis['error']}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\nğŸ“Š ANALYSIS SUMMARY\")\n",
    "    print(f\"âœ… Successful files: {successful_files}/{len(file_paths)}\")\n",
    "    print(f\"ğŸ“‹ Total estimated rows: {total_rows:,}\")\n",
    "    \n",
    "    # Check structure consistency\n",
    "    if successful_files > 1:\n",
    "        first_success = next(a for a in analyses if a['success'])\n",
    "        consistent_structure = True\n",
    "        \n",
    "        for analysis in analyses:\n",
    "            if analysis['success']:\n",
    "                if set(analysis['column_names']) != set(first_success['column_names']):\n",
    "                    consistent_structure = False\n",
    "                    print(f\"âš ï¸ Structure inconsistency in {analysis['file_name']}\")\n",
    "        \n",
    "        if consistent_structure:\n",
    "            print(\"âœ… All files have consistent structure\")\n",
    "    \n",
    "    return {\n",
    "        'analyses': analyses,\n",
    "        'successful_files': successful_files,\n",
    "        'total_rows': total_rows,\n",
    "        'consistent_structure': consistent_structure if successful_files > 1 else True\n",
    "    }\n",
    "\n",
    "# Analyze files if they were found\n",
    "if excel_files:\n",
    "    structure_analysis = analyze_all_files(excel_files)\n",
    "else:\n",
    "    print(\"âš ï¸ No files to analyze\")\n",
    "    structure_analysis = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8026000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”— CONSOLIDATING EXCEL FILES\n",
      "================================\n",
      "ğŸ“„ Loading 1/39: savedrecs1.xls\n",
      "  âœ… Loaded: 1,000 records\n",
      "ğŸ“„ Loading 2/39: savedrecs10.xls\n",
      "  âœ… Loaded: 1,000 records\n",
      "ğŸ“„ Loading 3/39: savedrecs11.xls\n",
      "  âœ… Loaded: 1,000 records\n",
      "ğŸ“„ Loading 4/39: savedrecs12.xls\n",
      "  âœ… Loaded: 1,000 records\n",
      "ğŸ“„ Loading 5/39: savedrecs13.xls\n",
      "  âœ… Loaded: 1,000 records\n",
      "ğŸ“„ Loading 6/39: savedrecs14.xls\n",
      "  âœ… Loaded: 1,000 records\n",
      "ğŸ“„ Loading 7/39: savedrecs15.xls\n",
      "  âœ… Loaded: 1,000 records\n",
      "ğŸ“„ Loading 8/39: savedrecs16.xls\n",
      "  âœ… Loaded: 1,000 records\n",
      "ğŸ“„ Loading 9/39: savedrecs17.xls\n",
      "  âœ… Loaded: 1,000 records\n",
      "ğŸ“„ Loading 10/39: savedrecs18.xls\n",
      "  âœ… Loaded: 1,000 records\n",
      "ğŸ“„ Loading 11/39: savedrecs19.xls\n",
      "  âœ… Loaded: 1,000 records\n",
      "ğŸ“„ Loading 12/39: savedrecs2.xls\n",
      "  âœ… Loaded: 1,000 records\n",
      "ğŸ“„ Loading 13/39: savedrecs20.xls\n",
      "  âœ… Loaded: 1,000 records\n",
      "ğŸ“„ Loading 14/39: savedrecs21.xls\n",
      "  âœ… Loaded: 1,000 records\n",
      "ğŸ“„ Loading 15/39: savedrecs22.xls\n",
      "  âœ… Loaded: 1,000 records\n",
      "ğŸ“„ Loading 16/39: savedrecs23.xls\n",
      "  âœ… Loaded: 1,000 records\n",
      "ğŸ“„ Loading 17/39: savedrecs24.xls\n",
      "  âœ… Loaded: 1,000 records\n",
      "ğŸ“„ Loading 18/39: savedrecs25.xls\n",
      "  âœ… Loaded: 1,000 records\n",
      "ğŸ“„ Loading 19/39: savedrecs26.xls\n",
      "  âœ… Loaded: 1,000 records\n",
      "ğŸ“„ Loading 20/39: savedrecs27.xls\n",
      "  âœ… Loaded: 1,000 records\n",
      "ğŸ“„ Loading 21/39: savedrecs28.xls\n",
      "  âœ… Loaded: 1,000 records\n",
      "ğŸ“„ Loading 22/39: savedrecs29.xls\n",
      "  âœ… Loaded: 1,000 records\n",
      "ğŸ“„ Loading 23/39: savedrecs3.xls\n",
      "  âœ… Loaded: 1,000 records\n",
      "ğŸ“„ Loading 24/39: savedrecs30.xls\n",
      "  âœ… Loaded: 1,000 records\n",
      "ğŸ“„ Loading 25/39: savedrecs31.xls\n",
      "  âœ… Loaded: 1,000 records\n",
      "ğŸ“„ Loading 26/39: savedrecs32.xls\n",
      "  âœ… Loaded: 1,000 records\n",
      "ğŸ“„ Loading 27/39: savedrecs33.xls\n",
      "  âœ… Loaded: 1,000 records\n",
      "ğŸ“„ Loading 28/39: savedrecs34.xls\n",
      "  âœ… Loaded: 1,000 records\n",
      "ğŸ“„ Loading 29/39: savedrecs35.xls\n",
      "  âœ… Loaded: 1,000 records\n",
      "ğŸ“„ Loading 30/39: savedrecs36.xls\n",
      "  âœ… Loaded: 1,000 records\n",
      "ğŸ“„ Loading 31/39: savedrecs37.xls\n",
      "  âœ… Loaded: 1,000 records\n",
      "ğŸ“„ Loading 32/39: savedrecs38.xls\n",
      "  âœ… Loaded: 1,000 records\n",
      "ğŸ“„ Loading 33/39: savedrecs39.xls\n",
      "  âœ… Loaded: 323 records\n",
      "ğŸ“„ Loading 34/39: savedrecs4.xls\n",
      "  âœ… Loaded: 1,000 records\n",
      "ğŸ“„ Loading 35/39: savedrecs5.xls\n",
      "  âœ… Loaded: 1,000 records\n",
      "ğŸ“„ Loading 36/39: savedrecs6.xls\n",
      "  âœ… Loaded: 1,000 records\n",
      "ğŸ“„ Loading 37/39: savedrecs7.xls\n",
      "  âœ… Loaded: 1,000 records\n",
      "ğŸ“„ Loading 38/39: savedrecs8.xls\n",
      "  âœ… Loaded: 1,000 records\n",
      "ğŸ“„ Loading 39/39: savedrecs9.xls\n",
      "  âœ… Loaded: 1,000 records\n",
      "\n",
      "ğŸ”„ Consolidating 39 DataFrames...\n",
      "\n",
      "âœ… CONSOLIDATION COMPLETE\n",
      "ğŸ“Š Total records: 38,323\n",
      "ğŸ“‹ Total columns: 75\n",
      "ğŸ“ Files processed: 39/39\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”— EXCEL FILES CONSOLIDATION\n",
    "def consolidate_excel_files(file_paths: List[str]) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Consolidate all Excel files into a single DataFrame\n",
    "    \"\"\"\n",
    "    print(\"\\nğŸ”— CONSOLIDATING EXCEL FILES\")\n",
    "    print(\"=\" * 32)\n",
    "    \n",
    "    if not file_paths:\n",
    "        print(\"âŒ No files to consolidate\")\n",
    "        return None\n",
    "    \n",
    "    dataframes = []\n",
    "    total_records = 0\n",
    "    errors = []\n",
    "    \n",
    "    for i, file_path in enumerate(file_paths, 1):\n",
    "        try:\n",
    "            print(f\"ğŸ“„ Loading {i}/{len(file_paths)}: {os.path.basename(file_path)}\")\n",
    "            \n",
    "            # Load Excel file\n",
    "            df_temp = pd.read_excel(file_path, engine='xlrd')\n",
    "            \n",
    "            # Add metadata columns\n",
    "            df_temp['source_file'] = os.path.basename(file_path)\n",
    "            df_temp['batch_number'] = i\n",
    "            df_temp['load_timestamp'] = datetime.now().isoformat()\n",
    "            \n",
    "            dataframes.append(df_temp)\n",
    "            total_records += len(df_temp)\n",
    "            \n",
    "            print(f\"  âœ… Loaded: {len(df_temp):,} records\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error in {os.path.basename(file_path)}: {str(e)}\"\n",
    "            print(f\"  âŒ {error_msg}\")\n",
    "            errors.append(error_msg)\n",
    "    \n",
    "    if not dataframes:\n",
    "        print(\"âŒ No files loaded successfully!\")\n",
    "        return None\n",
    "    \n",
    "    # Consolidate DataFrames\n",
    "    print(f\"\\nğŸ”„ Consolidating {len(dataframes)} DataFrames...\")\n",
    "    consolidated_df = pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "    print(f\"\\nâœ… CONSOLIDATION COMPLETE\")\n",
    "    print(f\"ğŸ“Š Total records: {len(consolidated_df):,}\")\n",
    "    print(f\"ğŸ“‹ Total columns: {len(consolidated_df.columns)}\")\n",
    "    print(f\"ğŸ“ Files processed: {len(dataframes)}/{len(file_paths)}\")\n",
    "    \n",
    "    if errors:\n",
    "        print(f\"\\nâš ï¸ Errors encountered ({len(errors)}):\")\n",
    "        for error in errors[:3]:  # Show first 3 errors\n",
    "            print(f\"  - {error}\")\n",
    "        if len(errors) > 3:\n",
    "            print(f\"  ... and {len(errors) - 3} more errors\")\n",
    "    \n",
    "    return consolidated_df\n",
    "\n",
    "# Consolidate files if available\n",
    "if excel_files and structure_analysis and structure_analysis['successful_files'] > 0:\n",
    "    consolidated_data = consolidate_excel_files(excel_files)\n",
    "else:\n",
    "    print(\"âš ï¸ Prerequisites not met for consolidation\")\n",
    "    consolidated_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c60ef09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ—ºï¸ MAPPING WOS COLUMNS TO STANDARD FORMAT\n",
      "=============================================\n",
      "ğŸ“‹ Column mapping status:\n",
      "  âœ… Article Title â†’ Title\n",
      "  âœ… Abstract â†’ Abstract\n",
      "  âœ… Author Keywords â†’ Author_Keywords\n",
      "  âœ… Keywords Plus â†’ Keywords_Plus\n",
      "  âœ… Authors â†’ Authors\n",
      "  âœ… Publication Year â†’ Year\n",
      "  âœ… Source Title â†’ Journal\n",
      "  âœ… DOI â†’ DOI\n",
      "  âœ… Addresses â†’ Addresses\n",
      "  âœ… Research Areas â†’ Research_Areas\n",
      "  âŒ Web of Science Categories â†’ WoS_Categories (missing)\n",
      "  âŒ Times Cited â†’ Citations (missing)\n",
      "  âœ… Volume â†’ Volume\n",
      "  âœ… Issue â†’ Issue\n",
      "  âŒ Pages â†’ Pages (missing)\n",
      "\n",
      "ğŸ“Š Mapping results:\n",
      "  âœ… Mapped columns: 12\n",
      "  âŒ Missing columns: 3\n",
      "  ğŸ“‹ Total columns after mapping: 75\n"
     ]
    }
   ],
   "source": [
    "# ğŸ—ºï¸ WEB OF SCIENCE COLUMN MAPPING\n",
    "def map_wos_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Map Web of Science columns to standardized format for CenanoInk pipeline\n",
    "    \"\"\"\n",
    "    print(\"\\nğŸ—ºï¸ MAPPING WOS COLUMNS TO STANDARD FORMAT\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Standard column mapping for WoS data\n",
    "    column_mapping = {\n",
    "        'Article Title': 'Title',\n",
    "        'Abstract': 'Abstract',\n",
    "        'Author Keywords': 'Author_Keywords',\n",
    "        'Keywords Plus': 'Keywords_Plus',\n",
    "        'Authors': 'Authors',\n",
    "        'Publication Year': 'Year',\n",
    "        'Source Title': 'Journal',\n",
    "        'DOI': 'DOI',\n",
    "        'Addresses': 'Addresses',\n",
    "        'Research Areas': 'Research_Areas',\n",
    "        'Web of Science Categories': 'WoS_Categories',\n",
    "        'Times Cited': 'Citations',\n",
    "        'Volume': 'Volume',\n",
    "        'Issue': 'Issue',\n",
    "        'Pages': 'Pages'\n",
    "    }\n",
    "    \n",
    "    # Identify available columns\n",
    "    available_columns = list(df.columns)\n",
    "    mapped_columns = {}\n",
    "    missing_columns = []\n",
    "    \n",
    "    print(\"ğŸ“‹ Column mapping status:\")\n",
    "    for wos_col, standard_col in column_mapping.items():\n",
    "        if wos_col in available_columns:\n",
    "            mapped_columns[wos_col] = standard_col\n",
    "            print(f\"  âœ… {wos_col} â†’ {standard_col}\")\n",
    "        else:\n",
    "            missing_columns.append(wos_col)\n",
    "            print(f\"  âŒ {wos_col} â†’ {standard_col} (missing)\")\n",
    "    \n",
    "    # Apply mapping\n",
    "    df_mapped = df.copy()\n",
    "    df_mapped = df_mapped.rename(columns=mapped_columns)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Mapping results:\")\n",
    "    print(f\"  âœ… Mapped columns: {len(mapped_columns)}\")\n",
    "    print(f\"  âŒ Missing columns: {len(missing_columns)}\")\n",
    "    print(f\"  ğŸ“‹ Total columns after mapping: {len(df_mapped.columns)}\")\n",
    "    \n",
    "    return df_mapped\n",
    "\n",
    "# Apply column mapping if data is available\n",
    "if consolidated_data is not None:\n",
    "    mapped_data = map_wos_columns(consolidated_data)\n",
    "else:\n",
    "    print(\"âš ï¸ No consolidated data available for mapping\")\n",
    "    mapped_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07a73ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… DATA QUALITY VALIDATION\n",
      "==============================\n",
      "ğŸ“‹ Essential columns status:\n",
      "  âœ… Title: 38,323 records (100.0%)\n",
      "  âœ… Abstract: 38,237 records (99.8%)\n",
      "  âœ… Authors: 38,323 records (100.0%)\n",
      "  âœ… Year: 38,323 records (100.0%)\n",
      "\n",
      "ğŸ“ Abstract Quality:\n",
      "  With abstract: 38,237 (99.8%)\n",
      "  Valid abstracts (>50 chars): 38,237 (99.8%)\n",
      "  Average length: 1266 characters\n",
      "\n",
      "ğŸ·ï¸ Title Quality:\n",
      "  With title: 38,323 (100.0%)\n",
      "  Valid titles (>10 chars): 38,323 (100.0%)\n",
      "  Average length: 110 characters\n",
      "\n",
      "ğŸ“… Year Distribution:\n",
      "  Period: 2014 - 2025\n",
      "  Unique years: 12\n",
      "  Top years:\n",
      "    2014: 2,192 articles\n",
      "    2015: 2,384 articles\n",
      "    2016: 2,622 articles\n",
      "\n",
      "ğŸ”¬ Nanotechnology Content:\n",
      "  nano: 21,792 mentions\n",
      "  nanoparticle: 6,424 mentions\n",
      "  nanotechnology: 110 mentions\n",
      "  nanomaterial: 558 mentions\n",
      "  coating: 46,224 mentions\n",
      "  paint: 1,814 mentions\n"
     ]
    }
   ],
   "source": [
    "# âœ… DATA QUALITY VALIDATION\n",
    "def validate_data_quality(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Comprehensive data quality validation for the consolidated dataset\n",
    "    \"\"\"\n",
    "    print(\"\\nâœ… DATA QUALITY VALIDATION\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    validation = {\n",
    "        'total_records': len(df),\n",
    "        'total_columns': len(df.columns),\n",
    "        'essential_columns': {},\n",
    "        'abstract_quality': {},\n",
    "        'title_quality': {},\n",
    "        'year_distribution': {},\n",
    "        'nano_content': {}\n",
    "    }\n",
    "    \n",
    "    # Check essential columns\n",
    "    essential_cols = ['Title', 'Abstract', 'Authors', 'Year']\n",
    "    print(\"ğŸ“‹ Essential columns status:\")\n",
    "    \n",
    "    for col in essential_cols:\n",
    "        if col in df.columns:\n",
    "            non_null = df[col].notna().sum()\n",
    "            percentage = (non_null / len(df)) * 100\n",
    "            validation['essential_columns'][col] = {\n",
    "                'present': True,\n",
    "                'non_null': non_null,\n",
    "                'percentage': percentage\n",
    "            }\n",
    "            print(f\"  âœ… {col}: {non_null:,} records ({percentage:.1f}%)\")\n",
    "        else:\n",
    "            validation['essential_columns'][col] = {'present': False}\n",
    "            print(f\"  âŒ {col}: Not found\")\n",
    "    \n",
    "    # Abstract quality analysis\n",
    "    if 'Abstract' in df.columns:\n",
    "        valid_abstracts = df['Abstract'].notna() & (df['Abstract'].str.len() > 50)\n",
    "        validation['abstract_quality'] = {\n",
    "            'total': len(df),\n",
    "            'with_abstract': df['Abstract'].notna().sum(),\n",
    "            'valid_abstracts': valid_abstracts.sum(),\n",
    "            'avg_length': df['Abstract'].str.len().mean()\n",
    "        }\n",
    "        \n",
    "        aq = validation['abstract_quality']\n",
    "        print(f\"\\nğŸ“ Abstract Quality:\")\n",
    "        print(f\"  With abstract: {aq['with_abstract']:,} ({(aq['with_abstract']/aq['total'])*100:.1f}%)\")\n",
    "        print(f\"  Valid abstracts (>50 chars): {aq['valid_abstracts']:,} ({(aq['valid_abstracts']/aq['total'])*100:.1f}%)\")\n",
    "        print(f\"  Average length: {aq['avg_length']:.0f} characters\")\n",
    "    \n",
    "    # Title quality analysis\n",
    "    if 'Title' in df.columns:\n",
    "        valid_titles = df['Title'].notna() & (df['Title'].str.len() > 10)\n",
    "        validation['title_quality'] = {\n",
    "            'total': len(df),\n",
    "            'with_title': df['Title'].notna().sum(),\n",
    "            'valid_titles': valid_titles.sum(),\n",
    "            'avg_length': df['Title'].str.len().mean()\n",
    "        }\n",
    "        \n",
    "        tq = validation['title_quality']\n",
    "        print(f\"\\nğŸ·ï¸ Title Quality:\")\n",
    "        print(f\"  With title: {tq['with_title']:,} ({(tq['with_title']/tq['total'])*100:.1f}%)\")\n",
    "        print(f\"  Valid titles (>10 chars): {tq['valid_titles']:,} ({(tq['valid_titles']/tq['total'])*100:.1f}%)\")\n",
    "        print(f\"  Average length: {tq['avg_length']:.0f} characters\")\n",
    "    \n",
    "    # Year distribution\n",
    "    if 'Year' in df.columns:\n",
    "        years = pd.to_numeric(df['Year'], errors='coerce')\n",
    "        year_dist = years.value_counts().sort_index()\n",
    "        validation['year_distribution'] = {\n",
    "            'min_year': int(years.min()) if years.notna().any() else None,\n",
    "            'max_year': int(years.max()) if years.notna().any() else None,\n",
    "            'unique_years': len(years.unique()),\n",
    "            'top_years': year_dist.head(5).to_dict()\n",
    "        }\n",
    "        \n",
    "        yd = validation['year_distribution']\n",
    "        if yd['min_year']:\n",
    "            print(f\"\\nğŸ“… Year Distribution:\")\n",
    "            print(f\"  Period: {yd['min_year']} - {yd['max_year']}\")\n",
    "            print(f\"  Unique years: {yd['unique_years']}\")\n",
    "            print(f\"  Top years:\")\n",
    "            for year, count in list(yd['top_years'].items())[:3]:\n",
    "                print(f\"    {year}: {count:,} articles\")\n",
    "    \n",
    "    # Nanotechnology content analysis\n",
    "    nano_terms = ['nano', 'nanoparticle', 'nanotechnology', 'nanomaterial', 'coating', 'paint']\n",
    "    nano_counts = {}\n",
    "    \n",
    "    for term in nano_terms:\n",
    "        count_total = 0\n",
    "        for col in ['Title', 'Abstract']:\n",
    "            if col in df.columns:\n",
    "                count = df[col].astype(str).str.lower().str.contains(term, na=False).sum()\n",
    "                count_total += count\n",
    "        nano_counts[term] = count_total\n",
    "    \n",
    "    validation['nano_content'] = nano_counts\n",
    "    \n",
    "    print(f\"\\nğŸ”¬ Nanotechnology Content:\")\n",
    "    for term, count in nano_counts.items():\n",
    "        if count > 0:\n",
    "            print(f\"  {term}: {count:,} mentions\")\n",
    "    \n",
    "    return validation\n",
    "\n",
    "# Validate data quality if available\n",
    "if mapped_data is not None:\n",
    "    data_validation = validate_data_quality(mapped_data)\n",
    "else:\n",
    "    print(\"âš ï¸ No mapped data available for validation\")\n",
    "    data_validation = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7040074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ EXPORTING CONSOLIDATED DATASET\n",
      "===================================\n",
      "ğŸ“ Exporting to: /home/delon/Modelos/modeloCenanoInk/data/processed/cenanoink_wos_consolidated_20250610_121227.csv\n",
      "âœ… Dataset exported: 38,323 records, 125.4 MB\n",
      "ğŸ“Š Metadata exported: /home/delon/Modelos/modeloCenanoInk/data/processed/cenanoink_wos_metadata_20250610_121227.json\n",
      "\n",
      "ğŸ‰ DATA LOADING COMPLETE!\n",
      "ğŸ“„ Ready for analysis: /home/delon/Modelos/modeloCenanoInk/data/processed/cenanoink_wos_consolidated_20250610_121227.csv\n",
      "ğŸ“Š Next step: Use '02_nanomaterials_database.ipynb' for pattern analysis\n"
     ]
    }
   ],
   "source": [
    "# ğŸ’¾ EXPORT CONSOLIDATED DATASET\n",
    "import numpy as np\n",
    "def export_consolidated_data(df: pd.DataFrame, \n",
    "                           output_dir: str = None,\n",
    "                           include_metadata: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Export consolidated dataset to CSV with metadata\n",
    "    \"\"\"\n",
    "    print(\"\\nğŸ’¾ EXPORTING CONSOLIDATED DATASET\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Initialize exported_file_path to None at the beginning of the function\n",
    "    # This ensures it has a value even if the export fails early.\n",
    "    exported_file_path = None\n",
    "    \n",
    "    if output_dir is None:\n",
    "        output_dir = CONFIG['output_directory']\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Generate filename with timestamp\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    filename = f'cenanoink_wos_consolidated_{timestamp}.csv'\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    \n",
    "    # Export main dataset\n",
    "    print(f\"ğŸ“ Exporting to: {filepath}\")\n",
    "    df.to_csv(filepath, index=False, encoding='utf-8')\n",
    "    \n",
    "    file_size = os.path.getsize(filepath) / (1024 * 1024)  # MB\n",
    "    print(f\"âœ… Dataset exported: {len(df):,} records, {file_size:.1f} MB\")\n",
    "    \n",
    "    # Export metadata if requested\n",
    "    if include_metadata and data_validation:\n",
    "        metadata_filename = f'cenanoink_wos_metadata_{timestamp}.json'\n",
    "        metadata_filepath = os.path.join(output_dir, metadata_filename)\n",
    "        \n",
    "        import json\n",
    "        # Ensure convert_numpy_types is defined or imported if needed,\n",
    "        # or handle numpy types directly if they appear in 'data_validation'\n",
    "        \n",
    "        # Helper function to convert NumPy types to native Python types for JSON serialization\n",
    "        def convert_numpy_types(obj):\n",
    "            if isinstance(obj, (np.integer, np.int64, np.int32)):\n",
    "                return int(obj)\n",
    "            elif isinstance(obj, (np.floating, np.float64, np.float32)):\n",
    "                return float(obj)\n",
    "            elif isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "            elif isinstance(obj, dict):\n",
    "                return {k: convert_numpy_types(v) for k, v in obj.items()}\n",
    "            elif isinstance(obj, list):\n",
    "                return [convert_numpy_types(i) for i in obj]\n",
    "            return obj\n",
    "\n",
    "        metadata = {\n",
    "            'export_timestamp': datetime.now().isoformat(),\n",
    "            'source_files_count': len(excel_files) if excel_files else 0,\n",
    "            'total_records': len(df),\n",
    "            'total_columns': len(df.columns),\n",
    "            'data_validation': convert_numpy_types(data_validation), # Apply conversion here\n",
    "            'column_names': list(df.columns)\n",
    "        }\n",
    "        \n",
    "        with open(metadata_filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"ğŸ“Š Metadata exported: {metadata_filepath}\")\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "# Export data if available\n",
    "if mapped_data is not None:\n",
    "    # This is where exported_file was defined.\n",
    "    # It's good practice to ensure it's defined in all paths.\n",
    "    exported_file = export_consolidated_data(mapped_data)\n",
    "    print(f\"\\nğŸ‰ DATA LOADING COMPLETE!\")\n",
    "    print(f\"ğŸ“„ Ready for analysis: {exported_file}\")\n",
    "    print(f\"ğŸ“Š Next step: Use '02_nanomaterials_database.ipynb' for pattern analysis\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ No data available for export\")\n",
    "    print(\"ğŸ”§ Check file paths and permissions\")\n",
    "    exported_file = None # Initialize exported_file to None if export is skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "810f23e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ“‹ CENANOINK DATA LOADER SUMMARY\n",
      "============================================================\n",
      "âœ… Successfully processed 39 Excel files\n",
      "ğŸ“Š Total records loaded: 38,323\n",
      "ğŸ“‹ Columns available: 75\n",
      "ğŸ’¾ Data exported to: /home/delon/Modelos/modeloCenanoInk/data/processed/cenanoink_wos_consolidated_20250610_121227.csv\n",
      "ğŸ“ Valid abstracts: 38,237\n",
      "ğŸ”¬ Nanotechnology mentions: 76,922\n",
      "\n",
      "ğŸ¯ NEXT STEPS:\n",
      "  1. Run '02_nanomaterials_database.ipynb' for pattern extraction\n",
      "  2. Use '03_gemini_analysis.ipynb' for AI analysis\n",
      "  3. Generate reports with '04_reporting_system.ipynb'\n",
      "  4. Orchestrate full pipeline with '05_main_orchestrator.ipynb'\n",
      "\n",
      "ğŸ¨ CenanoInk Project - Nanotechnology Research Pipeline\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“‹ SUMMARY AND NEXT STEPS\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“‹ CENANOINK DATA LOADER SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if excel_files and mapped_data is not None:\n",
    "    print(f\"âœ… Successfully processed {len(excel_files)} Excel files\")\n",
    "    print(f\"ğŸ“Š Total records loaded: {len(mapped_data):,}\")\n",
    "    print(f\"ğŸ“‹ Columns available: {len(mapped_data.columns)}\")\n",
    "    \n",
    "    # Conditionally print the exported file path\n",
    "    if exported_file:\n",
    "        print(f\"ğŸ’¾ Data exported to: {exported_file}\")\n",
    "    else:\n",
    "        print(\"ğŸ’¾ Data export was skipped or did not produce a file path.\")\n",
    "    \n",
    "    # Key statistics\n",
    "    if data_validation:\n",
    "        if 'abstract_quality' in data_validation and data_validation['abstract_quality']:\n",
    "            valid_abstracts = data_validation['abstract_quality']['valid_abstracts']\n",
    "            print(f\"ğŸ“ Valid abstracts: {valid_abstracts:,}\")\n",
    "        \n",
    "        if 'nano_content' in data_validation:\n",
    "            nano_total = sum(data_validation['nano_content'].values())\n",
    "            print(f\"ğŸ”¬ Nanotechnology mentions: {nano_total:,}\")\n",
    "    \n",
    "    print(\"\\nğŸ¯ NEXT STEPS:\")\n",
    "    print(\"  1. Run '02_nanomaterials_database.ipynb' for pattern extraction\")\n",
    "    print(\"  2. Use '03_gemini_analysis.ipynb' for AI analysis\") # This might need updating based on new notebook names\n",
    "    print(\"  3. Generate reports with '04_reporting_system.ipynb'\") # This might need updating\n",
    "    print(\"  4. Orchestrate full pipeline with '05_main_orchestrator.ipynb'\") # This might need updating\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ Data loading failed or was incomplete.\")\n",
    "    # If mapped_data is None, it implies earlier stages might have issues.\n",
    "    # exported_file would be None due to the change in the previous cell.\n",
    "    if not excel_files:\n",
    "        print(\"  - No Excel files were found or processed.\")\n",
    "    if mapped_data is None and excel_files: # Files found, but mapping/consolidation failed\n",
    "        print(\"  - Data consolidation or mapping might have failed.\")\n",
    "        \n",
    "    print(\"\\nğŸ”§ TROUBLESHOOTING:\")\n",
    "    print(\"  1. Check if Excel files exist in the data directory and match the pattern.\")\n",
    "    print(\"  2. Verify file permissions and that files are not corrupt.\")\n",
    "    print(\"  3. Ensure 'xlrd' library is installed if using .xls files.\")\n",
    "    print(\"  4. Review logs in previous cells for specific error messages.\")\n",
    "    print(\"  5. Check available disk space if export is failing.\")\n",
    "\n",
    "print(\"\\nğŸ¨ CenanoInk Project - Nanotechnology Research Pipeline\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
