{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd026831",
   "metadata": {},
   "source": [
    "# ðŸ“Š 02 - Carregamento e Processamento de Dados\n",
    "\n",
    "## ðŸ“– VisÃ£o Geral\n",
    "\n",
    "Este notebook Ã© responsÃ¡vel pelo carregamento, consolidaÃ§Ã£o e preprocessamento dos dados do Web of Science em formato Excel.\n",
    "\n",
    "### ðŸŽ¯ Responsabilidades\n",
    "\n",
    "- âœ… Carregamento de 39 arquivos Excel do Web of Science\n",
    "- âœ… ConsolidaÃ§Ã£o em dataset Ãºnico\n",
    "- âœ… Mapeamento e padronizaÃ§Ã£o de colunas\n",
    "- âœ… ValidaÃ§Ã£o e limpeza bÃ¡sica dos dados\n",
    "- âœ… AnÃ¡lise exploratÃ³ria inicial\n",
    "- âœ… GeraÃ§Ã£o de estatÃ­sticas de qualidade\n",
    "\n",
    "### ðŸ“¦ DependÃªncias\n",
    "\n",
    "- pandas\n",
    "- xlrd (para arquivos .xls)\n",
    "- numpy\n",
    "- pathlib\n",
    "\n",
    "### ðŸ”— Notebooks Relacionados\n",
    "\n",
    "- **Anterior**: `01_configuracao_sistema.ipynb`\n",
    "- **PrÃ³ximo**: `03_analise_regex.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3219aa4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ConfiguraÃ§Ãµes carregadas do notebook anterior\n",
      "   Sistema configurado: True\n",
      "ðŸ“ DiretÃ³rio de dados: /home/delon/Modelos/modeloCenanoInk/data/raw/arquivos_excel_artigo_cienciometrico/\n"
     ]
    }
   ],
   "source": [
    "# ðŸ“¦ Imports e carregamento de configuraÃ§Ãµes\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "# Carregar configuraÃ§Ãµes do sistema\n",
    "config_file = 'config_sistema.json'\n",
    "if os.path.exists(config_file):\n",
    "    with open(config_file, 'r', encoding='utf-8') as f:\n",
    "        config_data = json.load(f)\n",
    "    \n",
    "    SISTEMA_CONFIGURADO = config_data.get('sistema_configurado', False)\n",
    "    PATHS = config_data.get('paths', {})\n",
    "    CONFIG = config_data.get('config', {})\n",
    "    \n",
    "    print(\"âœ… ConfiguraÃ§Ãµes carregadas do notebook anterior\")\n",
    "    print(f\"   Sistema configurado: {SISTEMA_CONFIGURADO}\")\n",
    "else:\n",
    "    print(\"âŒ Arquivo de configuraÃ§Ã£o nÃ£o encontrado!\")\n",
    "    print(\"ðŸ’¡ Execute primeiro o notebook '01_configuracao_sistema.ipynb'\")\n",
    "    raise FileNotFoundError(\"ConfiguraÃ§Ã£o do sistema necessÃ¡ria\")\n",
    "\n",
    "# Verificar se hÃ¡ arquivos Excel\n",
    "excel_dir = PATHS.get('excel_files', '/home/delon/Modelos/modeloCenanoInk/data/raw/arquivos_excel_artigo_cienciometrico/')\n",
    "print(f\"ðŸ“ DiretÃ³rio de dados: {excel_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4655d462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” EXPLORANDO ARQUIVOS EXCEL\n",
      "===================================\n",
      "ðŸ“Š Total de arquivos encontrados: 39\n",
      "\n",
      "ðŸ“‹ Analisando estrutura dos primeiros 3 arquivos:\n",
      "\n",
      "ðŸ“„ Arquivo 1: savedrecs1.xls (4.3 MB)\n",
      "   ðŸ“Š DimensÃµes: 1,000 linhas Ã— 72 colunas\n",
      "   ðŸ“‹ Primeiras colunas: Publication Type, Authors, Book Authors, Book Editors, Book Group Authors...\n",
      "   âœ… Colunas WoS encontradas: 4/4\n",
      "\n",
      "ðŸ“„ Arquivo 2: savedrecs10.xls (4.3 MB)\n",
      "   ðŸ“Š DimensÃµes: 1,000 linhas Ã— 72 colunas\n",
      "   ðŸ“‹ Primeiras colunas: Publication Type, Authors, Book Authors, Book Editors, Book Group Authors...\n",
      "   âœ… Colunas WoS encontradas: 4/4\n",
      "\n",
      "ðŸ“„ Arquivo 3: savedrecs11.xls (4.3 MB)\n",
      "   ðŸ“Š DimensÃµes: 1,000 linhas Ã— 72 colunas\n",
      "   ðŸ“‹ Primeiras colunas: Publication Type, Authors, Book Authors, Book Editors, Book Group Authors...\n",
      "   âœ… Colunas WoS encontradas: 4/4\n",
      "\n",
      "ðŸ“Š Estimativas totais:\n",
      "   ðŸ“ˆ Registros por arquivo: ~1,000\n",
      "   ðŸ“ˆ Total estimado de registros: ~39,000\n",
      "   ðŸ’¾ Tamanho total: ~12.8 MB\n",
      "   ðŸ“Š DimensÃµes: 1,000 linhas Ã— 72 colunas\n",
      "   ðŸ“‹ Primeiras colunas: Publication Type, Authors, Book Authors, Book Editors, Book Group Authors...\n",
      "   âœ… Colunas WoS encontradas: 4/4\n",
      "\n",
      "ðŸ“„ Arquivo 3: savedrecs11.xls (4.3 MB)\n",
      "   ðŸ“Š DimensÃµes: 1,000 linhas Ã— 72 colunas\n",
      "   ðŸ“‹ Primeiras colunas: Publication Type, Authors, Book Authors, Book Editors, Book Group Authors...\n",
      "   âœ… Colunas WoS encontradas: 4/4\n",
      "\n",
      "ðŸ“Š Estimativas totais:\n",
      "   ðŸ“ˆ Registros por arquivo: ~1,000\n",
      "   ðŸ“ˆ Total estimado de registros: ~39,000\n",
      "   ðŸ’¾ Tamanho total: ~12.8 MB\n"
     ]
    }
   ],
   "source": [
    "# ðŸ” ExploraÃ§Ã£o inicial dos arquivos Excel\n",
    "\n",
    "def explorar_arquivos_excel(diretorio: str) -> Dict[str, any]:\n",
    "    \"\"\"Explora os arquivos Excel disponÃ­veis e retorna estatÃ­sticas\"\"\"\n",
    "    \n",
    "    print(\"ðŸ” EXPLORANDO ARQUIVOS EXCEL\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Buscar arquivos Excel\n",
    "    excel_files = glob.glob(os.path.join(diretorio, '*.xls'))\n",
    "    \n",
    "    if not excel_files:\n",
    "        print(f\"âŒ Nenhum arquivo Excel encontrado em: {diretorio}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"ðŸ“Š Total de arquivos encontrados: {len(excel_files)}\")\n",
    "    \n",
    "    # Analisar alguns arquivos para entender estrutura\n",
    "    exploracao = {\n",
    "        'total_arquivos': len(excel_files),\n",
    "        'arquivos': [],\n",
    "        'tamanhos': [],\n",
    "        'estruturas': [],\n",
    "        'total_estimado_registros': 0\n",
    "    }\n",
    "    \n",
    "    print(\"\\nðŸ“‹ Analisando estrutura dos primeiros 3 arquivos:\")\n",
    "    \n",
    "    for i, arquivo_path in enumerate(sorted(excel_files)[:3]):\n",
    "        nome_arquivo = os.path.basename(arquivo_path)\n",
    "        tamanho_mb = os.path.getsize(arquivo_path) / (1024 * 1024)\n",
    "        \n",
    "        print(f\"\\nðŸ“„ Arquivo {i+1}: {nome_arquivo} ({tamanho_mb:.1f} MB)\")\n",
    "        \n",
    "        try:\n",
    "            # Tentar ler o arquivo\n",
    "            df_temp = pd.read_excel(arquivo_path, engine='xlrd')\n",
    "            \n",
    "            estrutura = {\n",
    "                'arquivo': nome_arquivo,\n",
    "                'linhas': len(df_temp),\n",
    "                'colunas': len(df_temp.columns),\n",
    "                'tamanho_mb': tamanho_mb,\n",
    "                'colunas_principais': list(df_temp.columns[:10])\n",
    "            }\n",
    "            \n",
    "            exploracao['arquivos'].append(nome_arquivo)\n",
    "            exploracao['tamanhos'].append(tamanho_mb)\n",
    "            exploracao['estruturas'].append(estrutura)\n",
    "            \n",
    "            print(f\"   ðŸ“Š DimensÃµes: {len(df_temp):,} linhas Ã— {len(df_temp.columns)} colunas\")\n",
    "            print(f\"   ðŸ“‹ Primeiras colunas: {', '.join(df_temp.columns[:5])}...\")\n",
    "            \n",
    "            # Verificar colunas essenciais do Web of Science\n",
    "            colunas_wos = ['Article Title', 'Abstract', 'Authors', 'Publication Year']\n",
    "            colunas_encontradas = [col for col in colunas_wos if col in df_temp.columns]\n",
    "            print(f\"   âœ… Colunas WoS encontradas: {len(colunas_encontradas)}/{len(colunas_wos)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Erro ao ler arquivo: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Estimativa total\n",
    "    if exploracao['estruturas']:\n",
    "        media_linhas = np.mean([est['linhas'] for est in exploracao['estruturas']])\n",
    "        exploracao['total_estimado_registros'] = int(media_linhas * len(excel_files))\n",
    "        \n",
    "        print(f\"\\nðŸ“Š Estimativas totais:\")\n",
    "        print(f\"   ðŸ“ˆ Registros por arquivo: ~{media_linhas:,.0f}\")\n",
    "        print(f\"   ðŸ“ˆ Total estimado de registros: ~{exploracao['total_estimado_registros']:,}\")\n",
    "        print(f\"   ðŸ’¾ Tamanho total: ~{sum(exploracao['tamanhos']):.1f} MB\")\n",
    "    \n",
    "    return exploracao\n",
    "\n",
    "# Executar exploraÃ§Ã£o\n",
    "exploracao_inicial = explorar_arquivos_excel(excel_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2920e9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Iniciando carregamento dos dados...\n",
      "ðŸ“Š CARREGANDO ARQUIVOS EXCEL WOS\n",
      "========================================\n",
      "ðŸ“ Encontrados 39 arquivos Excel\n",
      "\n",
      "ðŸ“„ Carregando arquivo 1/39: savedrecs26.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 2/39: savedrecs20.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 2/39: savedrecs20.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 3/39: savedrecs3.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 3/39: savedrecs3.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 4/39: savedrecs6.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 4/39: savedrecs6.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 5/39: savedrecs22.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 5/39: savedrecs22.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 6/39: savedrecs18.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 6/39: savedrecs18.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 7/39: savedrecs5.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 7/39: savedrecs5.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 8/39: savedrecs29.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 8/39: savedrecs29.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 9/39: savedrecs24.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 9/39: savedrecs24.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 10/39: savedrecs37.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 10/39: savedrecs37.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 11/39: savedrecs11.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 11/39: savedrecs11.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 12/39: savedrecs30.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 13/39: savedrecs13.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 12/39: savedrecs30.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 13/39: savedrecs13.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 14/39: savedrecs14.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 15/39: savedrecs21.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 14/39: savedrecs14.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 15/39: savedrecs21.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 16/39: savedrecs38.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 17/39: savedrecs10.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 16/39: savedrecs38.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 17/39: savedrecs10.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 18/39: savedrecs7.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 19/39: savedrecs19.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 18/39: savedrecs7.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 19/39: savedrecs19.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 20/39: savedrecs1.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 21/39: savedrecs4.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 20/39: savedrecs1.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 21/39: savedrecs4.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 22/39: savedrecs35.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 23/39: savedrecs33.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 22/39: savedrecs35.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 23/39: savedrecs33.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 24/39: savedrecs27.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 25/39: savedrecs12.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 24/39: savedrecs27.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 25/39: savedrecs12.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 26/39: savedrecs15.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 27/39: savedrecs17.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 26/39: savedrecs15.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 27/39: savedrecs17.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 28/39: savedrecs39.xls\n",
      "  âœ… Carregado: 323 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 29/39: savedrecs28.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 30/39: savedrecs23.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 28/39: savedrecs39.xls\n",
      "  âœ… Carregado: 323 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 29/39: savedrecs28.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 30/39: savedrecs23.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 31/39: savedrecs25.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 32/39: savedrecs32.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 31/39: savedrecs25.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 32/39: savedrecs32.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 33/39: savedrecs36.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 34/39: savedrecs31.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 33/39: savedrecs36.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 34/39: savedrecs31.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 35/39: savedrecs16.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 36/39: savedrecs8.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 35/39: savedrecs16.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 36/39: savedrecs8.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 37/39: savedrecs34.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 37/39: savedrecs34.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 38/39: savedrecs2.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 38/39: savedrecs2.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 39/39: savedrecs9.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ“„ Carregando arquivo 39/39: savedrecs9.xls\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ”„ Consolidando 39 DataFrames...\n",
      "  âœ… Carregado: 1,000 registros\n",
      "\n",
      "ðŸ”„ Consolidando 39 DataFrames...\n",
      "\n",
      "âœ… CONSOLIDAÃ‡ÃƒO CONCLUÃDA\n",
      "ðŸ“Š Total de registros: 38,323\n",
      "ðŸ“‹ Total de colunas: 75\n",
      "ðŸ“ Arquivos processados: 39/39\n",
      "\n",
      "ðŸŽ‰ Dados carregados com sucesso!\n",
      "ðŸ“Š Dataset shape: (38323, 75)\n",
      "\n",
      "âœ… CONSOLIDAÃ‡ÃƒO CONCLUÃDA\n",
      "ðŸ“Š Total de registros: 38,323\n",
      "ðŸ“‹ Total de colunas: 75\n",
      "ðŸ“ Arquivos processados: 39/39\n",
      "\n",
      "ðŸŽ‰ Dados carregados com sucesso!\n",
      "ðŸ“Š Dataset shape: (38323, 75)\n"
     ]
    }
   ],
   "source": [
    "# ðŸ“¥ FunÃ§Ã£o de carregamento consolidado\n",
    "\n",
    "def carregar_arquivos_excel_wos(diretorio_excel: str = None) -> pd.DataFrame:\n",
    "    \"\"\"Carrega todos os arquivos Excel do Web of Science e consolida em um DataFrame Ãºnico\"\"\"\n",
    "    \n",
    "    if diretorio_excel is None:\n",
    "        diretorio_excel = excel_dir\n",
    "    \n",
    "    print(\"ðŸ“Š CARREGANDO ARQUIVOS EXCEL WOS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Buscar todos os arquivos Excel\n",
    "    excel_files = glob.glob(os.path.join(diretorio_excel, '*.xls'))\n",
    "    \n",
    "    if not excel_files:\n",
    "        print(\"âŒ Nenhum arquivo Excel encontrado!\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"ðŸ“ Encontrados {len(excel_files)} arquivos Excel\")\n",
    "    \n",
    "    dataframes = []\n",
    "    total_records = 0\n",
    "    errors = []\n",
    "    \n",
    "    for i, file_path in enumerate(excel_files, 1):\n",
    "        try:\n",
    "            print(f\"\\nðŸ“„ Carregando arquivo {i}/{len(excel_files)}: {os.path.basename(file_path)}\")\n",
    "            \n",
    "            # Carregar arquivo Excel\n",
    "            df_temp = pd.read_excel(file_path, engine='xlrd')\n",
    "            \n",
    "            # Adicionar informaÃ§Ãµes de origem\n",
    "            df_temp['arquivo_origem'] = os.path.basename(file_path)\n",
    "            df_temp['batch_numero'] = i\n",
    "            df_temp['data_carregamento'] = datetime.now().isoformat()\n",
    "            \n",
    "            print(f\"  âœ… Carregado: {len(df_temp):,} registros\")\n",
    "            \n",
    "            dataframes.append(df_temp)\n",
    "            total_records += len(df_temp)\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Erro no arquivo {os.path.basename(file_path)}: {str(e)}\"\n",
    "            print(f\"  âŒ {error_msg}\")\n",
    "            errors.append(error_msg)\n",
    "    \n",
    "    if not dataframes:\n",
    "        print(\"âŒ Nenhum arquivo foi carregado com sucesso!\")\n",
    "        return None\n",
    "    \n",
    "    # Consolidar todos os DataFrames\n",
    "    print(f\"\\nðŸ”„ Consolidando {len(dataframes)} DataFrames...\")\n",
    "    df_consolidado = pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "    print(f\"\\nâœ… CONSOLIDAÃ‡ÃƒO CONCLUÃDA\")\n",
    "    print(f\"ðŸ“Š Total de registros: {len(df_consolidado):,}\")\n",
    "    print(f\"ðŸ“‹ Total de colunas: {len(df_consolidado.columns)}\")\n",
    "    print(f\"ðŸ“ Arquivos processados: {len(dataframes)}/{len(excel_files)}\")\n",
    "    \n",
    "    if errors:\n",
    "        print(f\"\\nâš ï¸ Erros encontrados ({len(errors)}):\")\n",
    "        for error in errors[:3]:  # Mostrar apenas os primeiros 3 erros\n",
    "            print(f\"  - {error}\")\n",
    "        if len(errors) > 3:\n",
    "            print(f\"  ... e mais {len(errors) - 3} erros\")\n",
    "    \n",
    "    return df_consolidado\n",
    "\n",
    "# Executar carregamento\n",
    "print(\"ðŸš€ Iniciando carregamento dos dados...\")\n",
    "df_raw = carregar_arquivos_excel_wos()\n",
    "\n",
    "if df_raw is not None:\n",
    "    print(f\"\\nðŸŽ‰ Dados carregados com sucesso!\")\n",
    "    print(f\"ðŸ“Š Dataset shape: {df_raw.shape}\")\n",
    "else:\n",
    "    print(\"âŒ Falha no carregamento dos dados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd0f149e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Pulando filtro de Open Access - dados nÃ£o disponÃ­veis\n"
     ]
    }
   ],
   "source": [
    "# ðŸ” Filtro de registros por acesso aberto (Open Access)\n",
    "\n",
    "def filtrar_registros_open_access(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filtra os registros que possuem classificaÃ§Ã£o de Open Access\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame com dados processados\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame contendo apenas registros com classificaÃ§Ã£o de Open Access\n",
    "    \"\"\"\n",
    "    print(\"\\nðŸ”’ FILTRANDO REGISTROS POR OPEN ACCESS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    df_filtrado = df.copy()\n",
    "    \n",
    "    # Verificar se a coluna Open Access estÃ¡ disponÃ­vel\n",
    "    coluna_oa = None\n",
    "    possiveis_colunas = ['Open Access Designations', 'Open Access', 'OA', 'Acesso Aberto']\n",
    "    \n",
    "    for col in possiveis_colunas:\n",
    "        if col in df_filtrado.columns:\n",
    "            coluna_oa = col\n",
    "            break\n",
    "    \n",
    "    if coluna_oa is None:\n",
    "        print(\"âš ï¸ Nenhuma coluna de Open Access encontrada, utilizando dados sem filtro\")\n",
    "        # Adicionar flag mesmo assim para manter consistÃªncia\n",
    "        df_filtrado['filtrado_open_access'] = False\n",
    "        return df_filtrado\n",
    "    \n",
    "    # Filtrar registros com classificaÃ§Ã£o de Open Access (valores nÃ£o vazios)\n",
    "    registros_iniciais = len(df_filtrado)\n",
    "    df_filtrado = df_filtrado[df_filtrado[coluna_oa].notna()]\n",
    "    \n",
    "    # Excluir registros com valores vazios ou placeholders\n",
    "    df_filtrado = df_filtrado[df_filtrado[coluna_oa].astype(str).str.strip() != '']\n",
    "    df_filtrado = df_filtrado[df_filtrado[coluna_oa].astype(str).str.lower() != 'nan']\n",
    "    df_filtrado = df_filtrado[df_filtrado[coluna_oa].astype(str).str.lower() != 'none']\n",
    "    df_filtrado = df_filtrado[df_filtrado[coluna_oa].astype(str).str.lower() != 'unknown']\n",
    "    \n",
    "    registros_finais = len(df_filtrado)\n",
    "    registros_removidos = registros_iniciais - registros_finais\n",
    "    \n",
    "    # Analisar distribuiÃ§Ã£o das categorias de Open Access\n",
    "    if registros_finais > 0:\n",
    "        categorias_oa = df_filtrado[coluna_oa].value_counts()\n",
    "        \n",
    "        print(f\"\\nðŸ“Š DistribuiÃ§Ã£o das categorias de Open Access:\")\n",
    "        for categoria, contagem in categorias_oa.items():\n",
    "            percentual = (contagem / registros_finais) * 100\n",
    "            print(f\"   {categoria}: {contagem:,} registros ({percentual:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Resumo do filtro de Open Access:\")\n",
    "    print(f\"   ðŸ“¥ Registros iniciais: {registros_iniciais:,}\")\n",
    "    print(f\"   ðŸ“¤ Registros finais: {registros_finais:,}\")\n",
    "    print(f\"   ðŸ—‘ï¸ Registros removidos (sem OA): {registros_removidos:,} ({(registros_removidos/registros_iniciais)*100:.1f}%)\")\n",
    "    \n",
    "    # Criar flag para indicar filtro aplicado\n",
    "    df_filtrado['filtrado_open_access'] = True\n",
    "    \n",
    "    return df_filtrado\n",
    "\n",
    "# Aplicar filtro de Open Access apÃ³s limpeza bÃ¡sica\n",
    "if 'df_clean' in locals() and df_clean is not None:\n",
    "    df_clean = filtrar_registros_open_access(df_clean)\n",
    "    print(\"ðŸ”’ Filtro de Open Access aplicado e integrado ao fluxo principal\")\n",
    "else:\n",
    "    print(\"âš ï¸ Pulando filtro de Open Access - dados nÃ£o disponÃ­veis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7c4762a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ—ºï¸ MAPEANDO COLUNAS WOS PARA FORMATO PADRÃƒO\n",
      "=============================================\n",
      "ðŸ“‹ Verificando mapeamento de colunas:\n",
      "  âœ… Article Title â†’ Article Title\n",
      "  âœ… Abstract â†’ Abstract\n",
      "  âœ… Author Keywords â†’ Author Keywords\n",
      "  âœ… Keywords Plus â†’ Keywords Plus\n",
      "  âœ… Authors â†’ Authors\n",
      "  âœ… Publication Year â†’ Publication Year\n",
      "  âœ… Source Title â†’ Source Title\n",
      "  âœ… DOI â†’ DOI\n",
      "  âœ… Addresses â†’ Addresses\n",
      "  âœ… Research Areas â†’ Research Areas\n",
      "  âš ï¸ Web of Science Categories â†’ NÃƒO ENCONTRADA\n",
      "  âš ï¸ Times Cited â†’ NÃƒO ENCONTRADA\n",
      "  âš ï¸ Journal â†’ NÃƒO ENCONTRADA\n",
      "  âœ… Volume â†’ Volume\n",
      "  âœ… Issue â†’ Issue\n",
      "  âš ï¸ Pages â†’ NÃƒO ENCONTRADA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… 12 colunas mapeadas com sucesso\n",
      "\n",
      "âœ… Todas as colunas essenciais estÃ£o disponÃ­veis\n",
      "\n",
      "ðŸ“Š DataFrame final: 38,323 linhas x 75 colunas\n",
      "ðŸ—ºï¸ Mapeamento de colunas concluÃ­do\n"
     ]
    }
   ],
   "source": [
    "# ðŸ—ºï¸ Mapeamento e padronizaÃ§Ã£o de colunas\n",
    "\n",
    "def mapear_colunas_wos(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Mapeia colunas do formato Web of Science para formato padrÃ£o do sistema\"\"\"\n",
    "    \n",
    "    print(\"\\nðŸ—ºï¸ MAPEANDO COLUNAS WOS PARA FORMATO PADRÃƒO\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Mapeamento de colunas WoS para formato padrÃ£o\n",
    "    mapeamento_colunas = {\n",
    "        'Article Title': 'Article Title',\n",
    "        'Abstract': 'Abstract',\n",
    "        'Author Keywords': 'Author Keywords',\n",
    "        'Keywords Plus': 'Keywords Plus',\n",
    "        'Authors': 'Authors',\n",
    "        'Publication Year': 'Publication Year',\n",
    "        'Source Title': 'Source Title',\n",
    "        'DOI': 'DOI',\n",
    "        'Addresses': 'Addresses',\n",
    "        'Research Areas': 'Research Areas',\n",
    "        'Web of Science Categories': 'WoS Categories',\n",
    "        'Times Cited': 'Times Cited',\n",
    "        'Journal': 'Journal',\n",
    "        'Volume': 'Volume',\n",
    "        'Issue': 'Issue',\n",
    "        'Pages': 'Pages'\n",
    "    }\n",
    "    \n",
    "    # Identificar colunas disponÃ­veis\n",
    "    colunas_disponiveis = list(df.columns)\n",
    "    colunas_mapeadas = {}\n",
    "    colunas_nao_encontradas = []\n",
    "    \n",
    "    print(\"ðŸ“‹ Verificando mapeamento de colunas:\")\n",
    "    \n",
    "    for coluna_wos, coluna_padrao in mapeamento_colunas.items():\n",
    "        if coluna_wos in colunas_disponiveis:\n",
    "            colunas_mapeadas[coluna_wos] = coluna_padrao\n",
    "            print(f\"  âœ… {coluna_wos} â†’ {coluna_padrao}\")\n",
    "        else:\n",
    "            colunas_nao_encontradas.append(coluna_wos)\n",
    "            print(f\"  âš ï¸ {coluna_wos} â†’ NÃƒO ENCONTRADA\")\n",
    "    \n",
    "    # Criar DataFrame mapeado\n",
    "    df_mapeado = df.copy()\n",
    "    \n",
    "    # Renomear colunas encontradas\n",
    "    if colunas_mapeadas:\n",
    "        df_mapeado = df_mapeado.rename(columns=colunas_mapeadas)\n",
    "        print(f\"\\nâœ… {len(colunas_mapeadas)} colunas mapeadas com sucesso\")\n",
    "    \n",
    "    # Garantir colunas essenciais\n",
    "    colunas_essenciais = ['Article Title', 'Abstract', 'Authors', 'Publication Year']\n",
    "    colunas_faltantes = []\n",
    "    \n",
    "    for coluna in colunas_essenciais:\n",
    "        if coluna not in df_mapeado.columns:\n",
    "            colunas_faltantes.append(coluna)\n",
    "    \n",
    "    if colunas_faltantes:\n",
    "        print(f\"\\nâš ï¸ Colunas essenciais nÃ£o encontradas: {colunas_faltantes}\")\n",
    "        print(\"ðŸ’¡ O sistema tentarÃ¡ continuar, mas a anÃ¡lise pode ser limitada\")\n",
    "    else:\n",
    "        print(f\"\\nâœ… Todas as colunas essenciais estÃ£o disponÃ­veis\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š DataFrame final: {len(df_mapeado):,} linhas x {len(df_mapeado.columns)} colunas\")\n",
    "    \n",
    "    return df_mapeado\n",
    "\n",
    "# Aplicar mapeamento se dados foram carregados\n",
    "if df_raw is not None:\n",
    "    df_mapped = mapear_colunas_wos(df_raw)\n",
    "    print(\"ðŸ—ºï¸ Mapeamento de colunas concluÃ­do\")\n",
    "else:\n",
    "    print(\"âš ï¸ Pulando mapeamento - dados nÃ£o carregados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c3826e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”„ INICIANDO PIPELINE DE PROCESSAMENTO\n",
      "========================================\n",
      "\n",
      "ðŸ“‹ Etapa 1: ValidaÃ§Ã£o dos dados\n",
      "\n",
      "ðŸ” VALIDANDO QUALIDADE DOS DADOS WOS\n",
      "========================================\n",
      "ðŸ“„ Abstracts vÃ¡lidos: 38,237/38,323 (99.8%)\n",
      "ðŸ“° TÃ­tulos vÃ¡lidos: 38,323/38,323 (100.0%)\n",
      "ðŸ“… Anos vÃ¡lidos: 37,834/38,323 (98.7%)\n",
      "ðŸ‘¥ Autores vÃ¡lidos: 38,323/38,323 (100.0%)\n",
      "ðŸ”„ Duplicatas potenciais: 1,014 (2.6%)\n",
      "ðŸ“‹ Registros completos: 38,237/38,323 (99.8%)\n",
      "\n",
      "ðŸ“Š Qualidade geral dos dados: 99.6/100\n",
      "\n",
      "ðŸ”¬ ANÃLISE DE RELEVÃ‚NCIA PARA NANOTECNOLOGIA:\n",
      "  ðŸ“° 'nano' em tÃ­tulos: 8440 menÃ§Ãµes\n",
      "ðŸ“° TÃ­tulos vÃ¡lidos: 38,323/38,323 (100.0%)\n",
      "ðŸ“… Anos vÃ¡lidos: 37,834/38,323 (98.7%)\n",
      "ðŸ‘¥ Autores vÃ¡lidos: 38,323/38,323 (100.0%)\n",
      "ðŸ”„ Duplicatas potenciais: 1,014 (2.6%)\n",
      "ðŸ“‹ Registros completos: 38,237/38,323 (99.8%)\n",
      "\n",
      "ðŸ“Š Qualidade geral dos dados: 99.6/100\n",
      "\n",
      "ðŸ”¬ ANÃLISE DE RELEVÃ‚NCIA PARA NANOTECNOLOGIA:\n",
      "  ðŸ“° 'nano' em tÃ­tulos: 8440 menÃ§Ãµes\n",
      "  ðŸ“° 'nanoparticle' em tÃ­tulos: 1985 menÃ§Ãµes\n",
      "  ðŸ“° 'nanotechnology' em tÃ­tulos: 11 menÃ§Ãµes\n",
      "  ðŸ“° 'nanomaterial' em tÃ­tulos: 89 menÃ§Ãµes\n",
      "  ðŸ“° 'coating' em tÃ­tulos: 15126 menÃ§Ãµes\n",
      "  ðŸ“° 'nanoparticle' em tÃ­tulos: 1985 menÃ§Ãµes\n",
      "  ðŸ“° 'nanotechnology' em tÃ­tulos: 11 menÃ§Ãµes\n",
      "  ðŸ“° 'nanomaterial' em tÃ­tulos: 89 menÃ§Ãµes\n",
      "  ðŸ“° 'coating' em tÃ­tulos: 15126 menÃ§Ãµes\n",
      "  ðŸ“° 'paint' em tÃ­tulos: 322 menÃ§Ãµes\n",
      "  ðŸ“„ 'nano' em abstracts: 13352 menÃ§Ãµes\n",
      "  ðŸ“° 'paint' em tÃ­tulos: 322 menÃ§Ãµes\n",
      "  ðŸ“„ 'nano' em abstracts: 13352 menÃ§Ãµes\n",
      "  ðŸ“„ 'nanoparticle' em abstracts: 4439 menÃ§Ãµes\n",
      "  ðŸ“„ 'nanotechnology' em abstracts: 99 menÃ§Ãµes\n",
      "  ðŸ“„ 'nanoparticle' em abstracts: 4439 menÃ§Ãµes\n",
      "  ðŸ“„ 'nanotechnology' em abstracts: 99 menÃ§Ãµes\n",
      "  ðŸ“„ 'nanomaterial' em abstracts: 469 menÃ§Ãµes\n",
      "  ðŸ“„ 'nanomaterial' em abstracts: 469 menÃ§Ãµes\n",
      "  ðŸ“„ 'coating' em abstracts: 31098 menÃ§Ãµes\n",
      "  ðŸ“„ 'paint' em abstracts: 1492 menÃ§Ãµes\n",
      "  ðŸ“„ 'coating' em abstracts: 31098 menÃ§Ãµes\n",
      "  ðŸ“„ 'paint' em abstracts: 1492 menÃ§Ãµes\n",
      "\n",
      "ðŸ§¹ Etapa 2: Limpeza bÃ¡sica\n",
      "\n",
      "ðŸ§¹ APLICANDO LIMPEZA BÃSICA DOS DADOS\n",
      "========================================\n",
      "\n",
      "ðŸ§¹ Etapa 2: Limpeza bÃ¡sica\n",
      "\n",
      "ðŸ§¹ APLICANDO LIMPEZA BÃSICA DOS DADOS\n",
      "========================================\n",
      "âœ… Nenhuma duplicata exata encontrada\n",
      "ðŸ“„ Limpando abstracts...\n",
      "âœ… Nenhuma duplicata exata encontrada\n",
      "ðŸ“„ Limpando abstracts...\n",
      "ðŸ“° Limpando tÃ­tulos...\n",
      "ðŸ“… Validando anos de publicaÃ§Ã£o...\n",
      "\n",
      "ðŸ“Š Resumo da limpeza:\n",
      "   ðŸ“¥ Registros iniciais: 38,323\n",
      "   ðŸ“¤ Registros finais: 38,323\n",
      "\n",
      "ðŸ”’ Etapa 3: Filtragem por Open Access\n",
      "\n",
      "ðŸ”’ FILTRANDO REGISTROS POR OPEN ACCESS\n",
      "========================================\n",
      "\n",
      "ðŸ“Š Resumo do filtro de Open Access:\n",
      "   ðŸ“¥ Registros iniciais: 38,323\n",
      "   ðŸ“¤ Registros finais: 9,046\n",
      "   ðŸ—‘ï¸ Registros removidos: 29,277\n",
      "\n",
      "âœ… Pipeline de processamento concluÃ­do com sucesso!\n",
      "ðŸ“Š Registros finais: 9,046\n",
      "ðŸ“‹ Colunas: 76\n",
      "\n",
      "ðŸ”— VariÃ¡vel 'dataset_processado' disponÃ­vel para prÃ³ximos notebooks\n",
      "ðŸ“° Limpando tÃ­tulos...\n",
      "ðŸ“… Validando anos de publicaÃ§Ã£o...\n",
      "\n",
      "ðŸ“Š Resumo da limpeza:\n",
      "   ðŸ“¥ Registros iniciais: 38,323\n",
      "   ðŸ“¤ Registros finais: 38,323\n",
      "\n",
      "ðŸ”’ Etapa 3: Filtragem por Open Access\n",
      "\n",
      "ðŸ”’ FILTRANDO REGISTROS POR OPEN ACCESS\n",
      "========================================\n",
      "\n",
      "ðŸ“Š Resumo do filtro de Open Access:\n",
      "   ðŸ“¥ Registros iniciais: 38,323\n",
      "   ðŸ“¤ Registros finais: 9,046\n",
      "   ðŸ—‘ï¸ Registros removidos: 29,277\n",
      "\n",
      "âœ… Pipeline de processamento concluÃ­do com sucesso!\n",
      "ðŸ“Š Registros finais: 9,046\n",
      "ðŸ“‹ Colunas: 76\n",
      "\n",
      "ðŸ”— VariÃ¡vel 'dataset_processado' disponÃ­vel para prÃ³ximos notebooks\n"
     ]
    }
   ],
   "source": [
    "# ðŸ” ValidaÃ§Ã£o, Limpeza e Processamento dos Dados\n",
    "\n",
    "def validar_dados_wos(df: pd.DataFrame) -> Dict[str, any]:\n",
    "    \"\"\"Valida a qualidade dos dados WoS carregados\"\"\"\n",
    "    \n",
    "    print(\"\\nðŸ” VALIDANDO QUALIDADE DOS DADOS WOS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    validacao = {\n",
    "        'total_registros': len(df),\n",
    "        'colunas_disponiveis': list(df.columns),\n",
    "        'abstracts_validos': 0,\n",
    "        'titulos_validos': 0,\n",
    "        'anos_validos': 0,\n",
    "        'autores_validos': 0,\n",
    "        'duplicatas_potenciais': 0,\n",
    "        'registros_completos': 0,\n",
    "        'qualidade_geral': 0,\n",
    "        'nano_relevancia_titulo': 0,\n",
    "        'nano_relevancia_abstract': 0\n",
    "    }\n",
    "    \n",
    "    # Validar Abstract\n",
    "    if 'Abstract' in df.columns:\n",
    "        abstracts_nao_nulos = df['Abstract'].notna()\n",
    "        abstracts_nao_vazios = df['Abstract'].astype(str).str.strip().str.len() > 50\n",
    "        validacao['abstracts_validos'] = (abstracts_nao_nulos & abstracts_nao_vazios).sum()\n",
    "        print(f\"ðŸ“„ Abstracts vÃ¡lidos: {validacao['abstracts_validos']:,}/{len(df):,} ({validacao['abstracts_validos']/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Validar Article Title\n",
    "    if 'Article Title' in df.columns:\n",
    "        titulos_nao_nulos = df['Article Title'].notna()\n",
    "        titulos_nao_vazios = df['Article Title'].astype(str).str.strip().str.len() > 10\n",
    "        validacao['titulos_validos'] = (titulos_nao_nulos & titulos_nao_vazios).sum()\n",
    "        print(f\"ðŸ“° TÃ­tulos vÃ¡lidos: {validacao['titulos_validos']:,}/{len(df):,} ({validacao['titulos_validos']/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Validar Publication Year\n",
    "    if 'Publication Year' in df.columns:\n",
    "        try:\n",
    "            anos_numericos = pd.to_numeric(df['Publication Year'], errors='coerce')\n",
    "            anos_validos_mask = (anos_numericos >= 1990) & (anos_numericos <= 2024)\n",
    "            validacao['anos_validos'] = anos_validos_mask.sum()\n",
    "            print(f\"ðŸ“… Anos vÃ¡lidos: {validacao['anos_validos']:,}/{len(df):,} ({validacao['anos_validos']/len(df)*100:.1f}%)\")\n",
    "        except:\n",
    "            print(f\"âš ï¸ Erro na validaÃ§Ã£o de anos\")\n",
    "    \n",
    "    # Validar Authors\n",
    "    if 'Authors' in df.columns:\n",
    "        autores_nao_nulos = df['Authors'].notna()\n",
    "        autores_nao_vazios = df['Authors'].astype(str).str.strip().str.len() > 3\n",
    "        validacao['autores_validos'] = (autores_nao_nulos & autores_nao_vazios).sum()\n",
    "        print(f\"ðŸ‘¥ Autores vÃ¡lidos: {validacao['autores_validos']:,}/{len(df):,} ({validacao['autores_validos']/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Verificar duplicatas potenciais por tÃ­tulo\n",
    "    if 'Article Title' in df.columns:\n",
    "        duplicatas = df['Article Title'].duplicated().sum()\n",
    "        validacao['duplicatas_potenciais'] = duplicatas\n",
    "        print(f\"ðŸ”„ Duplicatas potenciais: {duplicatas:,} ({duplicatas/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Calcular registros completos\n",
    "    colunas_essenciais = ['Article Title', 'Abstract', 'Authors', 'Publication Year']\n",
    "    colunas_presentes = [col for col in colunas_essenciais if col in df.columns]\n",
    "    \n",
    "    if colunas_presentes:\n",
    "        mask_completos = df[colunas_presentes].notna().all(axis=1)\n",
    "        validacao['registros_completos'] = mask_completos.sum()\n",
    "        print(f\"ðŸ“‹ Registros completos: {validacao['registros_completos']:,}/{len(df):,} ({validacao['registros_completos']/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Calcular qualidade geral\n",
    "    scores = []\n",
    "    if validacao['abstracts_validos'] > 0:\n",
    "        scores.append(validacao['abstracts_validos'] / len(df) * 100)\n",
    "    if validacao['titulos_validos'] > 0:\n",
    "        scores.append(validacao['titulos_validos'] / len(df) * 100)\n",
    "    if validacao['anos_validos'] > 0:\n",
    "        scores.append(validacao['anos_validos'] / len(df) * 100)\n",
    "    if validacao['autores_validos'] > 0:\n",
    "        scores.append(validacao['autores_validos'] / len(df) * 100)\n",
    "    \n",
    "    if scores:\n",
    "        validacao['qualidade_geral'] = sum(scores) / len(scores)\n",
    "        print(f\"\\nðŸ“Š Qualidade geral dos dados: {validacao['qualidade_geral']:.1f}/100\")\n",
    "    \n",
    "    # AnÃ¡lise de nano-relevÃ¢ncia\n",
    "    print(f\"\\nðŸ”¬ ANÃLISE DE RELEVÃ‚NCIA PARA NANOTECNOLOGIA:\")\n",
    "    nano_keywords = ['nano', 'nanoparticle', 'nanotechnology', 'nanomaterial', 'coating', 'paint', 'revestimento']\n",
    "    \n",
    "    if 'Article Title' in df.columns:\n",
    "        nano_mentions_title = 0\n",
    "        for keyword in nano_keywords:\n",
    "            count = df['Article Title'].astype(str).str.lower().str.contains(keyword, na=False).sum()\n",
    "            if count > 0:\n",
    "                print(f\"  ðŸ“° '{keyword}' em tÃ­tulos: {count} menÃ§Ãµes\")\n",
    "                nano_mentions_title += count\n",
    "        validacao['nano_relevancia_titulo'] = nano_mentions_title\n",
    "    \n",
    "    if 'Abstract' in df.columns:\n",
    "        nano_mentions_abstract = 0\n",
    "        for keyword in nano_keywords:\n",
    "            count = df['Abstract'].astype(str).str.lower().str.contains(keyword, na=False).sum()\n",
    "            if count > 0:\n",
    "                print(f\"  ðŸ“„ '{keyword}' em abstracts: {count} menÃ§Ãµes\")\n",
    "                nano_mentions_abstract += count\n",
    "        validacao['nano_relevancia_abstract'] = nano_mentions_abstract\n",
    "    \n",
    "    return validacao\n",
    "\n",
    "def limpar_dados_basico(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Aplica limpeza bÃ¡sica nos dados carregados\"\"\"\n",
    "    \n",
    "    print(\"\\nðŸ§¹ APLICANDO LIMPEZA BÃSICA DOS DADOS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    df_clean = df.copy()\n",
    "    linhas_iniciais = len(df_clean)\n",
    "    \n",
    "    # 1. Remover duplicatas exatas\n",
    "    duplicatas_exatas = df_clean.duplicated().sum()\n",
    "    if duplicatas_exatas > 0:\n",
    "        print(f\"ðŸ”„ Removendo {duplicatas_exatas:,} duplicatas exatas\")\n",
    "        df_clean = df_clean.drop_duplicates()\n",
    "    else:\n",
    "        print(\"âœ… Nenhuma duplicata exata encontrada\")\n",
    "    \n",
    "    # 2. Limpeza de abstracts\n",
    "    if 'Abstract' in df_clean.columns:\n",
    "        print(f\"ðŸ“„ Limpando abstracts...\")\n",
    "        df_clean['Abstract'] = df_clean['Abstract'].astype(str)\n",
    "        df_clean['Abstract'] = df_clean['Abstract'].str.replace('\\n', ' ').str.replace('\\r', ' ')\n",
    "        df_clean['Abstract'] = df_clean['Abstract'].str.replace('\\t', ' ').str.strip()\n",
    "        df_clean['Abstract'] = df_clean['Abstract'].str.replace(r'\\s+', ' ', regex=True)\n",
    "    \n",
    "    # 3. Limpeza de tÃ­tulos\n",
    "    if 'Article Title' in df_clean.columns:\n",
    "        print(f\"ðŸ“° Limpando tÃ­tulos...\")\n",
    "        df_clean['Article Title'] = df_clean['Article Title'].astype(str).str.strip()\n",
    "    \n",
    "    # 4. Limpeza de anos\n",
    "    if 'Publication Year' in df_clean.columns:\n",
    "        print(f\"ðŸ“… Validando anos de publicaÃ§Ã£o...\")\n",
    "        df_clean['Publication Year'] = pd.to_numeric(df_clean['Publication Year'], errors='coerce')\n",
    "    \n",
    "    linhas_finais = len(df_clean)\n",
    "    print(f\"\\nðŸ“Š Resumo da limpeza:\")\n",
    "    print(f\"   ðŸ“¥ Registros iniciais: {linhas_iniciais:,}\")\n",
    "    print(f\"   ðŸ“¤ Registros finais: {linhas_finais:,}\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def filtrar_registros_open_access(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Filtra os registros que possuem classificaÃ§Ã£o de Open Access\"\"\"\n",
    "    \n",
    "    print(\"\\nðŸ”’ FILTRANDO REGISTROS POR OPEN ACCESS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    df_filtrado = df.copy()\n",
    "    coluna_oa = None\n",
    "    possiveis_colunas = ['Open Access Designations', 'Open Access', 'OA', 'Acesso Aberto']\n",
    "    \n",
    "    for col in possiveis_colunas:\n",
    "        if col in df_filtrado.columns:\n",
    "            coluna_oa = col\n",
    "            break\n",
    "    \n",
    "    if coluna_oa is None:\n",
    "        print(\"âš ï¸ Nenhuma coluna de Open Access encontrada, utilizando dados sem filtro\")\n",
    "        df_filtrado['filtrado_open_access'] = False\n",
    "        return df_filtrado\n",
    "    \n",
    "    registros_iniciais = len(df_filtrado)\n",
    "    df_filtrado = df_filtrado[df_filtrado[coluna_oa].notna()]\n",
    "    df_filtrado = df_filtrado[df_filtrado[coluna_oa].astype(str).str.strip() != '']\n",
    "    df_filtrado = df_filtrado[~df_filtrado[coluna_oa].astype(str).str.lower().isin(['nan', 'none', 'unknown'])]\n",
    "    \n",
    "    registros_finais = len(df_filtrado)\n",
    "    registros_removidos = registros_iniciais - registros_finais\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Resumo do filtro de Open Access:\")\n",
    "    print(f\"   ðŸ“¥ Registros iniciais: {registros_iniciais:,}\")\n",
    "    print(f\"   ðŸ“¤ Registros finais: {registros_finais:,}\")\n",
    "    print(f\"   ðŸ—‘ï¸ Registros removidos: {registros_removidos:,}\")\n",
    "    \n",
    "    df_filtrado['filtrado_open_access'] = True\n",
    "    return df_filtrado\n",
    "\n",
    "# Executar pipeline de processamento\n",
    "if 'df_mapped' in locals() and df_mapped is not None:\n",
    "    print(\"\\nðŸ”„ INICIANDO PIPELINE DE PROCESSAMENTO\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # 1. ValidaÃ§Ã£o\n",
    "    print(\"\\nðŸ“‹ Etapa 1: ValidaÃ§Ã£o dos dados\")\n",
    "    validacao_resultado = validar_dados_wos(df_mapped)\n",
    "    \n",
    "    # 2. Limpeza bÃ¡sica\n",
    "    print(\"\\nðŸ§¹ Etapa 2: Limpeza bÃ¡sica\")\n",
    "    df_clean = limpar_dados_basico(df_mapped)\n",
    "    \n",
    "    # 3. Filtro Open Access\n",
    "    print(\"\\nðŸ”’ Etapa 3: Filtragem por Open Access\")\n",
    "    df_clean = filtrar_registros_open_access(df_clean)\n",
    "    \n",
    "    # 4. Salvar e disponibilizar dados processados\n",
    "    dataset_processado = df_clean\n",
    "    print(\"\\nâœ… Pipeline de processamento concluÃ­do com sucesso!\")\n",
    "    print(f\"ðŸ“Š Registros finais: {len(dataset_processado):,}\")\n",
    "    print(f\"ðŸ“‹ Colunas: {len(dataset_processado.columns)}\")\n",
    "    print(\"\\nðŸ”— VariÃ¡vel 'dataset_processado' disponÃ­vel para prÃ³ximos notebooks\")\n",
    "else:\n",
    "    print(\"âš ï¸ NÃ£o foi possÃ­vel iniciar o processamento - dados nÃ£o disponÃ­veis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60f5404a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ’¾ SALVANDO DADOS PROCESSADOS\n",
      "===================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Dataset salvo em: /home/delon/Modelos/modeloCenanoInk/data/processed/dataset_wos_processado_20250610_121255.csv\n",
      "ðŸ“‹ Metadados salvos em: /home/delon/Modelos/modeloCenanoInk/data/processed/metadados_processamento_20250610_121255.json\n",
      "ðŸ”— ReferÃªncia criada: /home/delon/Modelos/modeloCenanoInk/data/processed/dataset_wos_latest.csv\n",
      "ðŸŽ² Sample (1000 registros) salvo: /home/delon/Modelos/modeloCenanoInk/data/processed/dataset_wos_sample_1000.csv\n",
      "\n",
      "âœ… Salvamento concluÃ­do!\n",
      "ðŸ“‚ DiretÃ³rio: /home/delon/Modelos/modeloCenanoInk/data/processed/\n",
      "ðŸ“Š Total de arquivos criados: 3-4 arquivos\n",
      "ðŸ’¾ Dados salvos com sucesso\n",
      "\n",
      "ðŸ”— VariÃ¡vel 'dataset_processado' disponÃ­vel para prÃ³ximos notebooks\n",
      "ðŸ”— ReferÃªncia criada: /home/delon/Modelos/modeloCenanoInk/data/processed/dataset_wos_latest.csv\n",
      "ðŸŽ² Sample (1000 registros) salvo: /home/delon/Modelos/modeloCenanoInk/data/processed/dataset_wos_sample_1000.csv\n",
      "\n",
      "âœ… Salvamento concluÃ­do!\n",
      "ðŸ“‚ DiretÃ³rio: /home/delon/Modelos/modeloCenanoInk/data/processed/\n",
      "ðŸ“Š Total de arquivos criados: 3-4 arquivos\n",
      "ðŸ’¾ Dados salvos com sucesso\n",
      "\n",
      "ðŸ”— VariÃ¡vel 'dataset_processado' disponÃ­vel para prÃ³ximos notebooks\n"
     ]
    }
   ],
   "source": [
    "# ðŸ’¾ Salvamento dos dados processados\n",
    "\n",
    "def salvar_dados_processados(df: pd.DataFrame, validacao: Dict) -> str:\n",
    "    \"\"\"Salva os dados processados e metadados\"\"\"\n",
    "    \n",
    "    print(\"\\nðŸ’¾ SALVANDO DADOS PROCESSADOS\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    def convert_numpy_types(obj):\n",
    "        \"\"\"Convert numpy types to native Python types for JSON serialization\"\"\"\n",
    "        if isinstance(obj, (np.integer, np.int64, np.int32)):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, (np.floating, np.float64, np.float32)):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, dict):\n",
    "            return {k: convert_numpy_types(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [convert_numpy_types(i) for i in obj]\n",
    "        return obj\n",
    "    \n",
    "    # Criar timestamp para nomeaÃ§Ã£o\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Definir caminhos\n",
    "    processed_dir = PATHS.get('processed', './processed/')\n",
    "    Path(processed_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Salvar DataFrame principal\n",
    "    arquivo_dados = f\"{processed_dir}dataset_wos_processado_{timestamp}.csv\"\n",
    "    df.to_csv(arquivo_dados, index=False, encoding='utf-8-sig')\n",
    "    print(f\"ðŸ“Š Dataset salvo em: {arquivo_dados}\")\n",
    "    \n",
    "    # Verificar se o filtro Open Access foi aplicado\n",
    "    filtro_oa_aplicado = 'filtrado_open_access' in df.columns\n",
    "    registros_open_access = int(df['filtrado_open_access'].sum()) if filtro_oa_aplicado else 0\n",
    "    \n",
    "    # Salvar metadados\n",
    "    metadados = {\n",
    "        'timestamp_processamento': timestamp,\n",
    "        'total_registros': len(df),\n",
    "        'total_colunas': len(df.columns),\n",
    "        'validacao': convert_numpy_types(validacao),\n",
    "        'colunas': list(df.columns),\n",
    "        'arquivos_origem': df['arquivo_origem'].unique().tolist() if 'arquivo_origem' in df.columns else [],\n",
    "        'estatisticas_basicas': {\n",
    "            'registros_alta_qualidade': int(df['qualidade_alta'].sum()) if 'qualidade_alta' in df.columns else 0,\n",
    "            'registros_open_access': registros_open_access,\n",
    "            'filtro_open_access_aplicado': filtro_oa_aplicado,\n",
    "            'anos_publicacao_range': {\n",
    "                'min': int(df['Publication Year'].min()) if 'Publication Year' in df.columns and df['Publication Year'].notna().any() else None,\n",
    "                'max': int(df['Publication Year'].max()) if 'Publication Year' in df.columns and df['Publication Year'].notna().any() else None\n",
    "            } if 'Publication Year' in df.columns else None\n",
    "        },\n",
    "        'processamento': {\n",
    "            'filtro_open_access': filtro_oa_aplicado,\n",
    "            'limpeza_aplicada': True,\n",
    "            'data_processamento': datetime.now().isoformat()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    arquivo_metadados = f\"{processed_dir}metadados_processamento_{timestamp}.json\"\n",
    "    with open(arquivo_metadados, 'w', encoding='utf-8') as f:\n",
    "        json.dump(metadados, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"ðŸ“‹ Metadados salvos em: {arquivo_metadados}\")\n",
    "    \n",
    "    # Criar arquivo de referÃªncia mais simples para uso futuro\n",
    "    arquivo_simples = f\"{processed_dir}dataset_wos_latest.csv\"\n",
    "    df.to_csv(arquivo_simples, index=False, encoding='utf-8-sig')\n",
    "    print(f\"ðŸ”— ReferÃªncia criada: {arquivo_simples}\")\n",
    "    \n",
    "    # Salvar apenas subset para anÃ¡lises teste\n",
    "    if len(df) > 1000:\n",
    "        df_sample = df.sample(n=1000, random_state=42)\n",
    "        arquivo_sample = f\"{processed_dir}dataset_wos_sample_1000.csv\"\n",
    "        df_sample.to_csv(arquivo_sample, index=False, encoding='utf-8-sig')\n",
    "        print(f\"ðŸŽ² Sample (1000 registros) salvo: {arquivo_sample}\")\n",
    "    \n",
    "    print(f\"\\nâœ… Salvamento concluÃ­do!\")\n",
    "    print(f\"ðŸ“‚ DiretÃ³rio: {processed_dir}\")\n",
    "    print(f\"ðŸ“Š Total de arquivos criados: 3-4 arquivos\")\n",
    "    \n",
    "    return arquivo_dados\n",
    "\n",
    "# Salvar se dados foram processados\n",
    "if 'df_clean' in locals() and df_clean is not None and 'validacao_resultado' in locals():\n",
    "    arquivo_salvo = salvar_dados_processados(df_clean, validacao_resultado)\n",
    "    print(\"ðŸ’¾ Dados salvos com sucesso\")\n",
    "    \n",
    "    # Disponibilizar dados para prÃ³ximos notebooks\n",
    "    dataset_processado = df_clean\n",
    "    print(\"\\nðŸ”— VariÃ¡vel 'dataset_processado' disponÃ­vel para prÃ³ximos notebooks\")\n",
    "else:\n",
    "    print(\"âš ï¸ Pulando salvamento - dados nÃ£o processados completamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9cff59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š RESUMO FINAL DO CARREGAMENTO E PROCESSAMENTO\n",
      "=======================================================\n",
      "âœ… Status: Processamento concluÃ­do com sucesso\n",
      "ðŸ“Š Registros finais: 9,046\n",
      "ðŸ“‹ Colunas: 76\n",
      "ðŸ”’ Registros Open Access: 9,046 (100% dos registros)\n",
      "ðŸ“… PerÃ­odo de publicaÃ§Ã£o: 2014 - 2025\n",
      "ðŸ”¬ RelevÃ¢ncia nano (tÃ­tulos): 25,973 menÃ§Ãµes\n",
      "ðŸ”¬ RelevÃ¢ncia nano (abstracts): 50,949 menÃ§Ãµes\n",
      "\n",
      "ðŸ”„ Pipeline completo:\n",
      "   âœ… 1. ðŸ“¥ Carregamento de dados brutos\n",
      "   âœ… 2. ðŸ—ºï¸ Mapeamento e padronizaÃ§Ã£o de colunas\n",
      "   âœ… 3. ðŸ” ValidaÃ§Ã£o de qualidade dos dados\n",
      "   âœ… 4. ðŸ§¹ Limpeza e preprocessamento bÃ¡sico\n",
      "   âœ… 5. ðŸ”’ Filtragem por Open Access\n",
      "   âœ… 6. ðŸ’¾ Salvamento e geraÃ§Ã£o de metadados\n",
      "\n",
      "ðŸŽ¯ PRÃ“XIMOS PASSOS:\n",
      "   1. Execute '03_analise_regex.ipynb' para anÃ¡lise de padrÃµes\n",
      "   2. Ou execute '04_analise_gemini.ipynb' para anÃ¡lise com IA\n",
      "   3. Dados disponÃ­veis na variÃ¡vel 'dataset_processado'\n",
      "\n",
      "ðŸ“‹ Amostra dos dados (primeiros 3 registros):\n",
      "\n",
      "   ðŸ“„ Registro 1:\n",
      "      Article Title: Functionally Gradient Coating of Aluminum Alloy via In Situ Arc Surface Nitriding with Subsequent Fr...\n",
      "      Abstract: Functionally gradient coating on the AA6082-T6 substrate is successfully fabricated to improve the w...\n",
      "      Publication Year: 2019...\n",
      "\n",
      "   ðŸ“„ Registro 2:\n",
      "      Article Title: In situ Y2Si2O7 coatings on SiC fibers: Thermodynamic analysis and processing...\n",
      "      Abstract: It is shown using thermodynamic analysis and kinetic modeling that a processing window exists for th...\n",
      "      Publication Year: 2019...\n",
      "\n",
      "   ðŸ“„ Registro 3:\n",
      "      Article Title: Nanoparticles of Ni1-xZnxFe2O4 used as Microwave Absorbers in the X-band...\n",
      "      Abstract: Ferrites nanoparticles of Ni1-xZnxFe2O4 ferrites with x varying between 0 and 1, in steps of 0.2, we...\n",
      "      Publication Year: 2019...\n",
      "\n",
      "ðŸŽ‰ Notebook de carregamento concluÃ­do!\n"
     ]
    }
   ],
   "source": [
    "# ðŸ“Š Resumo final do carregamento\n",
    "\n",
    "print(\"ðŸ“Š RESUMO FINAL DO CARREGAMENTO E PROCESSAMENTO\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "if 'dataset_processado' in locals():\n",
    "    df_final = dataset_processado\n",
    "    \n",
    "    print(f\"âœ… Status: Processamento concluÃ­do com sucesso\")\n",
    "    print(f\"ðŸ“Š Registros finais: {len(df_final):,}\")\n",
    "    print(f\"ðŸ“‹ Colunas: {len(df_final.columns)}\")\n",
    "    \n",
    "    # Destacar informaÃ§Ã£o sobre Open Access\n",
    "    if 'filtrado_open_access' in df_final.columns:\n",
    "        oa_count = df_final['filtrado_open_access'].sum()\n",
    "        if oa_count == len(df_final):\n",
    "            print(f\"ðŸ”’ Registros Open Access: {oa_count:,} (100% dos registros)\")\n",
    "        else:\n",
    "            print(f\"ðŸ”’ Registros Open Access: {oa_count:,} ({oa_count/len(df_final)*100:.1f}% dos registros)\")\n",
    "    else:\n",
    "        print(f\"ðŸ”’ Filtro Open Access: NÃ£o aplicado\")\n",
    "    \n",
    "    if 'qualidade_alta' in df_final.columns:\n",
    "        alta_qualidade = df_final['qualidade_alta'].sum()\n",
    "        print(f\"â­ Registros alta qualidade: {alta_qualidade:,} ({alta_qualidade/len(df_final)*100:.1f}%)\")\n",
    "    \n",
    "    if 'Publication Year' in df_final.columns:\n",
    "        anos_validos = df_final['Publication Year'].notna().sum()\n",
    "        if anos_validos > 0:\n",
    "            ano_min = int(df_final['Publication Year'].min())\n",
    "            ano_max = int(df_final['Publication Year'].max())\n",
    "            print(f\"ðŸ“… PerÃ­odo de publicaÃ§Ã£o: {ano_min} - {ano_max}\")\n",
    "    \n",
    "    # Verificar relevÃ¢ncia para nanotecnologia\n",
    "    if 'validacao_resultado' in locals():\n",
    "        nano_titulo = validacao_resultado.get('nano_relevancia_titulo', 0)\n",
    "        nano_abstract = validacao_resultado.get('nano_relevancia_abstract', 0)\n",
    "        print(f\"ðŸ”¬ RelevÃ¢ncia nano (tÃ­tulos): {nano_titulo:,} menÃ§Ãµes\")\n",
    "        print(f\"ðŸ”¬ RelevÃ¢ncia nano (abstracts): {nano_abstract:,} menÃ§Ãµes\")\n",
    "    \n",
    "    # Pipeline completo\n",
    "    pipeline_steps = [\n",
    "        \"1. ðŸ“¥ Carregamento de dados brutos\",\n",
    "        \"2. ðŸ—ºï¸ Mapeamento e padronizaÃ§Ã£o de colunas\", \n",
    "        \"3. ðŸ” ValidaÃ§Ã£o de qualidade dos dados\",\n",
    "        \"4. ðŸ§¹ Limpeza e preprocessamento bÃ¡sico\",\n",
    "        \"5. ðŸ”’ Filtragem por Open Access\",\n",
    "        \"6. ðŸ’¾ Salvamento e geraÃ§Ã£o de metadados\"\n",
    "    ]\n",
    "    print(f\"\\nðŸ”„ Pipeline completo:\")\n",
    "    for step in pipeline_steps:\n",
    "        print(f\"   âœ… {step}\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ PRÃ“XIMOS PASSOS:\")\n",
    "    print(f\"   1. Execute '03_analise_regex.ipynb' para anÃ¡lise de padrÃµes\")\n",
    "    print(f\"   2. Ou execute '04_analise_gemini.ipynb' para anÃ¡lise com IA\")\n",
    "    print(f\"   3. Dados disponÃ­veis na variÃ¡vel 'dataset_processado'\")\n",
    "    \n",
    "    # EstatÃ­sticas rÃ¡pidas de amostra\n",
    "    print(f\"\\nðŸ“‹ Amostra dos dados (primeiros 3 registros):\")\n",
    "    colunas_mostrar = ['Article Title', 'Abstract', 'Publication Year']\n",
    "    colunas_existentes = [col for col in colunas_mostrar if col in df_final.columns]\n",
    "    \n",
    "    if colunas_existentes:\n",
    "        for i in range(min(3, len(df_final))):\n",
    "            print(f\"\\n   ðŸ“„ Registro {i+1}:\")\n",
    "            for col in colunas_existentes:\n",
    "                valor = str(df_final.iloc[i][col])[:100]\n",
    "                print(f\"      {col}: {valor}...\")\n",
    "else:\n",
    "    print(\"âŒ Status: Processamento nÃ£o foi concluÃ­do\")\n",
    "    print(\"ðŸ’¡ Verifique os passos anteriores para identificar problemas\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Notebook de carregamento concluÃ­do!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
