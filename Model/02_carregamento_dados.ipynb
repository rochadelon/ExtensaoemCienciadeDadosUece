{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd026831",
   "metadata": {},
   "source": [
    "# 📊 02 - Carregamento e Processamento de Dados\n",
    "\n",
    "## 📖 Visão Geral\n",
    "\n",
    "Este notebook é responsável pelo carregamento, consolidação e preprocessamento dos dados do Web of Science em formato Excel.\n",
    "\n",
    "### 🎯 Responsabilidades\n",
    "\n",
    "- ✅ Carregamento de 39 arquivos Excel do Web of Science\n",
    "- ✅ Consolidação em dataset único\n",
    "- ✅ Mapeamento e padronização de colunas\n",
    "- ✅ Validação e limpeza básica dos dados\n",
    "- ✅ Análise exploratória inicial\n",
    "- ✅ Geração de estatísticas de qualidade\n",
    "\n",
    "### 📦 Dependências\n",
    "\n",
    "- pandas\n",
    "- xlrd (para arquivos .xls)\n",
    "- numpy\n",
    "- pathlib\n",
    "\n",
    "### 🔗 Notebooks Relacionados\n",
    "\n",
    "- **Anterior**: `01_configuracao_sistema.ipynb`\n",
    "- **Próximo**: `03_analise_regex.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3219aa4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configurações carregadas do notebook anterior\n",
      "   Sistema configurado: True\n",
      "📁 Diretório de dados: /home/delon/Modelos/modeloCenanoInk/data/raw/arquivos_excel_artigo_cienciometrico/\n"
     ]
    }
   ],
   "source": [
    "# 📦 Imports e carregamento de configurações\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "# Carregar configurações do sistema\n",
    "config_file = 'config_sistema.json'\n",
    "if os.path.exists(config_file):\n",
    "    with open(config_file, 'r', encoding='utf-8') as f:\n",
    "        config_data = json.load(f)\n",
    "    \n",
    "    SISTEMA_CONFIGURADO = config_data.get('sistema_configurado', False)\n",
    "    PATHS = config_data.get('paths', {})\n",
    "    CONFIG = config_data.get('config', {})\n",
    "    \n",
    "    print(\"✅ Configurações carregadas do notebook anterior\")\n",
    "    print(f\"   Sistema configurado: {SISTEMA_CONFIGURADO}\")\n",
    "else:\n",
    "    print(\"❌ Arquivo de configuração não encontrado!\")\n",
    "    print(\"💡 Execute primeiro o notebook '01_configuracao_sistema.ipynb'\")\n",
    "    raise FileNotFoundError(\"Configuração do sistema necessária\")\n",
    "\n",
    "# Verificar se há arquivos Excel\n",
    "excel_dir = PATHS.get('excel_files', '/home/delon/Modelos/modeloCenanoInk/data/raw/arquivos_excel_artigo_cienciometrico/')\n",
    "print(f\"📁 Diretório de dados: {excel_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4655d462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 EXPLORANDO ARQUIVOS EXCEL\n",
      "===================================\n",
      "📊 Total de arquivos encontrados: 39\n",
      "\n",
      "📋 Analisando estrutura dos primeiros 3 arquivos:\n",
      "\n",
      "📄 Arquivo 1: savedrecs1.xls (4.3 MB)\n",
      "   📊 Dimensões: 1,000 linhas × 72 colunas\n",
      "   📋 Primeiras colunas: Publication Type, Authors, Book Authors, Book Editors, Book Group Authors...\n",
      "   ✅ Colunas WoS encontradas: 4/4\n",
      "\n",
      "📄 Arquivo 2: savedrecs10.xls (4.3 MB)\n",
      "   📊 Dimensões: 1,000 linhas × 72 colunas\n",
      "   📋 Primeiras colunas: Publication Type, Authors, Book Authors, Book Editors, Book Group Authors...\n",
      "   ✅ Colunas WoS encontradas: 4/4\n",
      "\n",
      "📄 Arquivo 3: savedrecs11.xls (4.3 MB)\n",
      "   📊 Dimensões: 1,000 linhas × 72 colunas\n",
      "   📋 Primeiras colunas: Publication Type, Authors, Book Authors, Book Editors, Book Group Authors...\n",
      "   ✅ Colunas WoS encontradas: 4/4\n",
      "\n",
      "📊 Estimativas totais:\n",
      "   📈 Registros por arquivo: ~1,000\n",
      "   📈 Total estimado de registros: ~39,000\n",
      "   💾 Tamanho total: ~12.8 MB\n",
      "   📊 Dimensões: 1,000 linhas × 72 colunas\n",
      "   📋 Primeiras colunas: Publication Type, Authors, Book Authors, Book Editors, Book Group Authors...\n",
      "   ✅ Colunas WoS encontradas: 4/4\n",
      "\n",
      "📄 Arquivo 3: savedrecs11.xls (4.3 MB)\n",
      "   📊 Dimensões: 1,000 linhas × 72 colunas\n",
      "   📋 Primeiras colunas: Publication Type, Authors, Book Authors, Book Editors, Book Group Authors...\n",
      "   ✅ Colunas WoS encontradas: 4/4\n",
      "\n",
      "📊 Estimativas totais:\n",
      "   📈 Registros por arquivo: ~1,000\n",
      "   📈 Total estimado de registros: ~39,000\n",
      "   💾 Tamanho total: ~12.8 MB\n"
     ]
    }
   ],
   "source": [
    "# 🔍 Exploração inicial dos arquivos Excel\n",
    "\n",
    "def explorar_arquivos_excel(diretorio: str) -> Dict[str, any]:\n",
    "    \"\"\"Explora os arquivos Excel disponíveis e retorna estatísticas\"\"\"\n",
    "    \n",
    "    print(\"🔍 EXPLORANDO ARQUIVOS EXCEL\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Buscar arquivos Excel\n",
    "    excel_files = glob.glob(os.path.join(diretorio, '*.xls'))\n",
    "    \n",
    "    if not excel_files:\n",
    "        print(f\"❌ Nenhum arquivo Excel encontrado em: {diretorio}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"📊 Total de arquivos encontrados: {len(excel_files)}\")\n",
    "    \n",
    "    # Analisar alguns arquivos para entender estrutura\n",
    "    exploracao = {\n",
    "        'total_arquivos': len(excel_files),\n",
    "        'arquivos': [],\n",
    "        'tamanhos': [],\n",
    "        'estruturas': [],\n",
    "        'total_estimado_registros': 0\n",
    "    }\n",
    "    \n",
    "    print(\"\\n📋 Analisando estrutura dos primeiros 3 arquivos:\")\n",
    "    \n",
    "    for i, arquivo_path in enumerate(sorted(excel_files)[:3]):\n",
    "        nome_arquivo = os.path.basename(arquivo_path)\n",
    "        tamanho_mb = os.path.getsize(arquivo_path) / (1024 * 1024)\n",
    "        \n",
    "        print(f\"\\n📄 Arquivo {i+1}: {nome_arquivo} ({tamanho_mb:.1f} MB)\")\n",
    "        \n",
    "        try:\n",
    "            # Tentar ler o arquivo\n",
    "            df_temp = pd.read_excel(arquivo_path, engine='xlrd')\n",
    "            \n",
    "            estrutura = {\n",
    "                'arquivo': nome_arquivo,\n",
    "                'linhas': len(df_temp),\n",
    "                'colunas': len(df_temp.columns),\n",
    "                'tamanho_mb': tamanho_mb,\n",
    "                'colunas_principais': list(df_temp.columns[:10])\n",
    "            }\n",
    "            \n",
    "            exploracao['arquivos'].append(nome_arquivo)\n",
    "            exploracao['tamanhos'].append(tamanho_mb)\n",
    "            exploracao['estruturas'].append(estrutura)\n",
    "            \n",
    "            print(f\"   📊 Dimensões: {len(df_temp):,} linhas × {len(df_temp.columns)} colunas\")\n",
    "            print(f\"   📋 Primeiras colunas: {', '.join(df_temp.columns[:5])}...\")\n",
    "            \n",
    "            # Verificar colunas essenciais do Web of Science\n",
    "            colunas_wos = ['Article Title', 'Abstract', 'Authors', 'Publication Year']\n",
    "            colunas_encontradas = [col for col in colunas_wos if col in df_temp.columns]\n",
    "            print(f\"   ✅ Colunas WoS encontradas: {len(colunas_encontradas)}/{len(colunas_wos)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Erro ao ler arquivo: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Estimativa total\n",
    "    if exploracao['estruturas']:\n",
    "        media_linhas = np.mean([est['linhas'] for est in exploracao['estruturas']])\n",
    "        exploracao['total_estimado_registros'] = int(media_linhas * len(excel_files))\n",
    "        \n",
    "        print(f\"\\n📊 Estimativas totais:\")\n",
    "        print(f\"   📈 Registros por arquivo: ~{media_linhas:,.0f}\")\n",
    "        print(f\"   📈 Total estimado de registros: ~{exploracao['total_estimado_registros']:,}\")\n",
    "        print(f\"   💾 Tamanho total: ~{sum(exploracao['tamanhos']):.1f} MB\")\n",
    "    \n",
    "    return exploracao\n",
    "\n",
    "# Executar exploração\n",
    "exploracao_inicial = explorar_arquivos_excel(excel_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2920e9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Iniciando carregamento dos dados...\n",
      "📊 CARREGANDO ARQUIVOS EXCEL WOS\n",
      "========================================\n",
      "📁 Encontrados 39 arquivos Excel\n",
      "\n",
      "📄 Carregando arquivo 1/39: savedrecs26.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 2/39: savedrecs20.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 2/39: savedrecs20.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 3/39: savedrecs3.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 3/39: savedrecs3.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 4/39: savedrecs6.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 4/39: savedrecs6.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 5/39: savedrecs22.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 5/39: savedrecs22.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 6/39: savedrecs18.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 6/39: savedrecs18.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 7/39: savedrecs5.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 7/39: savedrecs5.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 8/39: savedrecs29.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 8/39: savedrecs29.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 9/39: savedrecs24.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 9/39: savedrecs24.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 10/39: savedrecs37.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 10/39: savedrecs37.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 11/39: savedrecs11.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 11/39: savedrecs11.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 12/39: savedrecs30.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 13/39: savedrecs13.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 12/39: savedrecs30.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 13/39: savedrecs13.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 14/39: savedrecs14.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 15/39: savedrecs21.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 14/39: savedrecs14.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 15/39: savedrecs21.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 16/39: savedrecs38.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 17/39: savedrecs10.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 16/39: savedrecs38.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 17/39: savedrecs10.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 18/39: savedrecs7.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 19/39: savedrecs19.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 18/39: savedrecs7.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 19/39: savedrecs19.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 20/39: savedrecs1.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 21/39: savedrecs4.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 20/39: savedrecs1.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 21/39: savedrecs4.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 22/39: savedrecs35.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 23/39: savedrecs33.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 22/39: savedrecs35.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 23/39: savedrecs33.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 24/39: savedrecs27.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 25/39: savedrecs12.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 24/39: savedrecs27.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 25/39: savedrecs12.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 26/39: savedrecs15.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 27/39: savedrecs17.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 26/39: savedrecs15.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 27/39: savedrecs17.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 28/39: savedrecs39.xls\n",
      "  ✅ Carregado: 323 registros\n",
      "\n",
      "📄 Carregando arquivo 29/39: savedrecs28.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 30/39: savedrecs23.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 28/39: savedrecs39.xls\n",
      "  ✅ Carregado: 323 registros\n",
      "\n",
      "📄 Carregando arquivo 29/39: savedrecs28.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 30/39: savedrecs23.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 31/39: savedrecs25.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 32/39: savedrecs32.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 31/39: savedrecs25.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 32/39: savedrecs32.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 33/39: savedrecs36.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 34/39: savedrecs31.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 33/39: savedrecs36.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 34/39: savedrecs31.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 35/39: savedrecs16.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 36/39: savedrecs8.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 35/39: savedrecs16.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 36/39: savedrecs8.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 37/39: savedrecs34.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 37/39: savedrecs34.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 38/39: savedrecs2.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 38/39: savedrecs2.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 39/39: savedrecs9.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "📄 Carregando arquivo 39/39: savedrecs9.xls\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "🔄 Consolidando 39 DataFrames...\n",
      "  ✅ Carregado: 1,000 registros\n",
      "\n",
      "🔄 Consolidando 39 DataFrames...\n",
      "\n",
      "✅ CONSOLIDAÇÃO CONCLUÍDA\n",
      "📊 Total de registros: 38,323\n",
      "📋 Total de colunas: 75\n",
      "📁 Arquivos processados: 39/39\n",
      "\n",
      "🎉 Dados carregados com sucesso!\n",
      "📊 Dataset shape: (38323, 75)\n",
      "\n",
      "✅ CONSOLIDAÇÃO CONCLUÍDA\n",
      "📊 Total de registros: 38,323\n",
      "📋 Total de colunas: 75\n",
      "📁 Arquivos processados: 39/39\n",
      "\n",
      "🎉 Dados carregados com sucesso!\n",
      "📊 Dataset shape: (38323, 75)\n"
     ]
    }
   ],
   "source": [
    "# 📥 Função de carregamento consolidado\n",
    "\n",
    "def carregar_arquivos_excel_wos(diretorio_excel: str = None) -> pd.DataFrame:\n",
    "    \"\"\"Carrega todos os arquivos Excel do Web of Science e consolida em um DataFrame único\"\"\"\n",
    "    \n",
    "    if diretorio_excel is None:\n",
    "        diretorio_excel = excel_dir\n",
    "    \n",
    "    print(\"📊 CARREGANDO ARQUIVOS EXCEL WOS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Buscar todos os arquivos Excel\n",
    "    excel_files = glob.glob(os.path.join(diretorio_excel, '*.xls'))\n",
    "    \n",
    "    if not excel_files:\n",
    "        print(\"❌ Nenhum arquivo Excel encontrado!\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"📁 Encontrados {len(excel_files)} arquivos Excel\")\n",
    "    \n",
    "    dataframes = []\n",
    "    total_records = 0\n",
    "    errors = []\n",
    "    \n",
    "    for i, file_path in enumerate(excel_files, 1):\n",
    "        try:\n",
    "            print(f\"\\n📄 Carregando arquivo {i}/{len(excel_files)}: {os.path.basename(file_path)}\")\n",
    "            \n",
    "            # Carregar arquivo Excel\n",
    "            df_temp = pd.read_excel(file_path, engine='xlrd')\n",
    "            \n",
    "            # Adicionar informações de origem\n",
    "            df_temp['arquivo_origem'] = os.path.basename(file_path)\n",
    "            df_temp['batch_numero'] = i\n",
    "            df_temp['data_carregamento'] = datetime.now().isoformat()\n",
    "            \n",
    "            print(f\"  ✅ Carregado: {len(df_temp):,} registros\")\n",
    "            \n",
    "            dataframes.append(df_temp)\n",
    "            total_records += len(df_temp)\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Erro no arquivo {os.path.basename(file_path)}: {str(e)}\"\n",
    "            print(f\"  ❌ {error_msg}\")\n",
    "            errors.append(error_msg)\n",
    "    \n",
    "    if not dataframes:\n",
    "        print(\"❌ Nenhum arquivo foi carregado com sucesso!\")\n",
    "        return None\n",
    "    \n",
    "    # Consolidar todos os DataFrames\n",
    "    print(f\"\\n🔄 Consolidando {len(dataframes)} DataFrames...\")\n",
    "    df_consolidado = pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "    print(f\"\\n✅ CONSOLIDAÇÃO CONCLUÍDA\")\n",
    "    print(f\"📊 Total de registros: {len(df_consolidado):,}\")\n",
    "    print(f\"📋 Total de colunas: {len(df_consolidado.columns)}\")\n",
    "    print(f\"📁 Arquivos processados: {len(dataframes)}/{len(excel_files)}\")\n",
    "    \n",
    "    if errors:\n",
    "        print(f\"\\n⚠️ Erros encontrados ({len(errors)}):\")\n",
    "        for error in errors[:3]:  # Mostrar apenas os primeiros 3 erros\n",
    "            print(f\"  - {error}\")\n",
    "        if len(errors) > 3:\n",
    "            print(f\"  ... e mais {len(errors) - 3} erros\")\n",
    "    \n",
    "    return df_consolidado\n",
    "\n",
    "# Executar carregamento\n",
    "print(\"🚀 Iniciando carregamento dos dados...\")\n",
    "df_raw = carregar_arquivos_excel_wos()\n",
    "\n",
    "if df_raw is not None:\n",
    "    print(f\"\\n🎉 Dados carregados com sucesso!\")\n",
    "    print(f\"📊 Dataset shape: {df_raw.shape}\")\n",
    "else:\n",
    "    print(\"❌ Falha no carregamento dos dados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd0f149e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Pulando filtro de Open Access - dados não disponíveis\n"
     ]
    }
   ],
   "source": [
    "# 🔍 Filtro de registros por acesso aberto (Open Access)\n",
    "\n",
    "def filtrar_registros_open_access(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filtra os registros que possuem classificação de Open Access\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame com dados processados\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame contendo apenas registros com classificação de Open Access\n",
    "    \"\"\"\n",
    "    print(\"\\n🔒 FILTRANDO REGISTROS POR OPEN ACCESS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    df_filtrado = df.copy()\n",
    "    \n",
    "    # Verificar se a coluna Open Access está disponível\n",
    "    coluna_oa = None\n",
    "    possiveis_colunas = ['Open Access Designations', 'Open Access', 'OA', 'Acesso Aberto']\n",
    "    \n",
    "    for col in possiveis_colunas:\n",
    "        if col in df_filtrado.columns:\n",
    "            coluna_oa = col\n",
    "            break\n",
    "    \n",
    "    if coluna_oa is None:\n",
    "        print(\"⚠️ Nenhuma coluna de Open Access encontrada, utilizando dados sem filtro\")\n",
    "        # Adicionar flag mesmo assim para manter consistência\n",
    "        df_filtrado['filtrado_open_access'] = False\n",
    "        return df_filtrado\n",
    "    \n",
    "    # Filtrar registros com classificação de Open Access (valores não vazios)\n",
    "    registros_iniciais = len(df_filtrado)\n",
    "    df_filtrado = df_filtrado[df_filtrado[coluna_oa].notna()]\n",
    "    \n",
    "    # Excluir registros com valores vazios ou placeholders\n",
    "    df_filtrado = df_filtrado[df_filtrado[coluna_oa].astype(str).str.strip() != '']\n",
    "    df_filtrado = df_filtrado[df_filtrado[coluna_oa].astype(str).str.lower() != 'nan']\n",
    "    df_filtrado = df_filtrado[df_filtrado[coluna_oa].astype(str).str.lower() != 'none']\n",
    "    df_filtrado = df_filtrado[df_filtrado[coluna_oa].astype(str).str.lower() != 'unknown']\n",
    "    \n",
    "    registros_finais = len(df_filtrado)\n",
    "    registros_removidos = registros_iniciais - registros_finais\n",
    "    \n",
    "    # Analisar distribuição das categorias de Open Access\n",
    "    if registros_finais > 0:\n",
    "        categorias_oa = df_filtrado[coluna_oa].value_counts()\n",
    "        \n",
    "        print(f\"\\n📊 Distribuição das categorias de Open Access:\")\n",
    "        for categoria, contagem in categorias_oa.items():\n",
    "            percentual = (contagem / registros_finais) * 100\n",
    "            print(f\"   {categoria}: {contagem:,} registros ({percentual:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n📊 Resumo do filtro de Open Access:\")\n",
    "    print(f\"   📥 Registros iniciais: {registros_iniciais:,}\")\n",
    "    print(f\"   📤 Registros finais: {registros_finais:,}\")\n",
    "    print(f\"   🗑️ Registros removidos (sem OA): {registros_removidos:,} ({(registros_removidos/registros_iniciais)*100:.1f}%)\")\n",
    "    \n",
    "    # Criar flag para indicar filtro aplicado\n",
    "    df_filtrado['filtrado_open_access'] = True\n",
    "    \n",
    "    return df_filtrado\n",
    "\n",
    "# Aplicar filtro de Open Access após limpeza básica\n",
    "if 'df_clean' in locals() and df_clean is not None:\n",
    "    df_clean = filtrar_registros_open_access(df_clean)\n",
    "    print(\"🔒 Filtro de Open Access aplicado e integrado ao fluxo principal\")\n",
    "else:\n",
    "    print(\"⚠️ Pulando filtro de Open Access - dados não disponíveis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7c4762a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🗺️ MAPEANDO COLUNAS WOS PARA FORMATO PADRÃO\n",
      "=============================================\n",
      "📋 Verificando mapeamento de colunas:\n",
      "  ✅ Article Title → Article Title\n",
      "  ✅ Abstract → Abstract\n",
      "  ✅ Author Keywords → Author Keywords\n",
      "  ✅ Keywords Plus → Keywords Plus\n",
      "  ✅ Authors → Authors\n",
      "  ✅ Publication Year → Publication Year\n",
      "  ✅ Source Title → Source Title\n",
      "  ✅ DOI → DOI\n",
      "  ✅ Addresses → Addresses\n",
      "  ✅ Research Areas → Research Areas\n",
      "  ⚠️ Web of Science Categories → NÃO ENCONTRADA\n",
      "  ⚠️ Times Cited → NÃO ENCONTRADA\n",
      "  ⚠️ Journal → NÃO ENCONTRADA\n",
      "  ✅ Volume → Volume\n",
      "  ✅ Issue → Issue\n",
      "  ⚠️ Pages → NÃO ENCONTRADA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ 12 colunas mapeadas com sucesso\n",
      "\n",
      "✅ Todas as colunas essenciais estão disponíveis\n",
      "\n",
      "📊 DataFrame final: 38,323 linhas x 75 colunas\n",
      "🗺️ Mapeamento de colunas concluído\n"
     ]
    }
   ],
   "source": [
    "# 🗺️ Mapeamento e padronização de colunas\n",
    "\n",
    "def mapear_colunas_wos(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Mapeia colunas do formato Web of Science para formato padrão do sistema\"\"\"\n",
    "    \n",
    "    print(\"\\n🗺️ MAPEANDO COLUNAS WOS PARA FORMATO PADRÃO\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Mapeamento de colunas WoS para formato padrão\n",
    "    mapeamento_colunas = {\n",
    "        'Article Title': 'Article Title',\n",
    "        'Abstract': 'Abstract',\n",
    "        'Author Keywords': 'Author Keywords',\n",
    "        'Keywords Plus': 'Keywords Plus',\n",
    "        'Authors': 'Authors',\n",
    "        'Publication Year': 'Publication Year',\n",
    "        'Source Title': 'Source Title',\n",
    "        'DOI': 'DOI',\n",
    "        'Addresses': 'Addresses',\n",
    "        'Research Areas': 'Research Areas',\n",
    "        'Web of Science Categories': 'WoS Categories',\n",
    "        'Times Cited': 'Times Cited',\n",
    "        'Journal': 'Journal',\n",
    "        'Volume': 'Volume',\n",
    "        'Issue': 'Issue',\n",
    "        'Pages': 'Pages'\n",
    "    }\n",
    "    \n",
    "    # Identificar colunas disponíveis\n",
    "    colunas_disponiveis = list(df.columns)\n",
    "    colunas_mapeadas = {}\n",
    "    colunas_nao_encontradas = []\n",
    "    \n",
    "    print(\"📋 Verificando mapeamento de colunas:\")\n",
    "    \n",
    "    for coluna_wos, coluna_padrao in mapeamento_colunas.items():\n",
    "        if coluna_wos in colunas_disponiveis:\n",
    "            colunas_mapeadas[coluna_wos] = coluna_padrao\n",
    "            print(f\"  ✅ {coluna_wos} → {coluna_padrao}\")\n",
    "        else:\n",
    "            colunas_nao_encontradas.append(coluna_wos)\n",
    "            print(f\"  ⚠️ {coluna_wos} → NÃO ENCONTRADA\")\n",
    "    \n",
    "    # Criar DataFrame mapeado\n",
    "    df_mapeado = df.copy()\n",
    "    \n",
    "    # Renomear colunas encontradas\n",
    "    if colunas_mapeadas:\n",
    "        df_mapeado = df_mapeado.rename(columns=colunas_mapeadas)\n",
    "        print(f\"\\n✅ {len(colunas_mapeadas)} colunas mapeadas com sucesso\")\n",
    "    \n",
    "    # Garantir colunas essenciais\n",
    "    colunas_essenciais = ['Article Title', 'Abstract', 'Authors', 'Publication Year']\n",
    "    colunas_faltantes = []\n",
    "    \n",
    "    for coluna in colunas_essenciais:\n",
    "        if coluna not in df_mapeado.columns:\n",
    "            colunas_faltantes.append(coluna)\n",
    "    \n",
    "    if colunas_faltantes:\n",
    "        print(f\"\\n⚠️ Colunas essenciais não encontradas: {colunas_faltantes}\")\n",
    "        print(\"💡 O sistema tentará continuar, mas a análise pode ser limitada\")\n",
    "    else:\n",
    "        print(f\"\\n✅ Todas as colunas essenciais estão disponíveis\")\n",
    "    \n",
    "    print(f\"\\n📊 DataFrame final: {len(df_mapeado):,} linhas x {len(df_mapeado.columns)} colunas\")\n",
    "    \n",
    "    return df_mapeado\n",
    "\n",
    "# Aplicar mapeamento se dados foram carregados\n",
    "if df_raw is not None:\n",
    "    df_mapped = mapear_colunas_wos(df_raw)\n",
    "    print(\"🗺️ Mapeamento de colunas concluído\")\n",
    "else:\n",
    "    print(\"⚠️ Pulando mapeamento - dados não carregados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c3826e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 INICIANDO PIPELINE DE PROCESSAMENTO\n",
      "========================================\n",
      "\n",
      "📋 Etapa 1: Validação dos dados\n",
      "\n",
      "🔍 VALIDANDO QUALIDADE DOS DADOS WOS\n",
      "========================================\n",
      "📄 Abstracts válidos: 38,237/38,323 (99.8%)\n",
      "📰 Títulos válidos: 38,323/38,323 (100.0%)\n",
      "📅 Anos válidos: 37,834/38,323 (98.7%)\n",
      "👥 Autores válidos: 38,323/38,323 (100.0%)\n",
      "🔄 Duplicatas potenciais: 1,014 (2.6%)\n",
      "📋 Registros completos: 38,237/38,323 (99.8%)\n",
      "\n",
      "📊 Qualidade geral dos dados: 99.6/100\n",
      "\n",
      "🔬 ANÁLISE DE RELEVÂNCIA PARA NANOTECNOLOGIA:\n",
      "  📰 'nano' em títulos: 8440 menções\n",
      "📰 Títulos válidos: 38,323/38,323 (100.0%)\n",
      "📅 Anos válidos: 37,834/38,323 (98.7%)\n",
      "👥 Autores válidos: 38,323/38,323 (100.0%)\n",
      "🔄 Duplicatas potenciais: 1,014 (2.6%)\n",
      "📋 Registros completos: 38,237/38,323 (99.8%)\n",
      "\n",
      "📊 Qualidade geral dos dados: 99.6/100\n",
      "\n",
      "🔬 ANÁLISE DE RELEVÂNCIA PARA NANOTECNOLOGIA:\n",
      "  📰 'nano' em títulos: 8440 menções\n",
      "  📰 'nanoparticle' em títulos: 1985 menções\n",
      "  📰 'nanotechnology' em títulos: 11 menções\n",
      "  📰 'nanomaterial' em títulos: 89 menções\n",
      "  📰 'coating' em títulos: 15126 menções\n",
      "  📰 'nanoparticle' em títulos: 1985 menções\n",
      "  📰 'nanotechnology' em títulos: 11 menções\n",
      "  📰 'nanomaterial' em títulos: 89 menções\n",
      "  📰 'coating' em títulos: 15126 menções\n",
      "  📰 'paint' em títulos: 322 menções\n",
      "  📄 'nano' em abstracts: 13352 menções\n",
      "  📰 'paint' em títulos: 322 menções\n",
      "  📄 'nano' em abstracts: 13352 menções\n",
      "  📄 'nanoparticle' em abstracts: 4439 menções\n",
      "  📄 'nanotechnology' em abstracts: 99 menções\n",
      "  📄 'nanoparticle' em abstracts: 4439 menções\n",
      "  📄 'nanotechnology' em abstracts: 99 menções\n",
      "  📄 'nanomaterial' em abstracts: 469 menções\n",
      "  📄 'nanomaterial' em abstracts: 469 menções\n",
      "  📄 'coating' em abstracts: 31098 menções\n",
      "  📄 'paint' em abstracts: 1492 menções\n",
      "  📄 'coating' em abstracts: 31098 menções\n",
      "  📄 'paint' em abstracts: 1492 menções\n",
      "\n",
      "🧹 Etapa 2: Limpeza básica\n",
      "\n",
      "🧹 APLICANDO LIMPEZA BÁSICA DOS DADOS\n",
      "========================================\n",
      "\n",
      "🧹 Etapa 2: Limpeza básica\n",
      "\n",
      "🧹 APLICANDO LIMPEZA BÁSICA DOS DADOS\n",
      "========================================\n",
      "✅ Nenhuma duplicata exata encontrada\n",
      "📄 Limpando abstracts...\n",
      "✅ Nenhuma duplicata exata encontrada\n",
      "📄 Limpando abstracts...\n",
      "📰 Limpando títulos...\n",
      "📅 Validando anos de publicação...\n",
      "\n",
      "📊 Resumo da limpeza:\n",
      "   📥 Registros iniciais: 38,323\n",
      "   📤 Registros finais: 38,323\n",
      "\n",
      "🔒 Etapa 3: Filtragem por Open Access\n",
      "\n",
      "🔒 FILTRANDO REGISTROS POR OPEN ACCESS\n",
      "========================================\n",
      "\n",
      "📊 Resumo do filtro de Open Access:\n",
      "   📥 Registros iniciais: 38,323\n",
      "   📤 Registros finais: 9,046\n",
      "   🗑️ Registros removidos: 29,277\n",
      "\n",
      "✅ Pipeline de processamento concluído com sucesso!\n",
      "📊 Registros finais: 9,046\n",
      "📋 Colunas: 76\n",
      "\n",
      "🔗 Variável 'dataset_processado' disponível para próximos notebooks\n",
      "📰 Limpando títulos...\n",
      "📅 Validando anos de publicação...\n",
      "\n",
      "📊 Resumo da limpeza:\n",
      "   📥 Registros iniciais: 38,323\n",
      "   📤 Registros finais: 38,323\n",
      "\n",
      "🔒 Etapa 3: Filtragem por Open Access\n",
      "\n",
      "🔒 FILTRANDO REGISTROS POR OPEN ACCESS\n",
      "========================================\n",
      "\n",
      "📊 Resumo do filtro de Open Access:\n",
      "   📥 Registros iniciais: 38,323\n",
      "   📤 Registros finais: 9,046\n",
      "   🗑️ Registros removidos: 29,277\n",
      "\n",
      "✅ Pipeline de processamento concluído com sucesso!\n",
      "📊 Registros finais: 9,046\n",
      "📋 Colunas: 76\n",
      "\n",
      "🔗 Variável 'dataset_processado' disponível para próximos notebooks\n"
     ]
    }
   ],
   "source": [
    "# 🔍 Validação, Limpeza e Processamento dos Dados\n",
    "\n",
    "def validar_dados_wos(df: pd.DataFrame) -> Dict[str, any]:\n",
    "    \"\"\"Valida a qualidade dos dados WoS carregados\"\"\"\n",
    "    \n",
    "    print(\"\\n🔍 VALIDANDO QUALIDADE DOS DADOS WOS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    validacao = {\n",
    "        'total_registros': len(df),\n",
    "        'colunas_disponiveis': list(df.columns),\n",
    "        'abstracts_validos': 0,\n",
    "        'titulos_validos': 0,\n",
    "        'anos_validos': 0,\n",
    "        'autores_validos': 0,\n",
    "        'duplicatas_potenciais': 0,\n",
    "        'registros_completos': 0,\n",
    "        'qualidade_geral': 0,\n",
    "        'nano_relevancia_titulo': 0,\n",
    "        'nano_relevancia_abstract': 0\n",
    "    }\n",
    "    \n",
    "    # Validar Abstract\n",
    "    if 'Abstract' in df.columns:\n",
    "        abstracts_nao_nulos = df['Abstract'].notna()\n",
    "        abstracts_nao_vazios = df['Abstract'].astype(str).str.strip().str.len() > 50\n",
    "        validacao['abstracts_validos'] = (abstracts_nao_nulos & abstracts_nao_vazios).sum()\n",
    "        print(f\"📄 Abstracts válidos: {validacao['abstracts_validos']:,}/{len(df):,} ({validacao['abstracts_validos']/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Validar Article Title\n",
    "    if 'Article Title' in df.columns:\n",
    "        titulos_nao_nulos = df['Article Title'].notna()\n",
    "        titulos_nao_vazios = df['Article Title'].astype(str).str.strip().str.len() > 10\n",
    "        validacao['titulos_validos'] = (titulos_nao_nulos & titulos_nao_vazios).sum()\n",
    "        print(f\"📰 Títulos válidos: {validacao['titulos_validos']:,}/{len(df):,} ({validacao['titulos_validos']/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Validar Publication Year\n",
    "    if 'Publication Year' in df.columns:\n",
    "        try:\n",
    "            anos_numericos = pd.to_numeric(df['Publication Year'], errors='coerce')\n",
    "            anos_validos_mask = (anos_numericos >= 1990) & (anos_numericos <= 2024)\n",
    "            validacao['anos_validos'] = anos_validos_mask.sum()\n",
    "            print(f\"📅 Anos válidos: {validacao['anos_validos']:,}/{len(df):,} ({validacao['anos_validos']/len(df)*100:.1f}%)\")\n",
    "        except:\n",
    "            print(f\"⚠️ Erro na validação de anos\")\n",
    "    \n",
    "    # Validar Authors\n",
    "    if 'Authors' in df.columns:\n",
    "        autores_nao_nulos = df['Authors'].notna()\n",
    "        autores_nao_vazios = df['Authors'].astype(str).str.strip().str.len() > 3\n",
    "        validacao['autores_validos'] = (autores_nao_nulos & autores_nao_vazios).sum()\n",
    "        print(f\"👥 Autores válidos: {validacao['autores_validos']:,}/{len(df):,} ({validacao['autores_validos']/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Verificar duplicatas potenciais por título\n",
    "    if 'Article Title' in df.columns:\n",
    "        duplicatas = df['Article Title'].duplicated().sum()\n",
    "        validacao['duplicatas_potenciais'] = duplicatas\n",
    "        print(f\"🔄 Duplicatas potenciais: {duplicatas:,} ({duplicatas/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Calcular registros completos\n",
    "    colunas_essenciais = ['Article Title', 'Abstract', 'Authors', 'Publication Year']\n",
    "    colunas_presentes = [col for col in colunas_essenciais if col in df.columns]\n",
    "    \n",
    "    if colunas_presentes:\n",
    "        mask_completos = df[colunas_presentes].notna().all(axis=1)\n",
    "        validacao['registros_completos'] = mask_completos.sum()\n",
    "        print(f\"📋 Registros completos: {validacao['registros_completos']:,}/{len(df):,} ({validacao['registros_completos']/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Calcular qualidade geral\n",
    "    scores = []\n",
    "    if validacao['abstracts_validos'] > 0:\n",
    "        scores.append(validacao['abstracts_validos'] / len(df) * 100)\n",
    "    if validacao['titulos_validos'] > 0:\n",
    "        scores.append(validacao['titulos_validos'] / len(df) * 100)\n",
    "    if validacao['anos_validos'] > 0:\n",
    "        scores.append(validacao['anos_validos'] / len(df) * 100)\n",
    "    if validacao['autores_validos'] > 0:\n",
    "        scores.append(validacao['autores_validos'] / len(df) * 100)\n",
    "    \n",
    "    if scores:\n",
    "        validacao['qualidade_geral'] = sum(scores) / len(scores)\n",
    "        print(f\"\\n📊 Qualidade geral dos dados: {validacao['qualidade_geral']:.1f}/100\")\n",
    "    \n",
    "    # Análise de nano-relevância\n",
    "    print(f\"\\n🔬 ANÁLISE DE RELEVÂNCIA PARA NANOTECNOLOGIA:\")\n",
    "    nano_keywords = ['nano', 'nanoparticle', 'nanotechnology', 'nanomaterial', 'coating', 'paint', 'revestimento']\n",
    "    \n",
    "    if 'Article Title' in df.columns:\n",
    "        nano_mentions_title = 0\n",
    "        for keyword in nano_keywords:\n",
    "            count = df['Article Title'].astype(str).str.lower().str.contains(keyword, na=False).sum()\n",
    "            if count > 0:\n",
    "                print(f\"  📰 '{keyword}' em títulos: {count} menções\")\n",
    "                nano_mentions_title += count\n",
    "        validacao['nano_relevancia_titulo'] = nano_mentions_title\n",
    "    \n",
    "    if 'Abstract' in df.columns:\n",
    "        nano_mentions_abstract = 0\n",
    "        for keyword in nano_keywords:\n",
    "            count = df['Abstract'].astype(str).str.lower().str.contains(keyword, na=False).sum()\n",
    "            if count > 0:\n",
    "                print(f\"  📄 '{keyword}' em abstracts: {count} menções\")\n",
    "                nano_mentions_abstract += count\n",
    "        validacao['nano_relevancia_abstract'] = nano_mentions_abstract\n",
    "    \n",
    "    return validacao\n",
    "\n",
    "def limpar_dados_basico(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Aplica limpeza básica nos dados carregados\"\"\"\n",
    "    \n",
    "    print(\"\\n🧹 APLICANDO LIMPEZA BÁSICA DOS DADOS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    df_clean = df.copy()\n",
    "    linhas_iniciais = len(df_clean)\n",
    "    \n",
    "    # 1. Remover duplicatas exatas\n",
    "    duplicatas_exatas = df_clean.duplicated().sum()\n",
    "    if duplicatas_exatas > 0:\n",
    "        print(f\"🔄 Removendo {duplicatas_exatas:,} duplicatas exatas\")\n",
    "        df_clean = df_clean.drop_duplicates()\n",
    "    else:\n",
    "        print(\"✅ Nenhuma duplicata exata encontrada\")\n",
    "    \n",
    "    # 2. Limpeza de abstracts\n",
    "    if 'Abstract' in df_clean.columns:\n",
    "        print(f\"📄 Limpando abstracts...\")\n",
    "        df_clean['Abstract'] = df_clean['Abstract'].astype(str)\n",
    "        df_clean['Abstract'] = df_clean['Abstract'].str.replace('\\n', ' ').str.replace('\\r', ' ')\n",
    "        df_clean['Abstract'] = df_clean['Abstract'].str.replace('\\t', ' ').str.strip()\n",
    "        df_clean['Abstract'] = df_clean['Abstract'].str.replace(r'\\s+', ' ', regex=True)\n",
    "    \n",
    "    # 3. Limpeza de títulos\n",
    "    if 'Article Title' in df_clean.columns:\n",
    "        print(f\"📰 Limpando títulos...\")\n",
    "        df_clean['Article Title'] = df_clean['Article Title'].astype(str).str.strip()\n",
    "    \n",
    "    # 4. Limpeza de anos\n",
    "    if 'Publication Year' in df_clean.columns:\n",
    "        print(f\"📅 Validando anos de publicação...\")\n",
    "        df_clean['Publication Year'] = pd.to_numeric(df_clean['Publication Year'], errors='coerce')\n",
    "    \n",
    "    linhas_finais = len(df_clean)\n",
    "    print(f\"\\n📊 Resumo da limpeza:\")\n",
    "    print(f\"   📥 Registros iniciais: {linhas_iniciais:,}\")\n",
    "    print(f\"   📤 Registros finais: {linhas_finais:,}\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def filtrar_registros_open_access(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Filtra os registros que possuem classificação de Open Access\"\"\"\n",
    "    \n",
    "    print(\"\\n🔒 FILTRANDO REGISTROS POR OPEN ACCESS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    df_filtrado = df.copy()\n",
    "    coluna_oa = None\n",
    "    possiveis_colunas = ['Open Access Designations', 'Open Access', 'OA', 'Acesso Aberto']\n",
    "    \n",
    "    for col in possiveis_colunas:\n",
    "        if col in df_filtrado.columns:\n",
    "            coluna_oa = col\n",
    "            break\n",
    "    \n",
    "    if coluna_oa is None:\n",
    "        print(\"⚠️ Nenhuma coluna de Open Access encontrada, utilizando dados sem filtro\")\n",
    "        df_filtrado['filtrado_open_access'] = False\n",
    "        return df_filtrado\n",
    "    \n",
    "    registros_iniciais = len(df_filtrado)\n",
    "    df_filtrado = df_filtrado[df_filtrado[coluna_oa].notna()]\n",
    "    df_filtrado = df_filtrado[df_filtrado[coluna_oa].astype(str).str.strip() != '']\n",
    "    df_filtrado = df_filtrado[~df_filtrado[coluna_oa].astype(str).str.lower().isin(['nan', 'none', 'unknown'])]\n",
    "    \n",
    "    registros_finais = len(df_filtrado)\n",
    "    registros_removidos = registros_iniciais - registros_finais\n",
    "    \n",
    "    print(f\"\\n📊 Resumo do filtro de Open Access:\")\n",
    "    print(f\"   📥 Registros iniciais: {registros_iniciais:,}\")\n",
    "    print(f\"   📤 Registros finais: {registros_finais:,}\")\n",
    "    print(f\"   🗑️ Registros removidos: {registros_removidos:,}\")\n",
    "    \n",
    "    df_filtrado['filtrado_open_access'] = True\n",
    "    return df_filtrado\n",
    "\n",
    "# Executar pipeline de processamento\n",
    "if 'df_mapped' in locals() and df_mapped is not None:\n",
    "    print(\"\\n🔄 INICIANDO PIPELINE DE PROCESSAMENTO\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # 1. Validação\n",
    "    print(\"\\n📋 Etapa 1: Validação dos dados\")\n",
    "    validacao_resultado = validar_dados_wos(df_mapped)\n",
    "    \n",
    "    # 2. Limpeza básica\n",
    "    print(\"\\n🧹 Etapa 2: Limpeza básica\")\n",
    "    df_clean = limpar_dados_basico(df_mapped)\n",
    "    \n",
    "    # 3. Filtro Open Access\n",
    "    print(\"\\n🔒 Etapa 3: Filtragem por Open Access\")\n",
    "    df_clean = filtrar_registros_open_access(df_clean)\n",
    "    \n",
    "    # 4. Salvar e disponibilizar dados processados\n",
    "    dataset_processado = df_clean\n",
    "    print(\"\\n✅ Pipeline de processamento concluído com sucesso!\")\n",
    "    print(f\"📊 Registros finais: {len(dataset_processado):,}\")\n",
    "    print(f\"📋 Colunas: {len(dataset_processado.columns)}\")\n",
    "    print(\"\\n🔗 Variável 'dataset_processado' disponível para próximos notebooks\")\n",
    "else:\n",
    "    print(\"⚠️ Não foi possível iniciar o processamento - dados não disponíveis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60f5404a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💾 SALVANDO DADOS PROCESSADOS\n",
      "===================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Dataset salvo em: /home/delon/Modelos/modeloCenanoInk/data/processed/dataset_wos_processado_20250610_121255.csv\n",
      "📋 Metadados salvos em: /home/delon/Modelos/modeloCenanoInk/data/processed/metadados_processamento_20250610_121255.json\n",
      "🔗 Referência criada: /home/delon/Modelos/modeloCenanoInk/data/processed/dataset_wos_latest.csv\n",
      "🎲 Sample (1000 registros) salvo: /home/delon/Modelos/modeloCenanoInk/data/processed/dataset_wos_sample_1000.csv\n",
      "\n",
      "✅ Salvamento concluído!\n",
      "📂 Diretório: /home/delon/Modelos/modeloCenanoInk/data/processed/\n",
      "📊 Total de arquivos criados: 3-4 arquivos\n",
      "💾 Dados salvos com sucesso\n",
      "\n",
      "🔗 Variável 'dataset_processado' disponível para próximos notebooks\n",
      "🔗 Referência criada: /home/delon/Modelos/modeloCenanoInk/data/processed/dataset_wos_latest.csv\n",
      "🎲 Sample (1000 registros) salvo: /home/delon/Modelos/modeloCenanoInk/data/processed/dataset_wos_sample_1000.csv\n",
      "\n",
      "✅ Salvamento concluído!\n",
      "📂 Diretório: /home/delon/Modelos/modeloCenanoInk/data/processed/\n",
      "📊 Total de arquivos criados: 3-4 arquivos\n",
      "💾 Dados salvos com sucesso\n",
      "\n",
      "🔗 Variável 'dataset_processado' disponível para próximos notebooks\n"
     ]
    }
   ],
   "source": [
    "# 💾 Salvamento dos dados processados\n",
    "\n",
    "def salvar_dados_processados(df: pd.DataFrame, validacao: Dict) -> str:\n",
    "    \"\"\"Salva os dados processados e metadados\"\"\"\n",
    "    \n",
    "    print(\"\\n💾 SALVANDO DADOS PROCESSADOS\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    def convert_numpy_types(obj):\n",
    "        \"\"\"Convert numpy types to native Python types for JSON serialization\"\"\"\n",
    "        if isinstance(obj, (np.integer, np.int64, np.int32)):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, (np.floating, np.float64, np.float32)):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, dict):\n",
    "            return {k: convert_numpy_types(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [convert_numpy_types(i) for i in obj]\n",
    "        return obj\n",
    "    \n",
    "    # Criar timestamp para nomeação\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Definir caminhos\n",
    "    processed_dir = PATHS.get('processed', './processed/')\n",
    "    Path(processed_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Salvar DataFrame principal\n",
    "    arquivo_dados = f\"{processed_dir}dataset_wos_processado_{timestamp}.csv\"\n",
    "    df.to_csv(arquivo_dados, index=False, encoding='utf-8-sig')\n",
    "    print(f\"📊 Dataset salvo em: {arquivo_dados}\")\n",
    "    \n",
    "    # Verificar se o filtro Open Access foi aplicado\n",
    "    filtro_oa_aplicado = 'filtrado_open_access' in df.columns\n",
    "    registros_open_access = int(df['filtrado_open_access'].sum()) if filtro_oa_aplicado else 0\n",
    "    \n",
    "    # Salvar metadados\n",
    "    metadados = {\n",
    "        'timestamp_processamento': timestamp,\n",
    "        'total_registros': len(df),\n",
    "        'total_colunas': len(df.columns),\n",
    "        'validacao': convert_numpy_types(validacao),\n",
    "        'colunas': list(df.columns),\n",
    "        'arquivos_origem': df['arquivo_origem'].unique().tolist() if 'arquivo_origem' in df.columns else [],\n",
    "        'estatisticas_basicas': {\n",
    "            'registros_alta_qualidade': int(df['qualidade_alta'].sum()) if 'qualidade_alta' in df.columns else 0,\n",
    "            'registros_open_access': registros_open_access,\n",
    "            'filtro_open_access_aplicado': filtro_oa_aplicado,\n",
    "            'anos_publicacao_range': {\n",
    "                'min': int(df['Publication Year'].min()) if 'Publication Year' in df.columns and df['Publication Year'].notna().any() else None,\n",
    "                'max': int(df['Publication Year'].max()) if 'Publication Year' in df.columns and df['Publication Year'].notna().any() else None\n",
    "            } if 'Publication Year' in df.columns else None\n",
    "        },\n",
    "        'processamento': {\n",
    "            'filtro_open_access': filtro_oa_aplicado,\n",
    "            'limpeza_aplicada': True,\n",
    "            'data_processamento': datetime.now().isoformat()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    arquivo_metadados = f\"{processed_dir}metadados_processamento_{timestamp}.json\"\n",
    "    with open(arquivo_metadados, 'w', encoding='utf-8') as f:\n",
    "        json.dump(metadados, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"📋 Metadados salvos em: {arquivo_metadados}\")\n",
    "    \n",
    "    # Criar arquivo de referência mais simples para uso futuro\n",
    "    arquivo_simples = f\"{processed_dir}dataset_wos_latest.csv\"\n",
    "    df.to_csv(arquivo_simples, index=False, encoding='utf-8-sig')\n",
    "    print(f\"🔗 Referência criada: {arquivo_simples}\")\n",
    "    \n",
    "    # Salvar apenas subset para análises teste\n",
    "    if len(df) > 1000:\n",
    "        df_sample = df.sample(n=1000, random_state=42)\n",
    "        arquivo_sample = f\"{processed_dir}dataset_wos_sample_1000.csv\"\n",
    "        df_sample.to_csv(arquivo_sample, index=False, encoding='utf-8-sig')\n",
    "        print(f\"🎲 Sample (1000 registros) salvo: {arquivo_sample}\")\n",
    "    \n",
    "    print(f\"\\n✅ Salvamento concluído!\")\n",
    "    print(f\"📂 Diretório: {processed_dir}\")\n",
    "    print(f\"📊 Total de arquivos criados: 3-4 arquivos\")\n",
    "    \n",
    "    return arquivo_dados\n",
    "\n",
    "# Salvar se dados foram processados\n",
    "if 'df_clean' in locals() and df_clean is not None and 'validacao_resultado' in locals():\n",
    "    arquivo_salvo = salvar_dados_processados(df_clean, validacao_resultado)\n",
    "    print(\"💾 Dados salvos com sucesso\")\n",
    "    \n",
    "    # Disponibilizar dados para próximos notebooks\n",
    "    dataset_processado = df_clean\n",
    "    print(\"\\n🔗 Variável 'dataset_processado' disponível para próximos notebooks\")\n",
    "else:\n",
    "    print(\"⚠️ Pulando salvamento - dados não processados completamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9cff59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 RESUMO FINAL DO CARREGAMENTO E PROCESSAMENTO\n",
      "=======================================================\n",
      "✅ Status: Processamento concluído com sucesso\n",
      "📊 Registros finais: 9,046\n",
      "📋 Colunas: 76\n",
      "🔒 Registros Open Access: 9,046 (100% dos registros)\n",
      "📅 Período de publicação: 2014 - 2025\n",
      "🔬 Relevância nano (títulos): 25,973 menções\n",
      "🔬 Relevância nano (abstracts): 50,949 menções\n",
      "\n",
      "🔄 Pipeline completo:\n",
      "   ✅ 1. 📥 Carregamento de dados brutos\n",
      "   ✅ 2. 🗺️ Mapeamento e padronização de colunas\n",
      "   ✅ 3. 🔍 Validação de qualidade dos dados\n",
      "   ✅ 4. 🧹 Limpeza e preprocessamento básico\n",
      "   ✅ 5. 🔒 Filtragem por Open Access\n",
      "   ✅ 6. 💾 Salvamento e geração de metadados\n",
      "\n",
      "🎯 PRÓXIMOS PASSOS:\n",
      "   1. Execute '03_analise_regex.ipynb' para análise de padrões\n",
      "   2. Ou execute '04_analise_gemini.ipynb' para análise com IA\n",
      "   3. Dados disponíveis na variável 'dataset_processado'\n",
      "\n",
      "📋 Amostra dos dados (primeiros 3 registros):\n",
      "\n",
      "   📄 Registro 1:\n",
      "      Article Title: Functionally Gradient Coating of Aluminum Alloy via In Situ Arc Surface Nitriding with Subsequent Fr...\n",
      "      Abstract: Functionally gradient coating on the AA6082-T6 substrate is successfully fabricated to improve the w...\n",
      "      Publication Year: 2019...\n",
      "\n",
      "   📄 Registro 2:\n",
      "      Article Title: In situ Y2Si2O7 coatings on SiC fibers: Thermodynamic analysis and processing...\n",
      "      Abstract: It is shown using thermodynamic analysis and kinetic modeling that a processing window exists for th...\n",
      "      Publication Year: 2019...\n",
      "\n",
      "   📄 Registro 3:\n",
      "      Article Title: Nanoparticles of Ni1-xZnxFe2O4 used as Microwave Absorbers in the X-band...\n",
      "      Abstract: Ferrites nanoparticles of Ni1-xZnxFe2O4 ferrites with x varying between 0 and 1, in steps of 0.2, we...\n",
      "      Publication Year: 2019...\n",
      "\n",
      "🎉 Notebook de carregamento concluído!\n"
     ]
    }
   ],
   "source": [
    "# 📊 Resumo final do carregamento\n",
    "\n",
    "print(\"📊 RESUMO FINAL DO CARREGAMENTO E PROCESSAMENTO\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "if 'dataset_processado' in locals():\n",
    "    df_final = dataset_processado\n",
    "    \n",
    "    print(f\"✅ Status: Processamento concluído com sucesso\")\n",
    "    print(f\"📊 Registros finais: {len(df_final):,}\")\n",
    "    print(f\"📋 Colunas: {len(df_final.columns)}\")\n",
    "    \n",
    "    # Destacar informação sobre Open Access\n",
    "    if 'filtrado_open_access' in df_final.columns:\n",
    "        oa_count = df_final['filtrado_open_access'].sum()\n",
    "        if oa_count == len(df_final):\n",
    "            print(f\"🔒 Registros Open Access: {oa_count:,} (100% dos registros)\")\n",
    "        else:\n",
    "            print(f\"🔒 Registros Open Access: {oa_count:,} ({oa_count/len(df_final)*100:.1f}% dos registros)\")\n",
    "    else:\n",
    "        print(f\"🔒 Filtro Open Access: Não aplicado\")\n",
    "    \n",
    "    if 'qualidade_alta' in df_final.columns:\n",
    "        alta_qualidade = df_final['qualidade_alta'].sum()\n",
    "        print(f\"⭐ Registros alta qualidade: {alta_qualidade:,} ({alta_qualidade/len(df_final)*100:.1f}%)\")\n",
    "    \n",
    "    if 'Publication Year' in df_final.columns:\n",
    "        anos_validos = df_final['Publication Year'].notna().sum()\n",
    "        if anos_validos > 0:\n",
    "            ano_min = int(df_final['Publication Year'].min())\n",
    "            ano_max = int(df_final['Publication Year'].max())\n",
    "            print(f\"📅 Período de publicação: {ano_min} - {ano_max}\")\n",
    "    \n",
    "    # Verificar relevância para nanotecnologia\n",
    "    if 'validacao_resultado' in locals():\n",
    "        nano_titulo = validacao_resultado.get('nano_relevancia_titulo', 0)\n",
    "        nano_abstract = validacao_resultado.get('nano_relevancia_abstract', 0)\n",
    "        print(f\"🔬 Relevância nano (títulos): {nano_titulo:,} menções\")\n",
    "        print(f\"🔬 Relevância nano (abstracts): {nano_abstract:,} menções\")\n",
    "    \n",
    "    # Pipeline completo\n",
    "    pipeline_steps = [\n",
    "        \"1. 📥 Carregamento de dados brutos\",\n",
    "        \"2. 🗺️ Mapeamento e padronização de colunas\", \n",
    "        \"3. 🔍 Validação de qualidade dos dados\",\n",
    "        \"4. 🧹 Limpeza e preprocessamento básico\",\n",
    "        \"5. 🔒 Filtragem por Open Access\",\n",
    "        \"6. 💾 Salvamento e geração de metadados\"\n",
    "    ]\n",
    "    print(f\"\\n🔄 Pipeline completo:\")\n",
    "    for step in pipeline_steps:\n",
    "        print(f\"   ✅ {step}\")\n",
    "    \n",
    "    print(f\"\\n🎯 PRÓXIMOS PASSOS:\")\n",
    "    print(f\"   1. Execute '03_analise_regex.ipynb' para análise de padrões\")\n",
    "    print(f\"   2. Ou execute '04_analise_gemini.ipynb' para análise com IA\")\n",
    "    print(f\"   3. Dados disponíveis na variável 'dataset_processado'\")\n",
    "    \n",
    "    # Estatísticas rápidas de amostra\n",
    "    print(f\"\\n📋 Amostra dos dados (primeiros 3 registros):\")\n",
    "    colunas_mostrar = ['Article Title', 'Abstract', 'Publication Year']\n",
    "    colunas_existentes = [col for col in colunas_mostrar if col in df_final.columns]\n",
    "    \n",
    "    if colunas_existentes:\n",
    "        for i in range(min(3, len(df_final))):\n",
    "            print(f\"\\n   📄 Registro {i+1}:\")\n",
    "            for col in colunas_existentes:\n",
    "                valor = str(df_final.iloc[i][col])[:100]\n",
    "                print(f\"      {col}: {valor}...\")\n",
    "else:\n",
    "    print(\"❌ Status: Processamento não foi concluído\")\n",
    "    print(\"💡 Verifique os passos anteriores para identificar problemas\")\n",
    "\n",
    "print(\"\\n🎉 Notebook de carregamento concluído!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
