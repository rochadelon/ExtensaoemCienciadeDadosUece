{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b96fde08",
   "metadata": {},
   "source": [
    "# 🔍 03 - Análise de Padrões com Regex\n",
    "\n",
    "## 📖 Visão Geral\n",
    "\n",
    "Este notebook realiza análise baseada em expressões regulares para identificar nanomateriais, propriedades funcionais e técnicas de preparação nos textos científicos.\n",
    "\n",
    "### 🎯 Responsabilidades\n",
    "\n",
    "- ✅ Identificação de nanomateriais usando padrões regex avançados\n",
    "- ✅ Extração de propriedades funcionais de tintas e revestimentos\n",
    "- ✅ Detecção de técnicas de preparação e aplicação\n",
    "- ✅ Análise de co-ocorrências entre materiais e propriedades\n",
    "- ✅ Geração de matrizes de relacionamento\n",
    "- ✅ Preparação de dados para análise Gemini\n",
    "\n",
    "### 📦 Dependências\n",
    "\n",
    "- pandas\n",
    "- numpy\n",
    "- re (regex)\n",
    "- json\n",
    "- plotly (para visualizações)\n",
    "- networkx (para análise de redes)\n",
    "\n",
    "### 🔗 Notebooks Relacionados\n",
    "\n",
    "- **Anterior**: `02_carregamento_dados.ipynb`\n",
    "- **Próximo**: `04_analise_gemini.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2626952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Plotly disponível para visualizações\n",
      "✅ NetworkX disponível para análise de redes\n",
      "\n",
      "📦 Imports concluídos\n"
     ]
    }
   ],
   "source": [
    "# 📦 Imports e carregamento de dados\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Set, Optional, Any\n",
    "from collections import Counter, defaultdict\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Tentar importar bibliotecas opcionais\n",
    "try:\n",
    "    import plotly.graph_objects as go\n",
    "    import plotly.express as px\n",
    "    from plotly.subplots import make_subplots\n",
    "    PLOTLY_AVAILABLE = True\n",
    "    print(\"✅ Plotly disponível para visualizações\")\n",
    "except ImportError:\n",
    "    PLOTLY_AVAILABLE = False\n",
    "    print(\"⚠️ Plotly não disponível - visualizações desabilitadas\")\n",
    "\n",
    "try:\n",
    "    import networkx as nx\n",
    "    NETWORKX_AVAILABLE = True\n",
    "    print(\"✅ NetworkX disponível para análise de redes\")\n",
    "except ImportError:\n",
    "    NETWORKX_AVAILABLE = False\n",
    "    print(\"⚠️ NetworkX não disponível - análise de redes desabilitada\")\n",
    "\n",
    "print(\"\\n📦 Imports concluídos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe07526",
   "metadata": {},
   "source": [
    "## 🔧 Carregamento de Configurações e Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd77096a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configurações carregadas do sistema\n",
      "   Sistema configurado: True\n",
      "   Nanomateriais disponíveis: 6 categorias\n",
      "   Palavras-chave disponíveis: 4 categorias\n",
      "\n",
      "📊 Carregando dados processados...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dados carregados: 9,046 registros × 76 colunas\n",
      "📋 Colunas essenciais presentes: 2/2\n",
      "\n",
      "🔍 Colunas relacionadas ao Open Access encontradas: ['Early Access Date', 'Open Access Designations', 'filtrado_open_access']\n",
      "📊 Registros já filtrados por Open Access: 9,046\n"
     ]
    }
   ],
   "source": [
    "# 🔧 Carregamento de configurações e dados\n",
    "\n",
    "# Carregar configurações do sistema\n",
    "config_file = 'config_sistema.json'\n",
    "if os.path.exists(config_file):\n",
    "    with open(config_file, 'r', encoding='utf-8') as f:\n",
    "        config_data = json.load(f)\n",
    "    \n",
    "    SISTEMA_CONFIGURADO = config_data.get('sistema_configurado', False)\n",
    "    PATHS = config_data.get('paths', {})\n",
    "    CONFIG = config_data.get('config', {})\n",
    "    NANOMATERIAIS_DB = config_data.get('nanomateriais', {})\n",
    "    PALAVRAS_CHAVE_DB = config_data.get('palavras_chave', {})\n",
    "    \n",
    "    print(\"✅ Configurações carregadas do sistema\")\n",
    "    print(f\"   Sistema configurado: {SISTEMA_CONFIGURADO}\")\n",
    "    print(f\"   Nanomateriais disponíveis: {len(NANOMATERIAIS_DB)} categorias\")\n",
    "    print(f\"   Palavras-chave disponíveis: {len(PALAVRAS_CHAVE_DB)} categorias\")\n",
    "else:\n",
    "    print(\"❌ Arquivo de configuração não encontrado!\")\n",
    "    print(\"💡 Execute primeiro o notebook '01_configuracao_sistema.ipynb'\")\n",
    "    raise FileNotFoundError(\"Configuração do sistema necessária\")\n",
    "\n",
    "# Tentar carregar dados do notebook anterior\n",
    "processed_dir = PATHS.get('processed', './processed/')\n",
    "latest_data_file = os.path.join(processed_dir, 'dataset_wos_latest.csv')\n",
    "\n",
    "if os.path.exists(latest_data_file):\n",
    "    print(f\"\\n📊 Carregando dados processados...\")\n",
    "    df_data = pd.read_csv(latest_data_file, encoding='utf-8-sig')\n",
    "    print(f\"✅ Dados carregados: {len(df_data):,} registros × {len(df_data.columns)} colunas\")\n",
    "    \n",
    "    # Verificar colunas essenciais\n",
    "    colunas_essenciais = ['Article Title', 'Abstract']\n",
    "    colunas_presentes = [col for col in colunas_essenciais if col in df_data.columns]\n",
    "    print(f\"📋 Colunas essenciais presentes: {len(colunas_presentes)}/{len(colunas_essenciais)}\")\n",
    "    \n",
    "    if len(colunas_presentes) < len(colunas_essenciais):\n",
    "        print(f\"⚠️ Colunas faltantes: {set(colunas_essenciais) - set(colunas_presentes)}\")\n",
    "    \n",
    "    # Verificar colunas de Open Access disponíveis\n",
    "    open_access_columns = [col for col in df_data.columns if 'open' in col.lower() or 'access' in col.lower()]\n",
    "    print(f\"\\n🔍 Colunas relacionadas ao Open Access encontradas: {open_access_columns}\")\n",
    "    \n",
    "    # Verificar se há coluna de filtro\n",
    "    if 'filtrado_open_access' in df_data.columns:\n",
    "        oa_filtered = df_data['filtrado_open_access'].sum()\n",
    "        print(f\"📊 Registros já filtrados por Open Access: {oa_filtered:,}\")\n",
    "else:\n",
    "    print(\"❌ Dados processados não encontrados!\")\n",
    "    print(\"💡 Execute primeiro o notebook '02_carregamento_dados.ipynb'\")\n",
    "    raise FileNotFoundError(\"Dados processados necessários\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4576d60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔒 Filtro de Open Access aplicado via coluna 'Open Access Designations': 9,046 registros\n",
      "📉 Registros removidos: 0\n",
      "📊 Dataset final para análise regex: 9,046 registros\n",
      "✅ Confirmação: Todos os 9,046 registros são Open Access\n",
      "\n",
      "🔒 VERIFICANDO E APLICANDO FILTRO DE OPEN ACCESS\n",
      "==================================================\n",
      "✅ Dados já estão filtrados por Open Access: 9,046 registros (100%)\n",
      "\n",
      "✅ Dataset final para análise: 9,046 registros\n",
      "📊 Colunas disponíveis: 76\n"
     ]
    }
   ],
   "source": [
    "# 🔎 Verificar e usar dados filtrados por Open Access se disponíveis\n",
    "if 'Open Access' in df_data.columns:\n",
    "    # Aplicar filtro de Open Access\n",
    "    df_clean = df_data[df_data['Open Access'] == True]\n",
    "    print(f\"🔒 Filtro de Open Access aplicado: {len(df_clean):,} registros de Open Access encontrados\")\n",
    "    print(f\"📉 Registros removidos (não Open Access): {len(df_data) - len(df_clean):,}\")\n",
    "else:\n",
    "    # Verificar outras colunas possíveis de Open Access\n",
    "    possiveis_colunas_oa = ['Open Access Designations', 'OA', 'Acesso Aberto']\n",
    "    coluna_oa_encontrada = None\n",
    "    \n",
    "    for col in possiveis_colunas_oa:\n",
    "        if col in df_data.columns:\n",
    "            coluna_oa_encontrada = col\n",
    "            break\n",
    "    \n",
    "    if coluna_oa_encontrada:\n",
    "        # Filtrar por registros com classificação de Open Access não vazia\n",
    "        df_clean = df_data[df_data[coluna_oa_encontrada].notna()]\n",
    "        df_clean = df_clean[df_clean[coluna_oa_encontrada].astype(str).str.strip() != '']\n",
    "        df_clean = df_clean[~df_clean[coluna_oa_encontrada].astype(str).str.lower().isin(['nan', 'none', 'unknown'])]\n",
    "        print(f\"🔒 Filtro de Open Access aplicado via coluna '{coluna_oa_encontrada}': {len(df_clean):,} registros\")\n",
    "        print(f\"📉 Registros removidos: {len(df_data) - len(df_clean):,}\")\n",
    "    else:\n",
    "        # Verificar se os dados já vieram filtrados (coluna filtrado_open_access)\n",
    "        if 'filtrado_open_access' in df_data.columns:\n",
    "            df_clean = df_data.copy()\n",
    "            registros_oa = df_data['filtrado_open_access'].sum()\n",
    "            print(f\"🔍 Dados já filtrados por Open Access: {len(df_clean):,} registros ({registros_oa:,} marcados como Open Access)\")\n",
    "        else:\n",
    "            df_clean = df_data.copy()\n",
    "            print(\"⚠️ Nenhuma coluna de Open Access encontrada. Usando todos os dados.\")\n",
    "\n",
    "print(f\"📊 Dataset final para análise regex: {len(df_clean):,} registros\")\n",
    "\n",
    "# Verificar se realmente temos dados de Open Access\n",
    "if 'filtrado_open_access' in df_clean.columns:\n",
    "    oa_marcados = df_clean['filtrado_open_access'].sum()\n",
    "    if oa_marcados == len(df_clean):\n",
    "        print(f\"✅ Confirmação: Todos os {len(df_clean):,} registros são Open Access\")\n",
    "    else:\n",
    "        print(f\"⚠️ Aviso: {oa_marcados:,} de {len(df_clean):,} registros marcados como Open Access\")\n",
    "        \n",
    "# 🔎 Aplicar filtro de Open Access se disponível\n",
    "print(\"\\n🔒 VERIFICANDO E APLICANDO FILTRO DE OPEN ACCESS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Verificar se dados já foram filtrados\n",
    "if 'filtrado_open_access' in df_data.columns:\n",
    "    # Dados já foram filtrados no notebook anterior\n",
    "    open_access_count = df_data['filtrado_open_access'].sum()\n",
    "    if open_access_count == len(df_data):\n",
    "        print(f\"✅ Dados já estão filtrados por Open Access: {len(df_data):,} registros (100%)\")\n",
    "        df_clean = df_data.copy()\n",
    "    else:\n",
    "        print(f\"⚠️ Dados parcialmente filtrados. Aplicando filtro completo...\")\n",
    "        df_clean = df_data[df_data['filtrado_open_access'] == True]\n",
    "        print(f\"📊 Após filtro: {len(df_clean):,} registros Open Access\")\n",
    "else:\n",
    "    # Tentar encontrar colunas de Open Access\n",
    "    open_access_columns = [col for col in df_data.columns if 'open' in col.lower() and 'access' in col.lower()]\n",
    "    \n",
    "    if open_access_columns:\n",
    "        oa_col = open_access_columns[0]\n",
    "        print(f\"🔍 Usando coluna: '{oa_col}'\")\n",
    "        \n",
    "        # Filtrar por Open Access\n",
    "        initial_count = len(df_data)\n",
    "        df_clean = df_data[df_data[oa_col].notna()]\n",
    "        df_clean = df_clean[df_clean[oa_col].astype(str).str.strip() != '']\n",
    "        df_clean = df_clean[~df_clean[oa_col].astype(str).str.lower().isin(['nan', 'none', 'unknown', ''])]\n",
    "        \n",
    "        final_count = len(df_clean)\n",
    "        removed_count = initial_count - final_count\n",
    "        \n",
    "        print(f\"📊 Registros iniciais: {initial_count:,}\")\n",
    "        print(f\"📊 Registros finais: {final_count:,}\")\n",
    "        print(f\"🗑️ Registros removidos: {removed_count:,} ({removed_count/initial_count*100:.1f}%)\")\n",
    "        \n",
    "        # Adicionar flag\n",
    "        df_clean['filtrado_open_access'] = True\n",
    "    else:\n",
    "        print(\"⚠️ Nenhuma coluna de Open Access encontrada. Usando todos os dados.\")\n",
    "        df_clean = df_data.copy()\n",
    "        df_clean['filtrado_open_access'] = False\n",
    "\n",
    "# Atualizar df_data para usar dados filtrados no resto do notebook\n",
    "df_data = df_clean\n",
    "print(f\"\\n✅ Dataset final para análise: {len(df_data):,} registros\")\n",
    "print(f\"📊 Colunas disponíveis: {len(df_data.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98c5f716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 RESUMO DO CARREGAMENTO DE DADOS:\n",
      "========================================\n",
      "📁 Dados carregados de: /home/delon/Modelos/modeloCenanoInk/data/processed/dataset_wos_latest.csv\n",
      "📊 Total de registros: 9,046\n",
      "📋 Total de colunas: 76\n",
      "✅ Filtro Open Access: APLICADO\n",
      "🔒 Registros Open Access: 9,046 (100.0% do total)\n",
      "\n",
      "🏁 Dataset pronto para análise regex com 9,046 registros\n"
     ]
    }
   ],
   "source": [
    "# 📊 Resumo do carregamento e filtro\n",
    "print(\"\\n📋 RESUMO DO CARREGAMENTO DE DADOS:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"📁 Dados carregados de: {latest_data_file}\")\n",
    "print(f\"📊 Total de registros: {len(df_data):,}\")\n",
    "print(f\"📋 Total de colunas: {len(df_data.columns)}\")\n",
    "\n",
    "if 'filtrado_open_access' in df_data.columns:\n",
    "    open_access_aplicado = df_data['filtrado_open_access'].any()\n",
    "    if open_access_aplicado:\n",
    "        print(f\"✅ Filtro Open Access: APLICADO\")\n",
    "        oa_count = df_data['filtrado_open_access'].sum()\n",
    "        print(f\"🔒 Registros Open Access: {oa_count:,} ({oa_count/len(df_data)*100:.1f}% do total)\")\n",
    "    else:\n",
    "        print(f\"⚠️ Filtro Open Access: NÃO APLICADO\")\n",
    "else:\n",
    "    print(f\"⚠️ Filtro Open Access: COLUNA NÃO ENCONTRADA\")\n",
    "\n",
    "print(f\"\\n🏁 Dataset pronto para análise regex com {len(df_data):,} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26372066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Inicializando analisador regex...\n",
      "🔧 Compilando padrões regex...\n",
      "✅ 12 padrões regex compilados\n",
      "✅ Analisador regex pronto\n"
     ]
    }
   ],
   "source": [
    "# 🎯 Definição de padrões regex avançados\n",
    "\n",
    "class RegexAnalyzer:\n",
    "    \"\"\"Classe para análise de padrões regex em textos científicos\"\"\"\n",
    "    \n",
    "    def __init__(self, nanomateriais_db: Dict, palavras_chave_db: Dict):\n",
    "        self.nanomateriais_db = nanomateriais_db\n",
    "        self.palavras_chave_db = palavras_chave_db\n",
    "        self.compiled_patterns = {}\n",
    "        self._compile_patterns()\n",
    "    \n",
    "    def _compile_patterns(self):\n",
    "        \"\"\"Compila todos os padrões regex para melhor performance\"\"\"\n",
    "        \n",
    "        print(\"🔧 Compilando padrões regex...\")\n",
    "        \n",
    "        # Padrões para nanomateriais\n",
    "        for categoria, materiais in self.nanomateriais_db.items():\n",
    "            patterns = []\n",
    "            for material in materiais:\n",
    "                # Padrão básico\n",
    "                pattern = rf'\\b{re.escape(material)}\\b'\n",
    "                patterns.append(pattern)\n",
    "                \n",
    "                # Variações com nano-prefix\n",
    "                if not material.lower().startswith('nano'):\n",
    "                    nano_pattern = rf'\\bnano[-\\s]?{re.escape(material)}\\b'\n",
    "                    patterns.append(nano_pattern)\n",
    "                \n",
    "                # Padrões para abreviações químicas\n",
    "                if len(material) <= 6 and material.isupper():\n",
    "                    abbrev_pattern = rf'\\b{re.escape(material)}(?:[\\s-]?NPs?|[\\s-]?nanoparticles?)?\\b'\n",
    "                    patterns.append(abbrev_pattern)\n",
    "            \n",
    "            # Compilar padrão combinado\n",
    "            combined_pattern = '|'.join(patterns)\n",
    "            self.compiled_patterns[f'nano_{categoria}'] = re.compile(\n",
    "                combined_pattern, re.IGNORECASE | re.MULTILINE\n",
    "            )\n",
    "        \n",
    "        # Padrões para palavras-chave funcionais\n",
    "        for categoria, palavras in self.palavras_chave_db.items():\n",
    "            patterns = []\n",
    "            for palavra in palavras:\n",
    "                # Padrão básico com word boundaries\n",
    "                pattern = rf'\\b{re.escape(palavra)}\\b'\n",
    "                patterns.append(pattern)\n",
    "                \n",
    "                # Variações plurais e compostas\n",
    "                if not palavra.endswith('s'):\n",
    "                    plural_pattern = rf'\\b{re.escape(palavra)}s?\\b'\n",
    "                    patterns.append(plural_pattern)\n",
    "            \n",
    "            combined_pattern = '|'.join(patterns)\n",
    "            self.compiled_patterns[f'func_{categoria}'] = re.compile(\n",
    "                combined_pattern, re.IGNORECASE | re.MULTILINE\n",
    "            )\n",
    "        \n",
    "        # Padrões especiais para medidas e valores\n",
    "        self.compiled_patterns['medidas'] = re.compile(\n",
    "            r'\\b(?:\\d+[.,]?\\d*)\\s*(?:nm|μm|mm|cm|m|ml|l|g|kg|%|°C|K|Pa|MPa|GPa)\\b',\n",
    "            re.IGNORECASE\n",
    "        )\n",
    "        \n",
    "        # Padrões para métodos de síntese\n",
    "        synthesis_methods = [\n",
    "            'sol-gel', 'chemical vapor deposition', 'CVD', 'physical vapor deposition', 'PVD',\n",
    "            'electrochemical', 'hydrothermal', 'solvothermal', 'precipitation',\n",
    "            'co-precipitation', 'ball milling', 'spray pyrolysis'\n",
    "        ]\n",
    "        synthesis_pattern = '|'.join([rf'\\b{re.escape(method)}\\b' for method in synthesis_methods])\n",
    "        self.compiled_patterns['sintese'] = re.compile(synthesis_pattern, re.IGNORECASE)\n",
    "        \n",
    "        print(f\"✅ {len(self.compiled_patterns)} padrões regex compilados\")\n",
    "    \n",
    "    def extract_matches(self, text: str, pattern_category: str) -> List[str]:\n",
    "        \"\"\"Extrai todas as correspondências de um padrão específico\"\"\"\n",
    "        if not text or not isinstance(text, str):\n",
    "            return []\n",
    "        \n",
    "        if pattern_category not in self.compiled_patterns:\n",
    "            return []\n",
    "        \n",
    "        matches = self.compiled_patterns[pattern_category].findall(text)\n",
    "        # Limpar e normalizar matches\n",
    "        clean_matches = [match.strip().lower() for match in matches if match.strip()]\n",
    "        return list(set(clean_matches))  # Remover duplicatas\n",
    "    \n",
    "    def extract_all_patterns(self, text: str) -> Dict[str, List[str]]:\n",
    "        \"\"\"Extrai todos os padrões de um texto\"\"\"\n",
    "        results = {}\n",
    "        for pattern_name in self.compiled_patterns.keys():\n",
    "            results[pattern_name] = self.extract_matches(text, pattern_name)\n",
    "        return results\n",
    "    \n",
    "    def analyze_text_comprehensive(self, text: str) -> Dict[str, any]:\n",
    "        \"\"\"Análise abrangente de um texto\"\"\"\n",
    "        if not text or not isinstance(text, str):\n",
    "            return {}\n",
    "        # Iniciando análise abrangente\n",
    "        analysis = {\n",
    "            'text_length': len(text),\n",
    "            'word_count': len(text.split()),\n",
    "            'patterns_found': {},\n",
    "            'total_matches': 0,\n",
    "            'nano_relevance_score': 0,\n",
    "            'functional_relevance_score': 0\n",
    "        }\n",
    "        \n",
    "        # Extrair todos os padrões\n",
    "        all_patterns = self.extract_all_patterns(text)\n",
    "        analysis['patterns_found'] = all_patterns\n",
    "        \n",
    "        # Calcular scores de relevância\n",
    "        nano_matches = 0\n",
    "        func_matches = 0\n",
    "        \n",
    "        for pattern_name, matches in all_patterns.items():\n",
    "            match_count = len(matches)\n",
    "            analysis['total_matches'] += match_count\n",
    "            \n",
    "            if pattern_name.startswith('nano_'):\n",
    "                nano_matches += match_count\n",
    "            elif pattern_name.startswith('func_'):\n",
    "                func_matches += match_count\n",
    "        \n",
    "        # Normalizar scores (0-100)\n",
    "        if analysis['word_count'] > 0:\n",
    "            analysis['nano_relevance_score'] = min(100, (nano_matches / analysis['word_count']) * 1000)\n",
    "            analysis['functional_relevance_score'] = min(100, (func_matches / analysis['word_count']) * 1000)\n",
    "        \n",
    "        return analysis\n",
    "\n",
    "# Inicializar o analisador\n",
    "print(\"🚀 Inicializando analisador regex...\")\n",
    "regex_analyzer = RegexAnalyzer(NANOMATERIAIS_DB, PALAVRAS_CHAVE_DB)\n",
    "print(\"✅ Analisador regex pronto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf7a53c",
   "metadata": {},
   "source": [
    "## 🔄 Análise em Lote dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "006f711c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Iniciando análise regex do dataset filtrado...\n",
      "🔄 INICIANDO ANÁLISE REGEX EM LOTE\n",
      "========================================\n",
      "📊 Total de registros: 9,046\n",
      "📦 Tamanho do lote: 50\n",
      "\n",
      "📦 Processando lote 1/181 (1-50)\n",
      "✅ Lote concluído (0.6% total)\n",
      "\n",
      "📦 Processando lote 2/181 (51-100)\n",
      "✅ Lote concluído (0.6% total)\n",
      "\n",
      "📦 Processando lote 2/181 (51-100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Lote concluído (1.1% total)\n",
      "\n",
      "📦 Processando lote 3/181 (101-150)\n",
      "✅ Lote concluído (1.7% total)\n",
      "\n",
      "📦 Processando lote 4/181 (151-200)\n",
      "✅ Lote concluído (2.2% total)\n",
      "\n",
      "📦 Processando lote 5/181 (201-250)\n",
      "✅ Lote concluído (2.8% total)\n",
      "\n",
      "📦 Processando lote 6/181 (251-300)\n",
      "✅ Lote concluído (1.7% total)\n",
      "\n",
      "📦 Processando lote 4/181 (151-200)\n",
      "✅ Lote concluído (2.2% total)\n",
      "\n",
      "📦 Processando lote 5/181 (201-250)\n",
      "✅ Lote concluído (2.8% total)\n",
      "\n",
      "📦 Processando lote 6/181 (251-300)\n",
      "✅ Lote concluído (3.3% total)\n",
      "\n",
      "📦 Processando lote 7/181 (301-350)\n",
      "✅ Lote concluído (3.3% total)\n",
      "\n",
      "📦 Processando lote 7/181 (301-350)\n",
      "✅ Lote concluído (3.9% total)\n",
      "\n",
      "📦 Processando lote 8/181 (351-400)\n",
      "✅ Lote concluído (3.9% total)\n",
      "\n",
      "📦 Processando lote 8/181 (351-400)\n",
      "✅ Lote concluído (4.4% total)\n",
      "\n",
      "📦 Processando lote 9/181 (401-450)\n",
      "✅ Lote concluído (4.4% total)\n",
      "\n",
      "📦 Processando lote 9/181 (401-450)\n",
      "✅ Lote concluído (5.0% total)\n",
      "\n",
      "📦 Processando lote 10/181 (451-500)\n",
      "✅ Lote concluído (5.0% total)\n",
      "\n",
      "📦 Processando lote 10/181 (451-500)\n",
      "✅ Lote concluído (5.5% total)\n",
      "\n",
      "📦 Processando lote 11/181 (501-550)\n",
      "✅ Lote concluído (5.5% total)\n",
      "\n",
      "📦 Processando lote 11/181 (501-550)\n",
      "✅ Lote concluído (6.1% total)\n",
      "\n",
      "📦 Processando lote 12/181 (551-600)\n",
      "✅ Lote concluído (6.1% total)\n",
      "\n",
      "📦 Processando lote 12/181 (551-600)\n",
      "✅ Lote concluído (6.6% total)\n",
      "\n",
      "📦 Processando lote 13/181 (601-650)\n",
      "✅ Lote concluído (6.6% total)\n",
      "\n",
      "📦 Processando lote 13/181 (601-650)\n",
      "✅ Lote concluído (7.2% total)\n",
      "\n",
      "📦 Processando lote 14/181 (651-700)\n",
      "✅ Lote concluído (7.2% total)\n",
      "\n",
      "📦 Processando lote 14/181 (651-700)\n",
      "✅ Lote concluído (7.7% total)\n",
      "\n",
      "📦 Processando lote 15/181 (701-750)\n",
      "✅ Lote concluído (7.7% total)\n",
      "\n",
      "📦 Processando lote 15/181 (701-750)\n",
      "✅ Lote concluído (8.3% total)\n",
      "\n",
      "📦 Processando lote 16/181 (751-800)\n",
      "✅ Lote concluído (8.3% total)\n",
      "\n",
      "📦 Processando lote 16/181 (751-800)\n",
      "✅ Lote concluído (8.8% total)\n",
      "\n",
      "📦 Processando lote 17/181 (801-850)\n",
      "✅ Lote concluído (8.8% total)\n",
      "\n",
      "📦 Processando lote 17/181 (801-850)\n",
      "✅ Lote concluído (9.4% total)\n",
      "\n",
      "📦 Processando lote 18/181 (851-900)\n",
      "✅ Lote concluído (9.9% total)\n",
      "\n",
      "📦 Processando lote 19/181 (901-950)\n",
      "✅ Lote concluído (9.4% total)\n",
      "\n",
      "📦 Processando lote 18/181 (851-900)\n",
      "✅ Lote concluído (9.9% total)\n",
      "\n",
      "📦 Processando lote 19/181 (901-950)\n",
      "✅ Lote concluído (10.5% total)\n",
      "\n",
      "📦 Processando lote 20/181 (951-1000)\n",
      "✅ Lote concluído (10.5% total)\n",
      "\n",
      "📦 Processando lote 20/181 (951-1000)\n",
      "✅ Lote concluído (11.0% total)\n",
      "\n",
      "📦 Processando lote 21/181 (1001-1050)\n",
      "✅ Lote concluído (11.0% total)\n",
      "\n",
      "📦 Processando lote 21/181 (1001-1050)\n",
      "✅ Lote concluído (11.6% total)\n",
      "\n",
      "📦 Processando lote 22/181 (1051-1100)\n",
      "✅ Lote concluído (11.6% total)\n",
      "\n",
      "📦 Processando lote 22/181 (1051-1100)\n",
      "✅ Lote concluído (12.2% total)\n",
      "\n",
      "📦 Processando lote 23/181 (1101-1150)\n",
      "✅ Lote concluído (12.2% total)\n",
      "\n",
      "📦 Processando lote 23/181 (1101-1150)\n",
      "✅ Lote concluído (12.7% total)\n",
      "\n",
      "📦 Processando lote 24/181 (1151-1200)\n",
      "✅ Lote concluído (12.7% total)\n",
      "\n",
      "📦 Processando lote 24/181 (1151-1200)\n",
      "✅ Lote concluído (13.3% total)\n",
      "\n",
      "📦 Processando lote 25/181 (1201-1250)\n",
      "✅ Lote concluído (13.3% total)\n",
      "\n",
      "📦 Processando lote 25/181 (1201-1250)\n",
      "✅ Lote concluído (13.8% total)\n",
      "\n",
      "📦 Processando lote 26/181 (1251-1300)\n",
      "✅ Lote concluído (13.8% total)\n",
      "\n",
      "📦 Processando lote 26/181 (1251-1300)\n",
      "✅ Lote concluído (14.4% total)\n",
      "\n",
      "📦 Processando lote 27/181 (1301-1350)\n",
      "✅ Lote concluído (14.4% total)\n",
      "\n",
      "📦 Processando lote 27/181 (1301-1350)\n",
      "✅ Lote concluído (14.9% total)\n",
      "\n",
      "📦 Processando lote 28/181 (1351-1400)\n",
      "✅ Lote concluído (14.9% total)\n",
      "\n",
      "📦 Processando lote 28/181 (1351-1400)\n",
      "✅ Lote concluído (15.5% total)\n",
      "\n",
      "📦 Processando lote 29/181 (1401-1450)\n",
      "✅ Lote concluído (15.5% total)\n",
      "\n",
      "📦 Processando lote 29/181 (1401-1450)\n",
      "✅ Lote concluído (16.0% total)\n",
      "\n",
      "📦 Processando lote 30/181 (1451-1500)\n",
      "✅ Lote concluído (16.0% total)\n",
      "\n",
      "📦 Processando lote 30/181 (1451-1500)\n",
      "✅ Lote concluído (16.6% total)\n",
      "\n",
      "📦 Processando lote 31/181 (1501-1550)\n",
      "✅ Lote concluído (16.6% total)\n",
      "\n",
      "📦 Processando lote 31/181 (1501-1550)\n",
      "✅ Lote concluído (17.1% total)\n",
      "\n",
      "📦 Processando lote 32/181 (1551-1600)\n",
      "✅ Lote concluído (17.1% total)\n",
      "\n",
      "📦 Processando lote 32/181 (1551-1600)\n",
      "✅ Lote concluído (17.7% total)\n",
      "\n",
      "📦 Processando lote 33/181 (1601-1650)\n",
      "✅ Lote concluído (17.7% total)\n",
      "\n",
      "📦 Processando lote 33/181 (1601-1650)\n",
      "✅ Lote concluído (18.2% total)\n",
      "\n",
      "📦 Processando lote 34/181 (1651-1700)\n",
      "✅ Lote concluído (18.2% total)\n",
      "\n",
      "📦 Processando lote 34/181 (1651-1700)\n",
      "✅ Lote concluído (18.8% total)\n",
      "\n",
      "📦 Processando lote 35/181 (1701-1750)\n",
      "✅ Lote concluído (18.8% total)\n",
      "\n",
      "📦 Processando lote 35/181 (1701-1750)\n",
      "✅ Lote concluído (19.3% total)\n",
      "\n",
      "📦 Processando lote 36/181 (1751-1800)\n",
      "✅ Lote concluído (19.3% total)\n",
      "\n",
      "📦 Processando lote 36/181 (1751-1800)\n",
      "✅ Lote concluído (19.9% total)\n",
      "\n",
      "📦 Processando lote 37/181 (1801-1850)\n",
      "✅ Lote concluído (19.9% total)\n",
      "\n",
      "📦 Processando lote 37/181 (1801-1850)\n",
      "✅ Lote concluído (20.4% total)\n",
      "\n",
      "📦 Processando lote 38/181 (1851-1900)\n",
      "✅ Lote concluído (20.4% total)\n",
      "\n",
      "📦 Processando lote 38/181 (1851-1900)\n",
      "✅ Lote concluído (21.0% total)\n",
      "\n",
      "📦 Processando lote 39/181 (1901-1950)\n",
      "✅ Lote concluído (21.0% total)\n",
      "\n",
      "📦 Processando lote 39/181 (1901-1950)\n",
      "✅ Lote concluído (21.5% total)\n",
      "\n",
      "📦 Processando lote 40/181 (1951-2000)\n",
      "✅ Lote concluído (21.5% total)\n",
      "\n",
      "📦 Processando lote 40/181 (1951-2000)\n",
      "✅ Lote concluído (22.1% total)\n",
      "\n",
      "📦 Processando lote 41/181 (2001-2050)\n",
      "✅ Lote concluído (22.1% total)\n",
      "\n",
      "📦 Processando lote 41/181 (2001-2050)\n",
      "✅ Lote concluído (22.7% total)\n",
      "\n",
      "📦 Processando lote 42/181 (2051-2100)\n",
      "✅ Lote concluído (22.7% total)\n",
      "\n",
      "📦 Processando lote 42/181 (2051-2100)\n",
      "✅ Lote concluído (23.2% total)\n",
      "\n",
      "📦 Processando lote 43/181 (2101-2150)\n",
      "✅ Lote concluído (23.2% total)\n",
      "\n",
      "📦 Processando lote 43/181 (2101-2150)\n",
      "✅ Lote concluído (23.8% total)\n",
      "\n",
      "📦 Processando lote 44/181 (2151-2200)\n",
      "✅ Lote concluído (23.8% total)\n",
      "\n",
      "📦 Processando lote 44/181 (2151-2200)\n",
      "✅ Lote concluído (24.3% total)\n",
      "\n",
      "📦 Processando lote 45/181 (2201-2250)\n",
      "✅ Lote concluído (24.3% total)\n",
      "\n",
      "📦 Processando lote 45/181 (2201-2250)\n",
      "✅ Lote concluído (24.9% total)\n",
      "\n",
      "📦 Processando lote 46/181 (2251-2300)\n",
      "✅ Lote concluído (24.9% total)\n",
      "\n",
      "📦 Processando lote 46/181 (2251-2300)\n",
      "✅ Lote concluído (25.4% total)\n",
      "\n",
      "📦 Processando lote 47/181 (2301-2350)\n",
      "✅ Lote concluído (25.4% total)\n",
      "\n",
      "📦 Processando lote 47/181 (2301-2350)\n",
      "✅ Lote concluído (26.0% total)\n",
      "\n",
      "📦 Processando lote 48/181 (2351-2400)\n",
      "✅ Lote concluído (26.0% total)\n",
      "\n",
      "📦 Processando lote 48/181 (2351-2400)\n",
      "✅ Lote concluído (26.5% total)\n",
      "\n",
      "📦 Processando lote 49/181 (2401-2450)\n",
      "✅ Lote concluído (26.5% total)\n",
      "\n",
      "📦 Processando lote 49/181 (2401-2450)\n",
      "✅ Lote concluído (27.1% total)\n",
      "\n",
      "📦 Processando lote 50/181 (2451-2500)\n",
      "✅ Lote concluído (27.1% total)\n",
      "\n",
      "📦 Processando lote 50/181 (2451-2500)\n",
      "✅ Lote concluído (27.6% total)\n",
      "\n",
      "📦 Processando lote 51/181 (2501-2550)\n",
      "✅ Lote concluído (27.6% total)\n",
      "\n",
      "📦 Processando lote 51/181 (2501-2550)\n",
      "✅ Lote concluído (28.2% total)\n",
      "\n",
      "📦 Processando lote 52/181 (2551-2600)\n",
      "✅ Lote concluído (28.2% total)\n",
      "\n",
      "📦 Processando lote 52/181 (2551-2600)\n",
      "✅ Lote concluído (28.7% total)\n",
      "\n",
      "📦 Processando lote 53/181 (2601-2650)\n",
      "✅ Lote concluído (28.7% total)\n",
      "\n",
      "📦 Processando lote 53/181 (2601-2650)\n",
      "✅ Lote concluído (29.3% total)\n",
      "\n",
      "📦 Processando lote 54/181 (2651-2700)\n",
      "✅ Lote concluído (29.3% total)\n",
      "\n",
      "📦 Processando lote 54/181 (2651-2700)\n",
      "✅ Lote concluído (29.8% total)\n",
      "\n",
      "📦 Processando lote 55/181 (2701-2750)\n",
      "✅ Lote concluído (29.8% total)\n",
      "\n",
      "📦 Processando lote 55/181 (2701-2750)\n",
      "✅ Lote concluído (30.4% total)\n",
      "\n",
      "📦 Processando lote 56/181 (2751-2800)\n",
      "✅ Lote concluído (30.4% total)\n",
      "\n",
      "📦 Processando lote 56/181 (2751-2800)\n",
      "✅ Lote concluído (30.9% total)\n",
      "\n",
      "📦 Processando lote 57/181 (2801-2850)\n",
      "✅ Lote concluído (30.9% total)\n",
      "\n",
      "📦 Processando lote 57/181 (2801-2850)\n",
      "✅ Lote concluído (31.5% total)\n",
      "\n",
      "📦 Processando lote 58/181 (2851-2900)\n",
      "✅ Lote concluído (31.5% total)\n",
      "\n",
      "📦 Processando lote 58/181 (2851-2900)\n",
      "✅ Lote concluído (32.0% total)\n",
      "\n",
      "📦 Processando lote 59/181 (2901-2950)\n",
      "✅ Lote concluído (32.0% total)\n",
      "\n",
      "📦 Processando lote 59/181 (2901-2950)\n",
      "✅ Lote concluído (32.6% total)\n",
      "\n",
      "📦 Processando lote 60/181 (2951-3000)\n",
      "✅ Lote concluído (32.6% total)\n",
      "\n",
      "📦 Processando lote 60/181 (2951-3000)\n",
      "✅ Lote concluído (33.1% total)\n",
      "\n",
      "📦 Processando lote 61/181 (3001-3050)\n",
      "✅ Lote concluído (33.1% total)\n",
      "\n",
      "📦 Processando lote 61/181 (3001-3050)\n",
      "✅ Lote concluído (33.7% total)\n",
      "\n",
      "📦 Processando lote 62/181 (3051-3100)\n",
      "✅ Lote concluído (33.7% total)\n",
      "\n",
      "📦 Processando lote 62/181 (3051-3100)\n",
      "✅ Lote concluído (34.3% total)\n",
      "\n",
      "📦 Processando lote 63/181 (3101-3150)\n",
      "✅ Lote concluído (34.3% total)\n",
      "\n",
      "📦 Processando lote 63/181 (3101-3150)\n",
      "✅ Lote concluído (34.8% total)\n",
      "\n",
      "📦 Processando lote 64/181 (3151-3200)\n",
      "✅ Lote concluído (34.8% total)\n",
      "\n",
      "📦 Processando lote 64/181 (3151-3200)\n",
      "✅ Lote concluído (35.4% total)\n",
      "\n",
      "📦 Processando lote 65/181 (3201-3250)\n",
      "✅ Lote concluído (35.4% total)\n",
      "\n",
      "📦 Processando lote 65/181 (3201-3250)\n",
      "✅ Lote concluído (35.9% total)\n",
      "\n",
      "📦 Processando lote 66/181 (3251-3300)\n",
      "✅ Lote concluído (35.9% total)\n",
      "\n",
      "📦 Processando lote 66/181 (3251-3300)\n",
      "✅ Lote concluído (36.5% total)\n",
      "\n",
      "📦 Processando lote 67/181 (3301-3350)\n",
      "✅ Lote concluído (36.5% total)\n",
      "\n",
      "📦 Processando lote 67/181 (3301-3350)\n",
      "✅ Lote concluído (37.0% total)\n",
      "\n",
      "📦 Processando lote 68/181 (3351-3400)\n",
      "✅ Lote concluído (37.6% total)\n",
      "\n",
      "📦 Processando lote 69/181 (3401-3450)\n",
      "✅ Lote concluído (37.0% total)\n",
      "\n",
      "📦 Processando lote 68/181 (3351-3400)\n",
      "✅ Lote concluído (37.6% total)\n",
      "\n",
      "📦 Processando lote 69/181 (3401-3450)\n",
      "✅ Lote concluído (38.1% total)\n",
      "\n",
      "📦 Processando lote 70/181 (3451-3500)\n",
      "✅ Lote concluído (38.1% total)\n",
      "\n",
      "📦 Processando lote 70/181 (3451-3500)\n",
      "✅ Lote concluído (38.7% total)\n",
      "\n",
      "📦 Processando lote 71/181 (3501-3550)\n",
      "✅ Lote concluído (38.7% total)\n",
      "\n",
      "📦 Processando lote 71/181 (3501-3550)\n",
      "✅ Lote concluído (39.2% total)\n",
      "\n",
      "📦 Processando lote 72/181 (3551-3600)\n",
      "✅ Lote concluído (39.2% total)\n",
      "\n",
      "📦 Processando lote 72/181 (3551-3600)\n",
      "✅ Lote concluído (39.8% total)\n",
      "\n",
      "📦 Processando lote 73/181 (3601-3650)\n",
      "✅ Lote concluído (39.8% total)\n",
      "\n",
      "📦 Processando lote 73/181 (3601-3650)\n",
      "✅ Lote concluído (40.3% total)\n",
      "\n",
      "📦 Processando lote 74/181 (3651-3700)\n",
      "✅ Lote concluído (40.3% total)\n",
      "\n",
      "📦 Processando lote 74/181 (3651-3700)\n",
      "✅ Lote concluído (40.9% total)\n",
      "\n",
      "📦 Processando lote 75/181 (3701-3750)\n",
      "✅ Lote concluído (41.4% total)\n",
      "\n",
      "📦 Processando lote 76/181 (3751-3800)\n",
      "✅ Lote concluído (40.9% total)\n",
      "\n",
      "📦 Processando lote 75/181 (3701-3750)\n",
      "✅ Lote concluído (41.4% total)\n",
      "\n",
      "📦 Processando lote 76/181 (3751-3800)\n",
      "✅ Lote concluído (42.0% total)\n",
      "\n",
      "📦 Processando lote 77/181 (3801-3850)\n",
      "✅ Lote concluído (42.0% total)\n",
      "\n",
      "📦 Processando lote 77/181 (3801-3850)\n",
      "✅ Lote concluído (42.5% total)\n",
      "\n",
      "📦 Processando lote 78/181 (3851-3900)\n",
      "✅ Lote concluído (43.1% total)\n",
      "\n",
      "📦 Processando lote 79/181 (3901-3950)\n",
      "✅ Lote concluído (42.5% total)\n",
      "\n",
      "📦 Processando lote 78/181 (3851-3900)\n",
      "✅ Lote concluído (43.1% total)\n",
      "\n",
      "📦 Processando lote 79/181 (3901-3950)\n",
      "✅ Lote concluído (43.6% total)\n",
      "\n",
      "📦 Processando lote 80/181 (3951-4000)\n",
      "✅ Lote concluído (43.6% total)\n",
      "\n",
      "📦 Processando lote 80/181 (3951-4000)\n",
      "✅ Lote concluído (44.2% total)\n",
      "\n",
      "📦 Processando lote 81/181 (4001-4050)\n",
      "✅ Lote concluído (44.8% total)\n",
      "\n",
      "📦 Processando lote 82/181 (4051-4100)\n",
      "✅ Lote concluído (44.2% total)\n",
      "\n",
      "📦 Processando lote 81/181 (4001-4050)\n",
      "✅ Lote concluído (44.8% total)\n",
      "\n",
      "📦 Processando lote 82/181 (4051-4100)\n",
      "✅ Lote concluído (45.3% total)\n",
      "\n",
      "📦 Processando lote 83/181 (4101-4150)\n",
      "✅ Lote concluído (45.3% total)\n",
      "\n",
      "📦 Processando lote 83/181 (4101-4150)\n",
      "✅ Lote concluído (45.9% total)\n",
      "\n",
      "📦 Processando lote 84/181 (4151-4200)\n",
      "✅ Lote concluído (45.9% total)\n",
      "\n",
      "📦 Processando lote 84/181 (4151-4200)\n",
      "✅ Lote concluído (46.4% total)\n",
      "\n",
      "📦 Processando lote 85/181 (4201-4250)\n",
      "✅ Lote concluído (46.4% total)\n",
      "\n",
      "📦 Processando lote 85/181 (4201-4250)\n",
      "✅ Lote concluído (47.0% total)\n",
      "\n",
      "📦 Processando lote 86/181 (4251-4300)\n",
      "✅ Lote concluído (47.0% total)\n",
      "\n",
      "📦 Processando lote 86/181 (4251-4300)\n",
      "✅ Lote concluído (47.5% total)\n",
      "\n",
      "📦 Processando lote 87/181 (4301-4350)\n",
      "✅ Lote concluído (48.1% total)\n",
      "\n",
      "📦 Processando lote 88/181 (4351-4400)\n",
      "✅ Lote concluído (47.5% total)\n",
      "\n",
      "📦 Processando lote 87/181 (4301-4350)\n",
      "✅ Lote concluído (48.1% total)\n",
      "\n",
      "📦 Processando lote 88/181 (4351-4400)\n",
      "✅ Lote concluído (48.6% total)\n",
      "\n",
      "📦 Processando lote 89/181 (4401-4450)\n",
      "✅ Lote concluído (48.6% total)\n",
      "\n",
      "📦 Processando lote 89/181 (4401-4450)\n",
      "✅ Lote concluído (49.2% total)\n",
      "\n",
      "📦 Processando lote 90/181 (4451-4500)\n",
      "✅ Lote concluído (49.2% total)\n",
      "\n",
      "📦 Processando lote 90/181 (4451-4500)\n",
      "✅ Lote concluído (49.7% total)\n",
      "\n",
      "📦 Processando lote 91/181 (4501-4550)\n",
      "✅ Lote concluído (49.7% total)\n",
      "\n",
      "📦 Processando lote 91/181 (4501-4550)\n",
      "✅ Lote concluído (50.3% total)\n",
      "\n",
      "📦 Processando lote 92/181 (4551-4600)\n",
      "✅ Lote concluído (50.3% total)\n",
      "\n",
      "📦 Processando lote 92/181 (4551-4600)\n",
      "✅ Lote concluído (50.8% total)\n",
      "\n",
      "📦 Processando lote 93/181 (4601-4650)\n",
      "✅ Lote concluído (50.8% total)\n",
      "\n",
      "📦 Processando lote 93/181 (4601-4650)\n",
      "✅ Lote concluído (51.4% total)\n",
      "\n",
      "📦 Processando lote 94/181 (4651-4700)\n",
      "✅ Lote concluído (51.4% total)\n",
      "\n",
      "📦 Processando lote 94/181 (4651-4700)\n",
      "✅ Lote concluído (51.9% total)\n",
      "\n",
      "📦 Processando lote 95/181 (4701-4750)\n",
      "✅ Lote concluído (51.9% total)\n",
      "\n",
      "📦 Processando lote 95/181 (4701-4750)\n",
      "✅ Lote concluído (52.5% total)\n",
      "\n",
      "📦 Processando lote 96/181 (4751-4800)\n",
      "✅ Lote concluído (52.5% total)\n",
      "\n",
      "📦 Processando lote 96/181 (4751-4800)\n",
      "✅ Lote concluído (53.0% total)\n",
      "\n",
      "📦 Processando lote 97/181 (4801-4850)\n",
      "✅ Lote concluído (53.0% total)\n",
      "\n",
      "📦 Processando lote 97/181 (4801-4850)\n",
      "✅ Lote concluído (53.6% total)\n",
      "\n",
      "📦 Processando lote 98/181 (4851-4900)\n",
      "✅ Lote concluído (53.6% total)\n",
      "\n",
      "📦 Processando lote 98/181 (4851-4900)\n",
      "✅ Lote concluído (54.1% total)\n",
      "\n",
      "📦 Processando lote 99/181 (4901-4950)\n",
      "✅ Lote concluído (54.7% total)\n",
      "\n",
      "📦 Processando lote 100/181 (4951-5000)\n",
      "✅ Lote concluído (54.1% total)\n",
      "\n",
      "📦 Processando lote 99/181 (4901-4950)\n",
      "✅ Lote concluído (54.7% total)\n",
      "\n",
      "📦 Processando lote 100/181 (4951-5000)\n",
      "✅ Lote concluído (55.2% total)\n",
      "\n",
      "📦 Processando lote 101/181 (5001-5050)\n",
      "✅ Lote concluído (55.2% total)\n",
      "\n",
      "📦 Processando lote 101/181 (5001-5050)\n",
      "✅ Lote concluído (55.8% total)\n",
      "\n",
      "📦 Processando lote 102/181 (5051-5100)\n",
      "✅ Lote concluído (55.8% total)\n",
      "\n",
      "📦 Processando lote 102/181 (5051-5100)\n",
      "✅ Lote concluído (56.4% total)\n",
      "\n",
      "📦 Processando lote 103/181 (5101-5150)\n",
      "✅ Lote concluído (56.4% total)\n",
      "\n",
      "📦 Processando lote 103/181 (5101-5150)\n",
      "✅ Lote concluído (56.9% total)\n",
      "\n",
      "📦 Processando lote 104/181 (5151-5200)\n",
      "✅ Lote concluído (56.9% total)\n",
      "\n",
      "📦 Processando lote 104/181 (5151-5200)\n",
      "✅ Lote concluído (57.5% total)\n",
      "\n",
      "📦 Processando lote 105/181 (5201-5250)\n",
      "✅ Lote concluído (57.5% total)\n",
      "\n",
      "📦 Processando lote 105/181 (5201-5250)\n",
      "✅ Lote concluído (58.0% total)\n",
      "\n",
      "📦 Processando lote 106/181 (5251-5300)\n",
      "✅ Lote concluído (58.0% total)\n",
      "\n",
      "📦 Processando lote 106/181 (5251-5300)\n",
      "✅ Lote concluído (58.6% total)\n",
      "\n",
      "📦 Processando lote 107/181 (5301-5350)\n",
      "✅ Lote concluído (58.6% total)\n",
      "\n",
      "📦 Processando lote 107/181 (5301-5350)\n",
      "✅ Lote concluído (59.1% total)\n",
      "\n",
      "📦 Processando lote 108/181 (5351-5400)\n",
      "✅ Lote concluído (59.7% total)\n",
      "\n",
      "📦 Processando lote 109/181 (5401-5450)\n",
      "✅ Lote concluído (59.1% total)\n",
      "\n",
      "📦 Processando lote 108/181 (5351-5400)\n",
      "✅ Lote concluído (59.7% total)\n",
      "\n",
      "📦 Processando lote 109/181 (5401-5450)\n",
      "✅ Lote concluído (60.2% total)\n",
      "\n",
      "📦 Processando lote 110/181 (5451-5500)\n",
      "✅ Lote concluído (60.8% total)\n",
      "\n",
      "📦 Processando lote 111/181 (5501-5550)\n",
      "✅ Lote concluído (60.2% total)\n",
      "\n",
      "📦 Processando lote 110/181 (5451-5500)\n",
      "✅ Lote concluído (60.8% total)\n",
      "\n",
      "📦 Processando lote 111/181 (5501-5550)\n",
      "✅ Lote concluído (61.3% total)\n",
      "\n",
      "📦 Processando lote 112/181 (5551-5600)\n",
      "✅ Lote concluído (61.3% total)\n",
      "\n",
      "📦 Processando lote 112/181 (5551-5600)\n",
      "✅ Lote concluído (61.9% total)\n",
      "\n",
      "📦 Processando lote 113/181 (5601-5650)\n",
      "✅ Lote concluído (62.4% total)\n",
      "\n",
      "📦 Processando lote 114/181 (5651-5700)\n",
      "✅ Lote concluído (61.9% total)\n",
      "\n",
      "📦 Processando lote 113/181 (5601-5650)\n",
      "✅ Lote concluído (62.4% total)\n",
      "\n",
      "📦 Processando lote 114/181 (5651-5700)\n",
      "✅ Lote concluído (63.0% total)\n",
      "\n",
      "📦 Processando lote 115/181 (5701-5750)\n",
      "✅ Lote concluído (63.0% total)\n",
      "\n",
      "📦 Processando lote 115/181 (5701-5750)\n",
      "✅ Lote concluído (63.5% total)\n",
      "\n",
      "📦 Processando lote 116/181 (5751-5800)\n",
      "✅ Lote concluído (63.5% total)\n",
      "\n",
      "📦 Processando lote 116/181 (5751-5800)\n",
      "✅ Lote concluído (64.1% total)\n",
      "\n",
      "📦 Processando lote 117/181 (5801-5850)\n",
      "✅ Lote concluído (64.1% total)\n",
      "\n",
      "📦 Processando lote 117/181 (5801-5850)\n",
      "✅ Lote concluído (64.6% total)\n",
      "\n",
      "📦 Processando lote 118/181 (5851-5900)\n",
      "✅ Lote concluído (65.2% total)\n",
      "\n",
      "📦 Processando lote 119/181 (5901-5950)\n",
      "✅ Lote concluído (64.6% total)\n",
      "\n",
      "📦 Processando lote 118/181 (5851-5900)\n",
      "✅ Lote concluído (65.2% total)\n",
      "\n",
      "📦 Processando lote 119/181 (5901-5950)\n",
      "✅ Lote concluído (65.7% total)\n",
      "\n",
      "📦 Processando lote 120/181 (5951-6000)\n",
      "✅ Lote concluído (65.7% total)\n",
      "\n",
      "📦 Processando lote 120/181 (5951-6000)\n",
      "✅ Lote concluído (66.3% total)\n",
      "\n",
      "📦 Processando lote 121/181 (6001-6050)\n",
      "✅ Lote concluído (66.3% total)\n",
      "\n",
      "📦 Processando lote 121/181 (6001-6050)\n",
      "✅ Lote concluído (66.9% total)\n",
      "\n",
      "📦 Processando lote 122/181 (6051-6100)\n",
      "✅ Lote concluído (66.9% total)\n",
      "\n",
      "📦 Processando lote 122/181 (6051-6100)\n",
      "✅ Lote concluído (67.4% total)\n",
      "\n",
      "📦 Processando lote 123/181 (6101-6150)\n",
      "✅ Lote concluído (67.4% total)\n",
      "\n",
      "📦 Processando lote 123/181 (6101-6150)\n",
      "✅ Lote concluído (68.0% total)\n",
      "\n",
      "📦 Processando lote 124/181 (6151-6200)\n",
      "✅ Lote concluído (68.0% total)\n",
      "\n",
      "📦 Processando lote 124/181 (6151-6200)\n",
      "✅ Lote concluído (68.5% total)\n",
      "\n",
      "📦 Processando lote 125/181 (6201-6250)\n",
      "✅ Lote concluído (68.5% total)\n",
      "\n",
      "📦 Processando lote 125/181 (6201-6250)\n",
      "✅ Lote concluído (69.1% total)\n",
      "\n",
      "📦 Processando lote 126/181 (6251-6300)\n",
      "✅ Lote concluído (69.1% total)\n",
      "\n",
      "📦 Processando lote 126/181 (6251-6300)\n",
      "✅ Lote concluído (69.6% total)\n",
      "\n",
      "📦 Processando lote 127/181 (6301-6350)\n",
      "✅ Lote concluído (69.6% total)\n",
      "\n",
      "📦 Processando lote 127/181 (6301-6350)\n",
      "✅ Lote concluído (70.2% total)\n",
      "\n",
      "📦 Processando lote 128/181 (6351-6400)\n",
      "✅ Lote concluído (70.2% total)\n",
      "\n",
      "📦 Processando lote 128/181 (6351-6400)\n",
      "✅ Lote concluído (70.7% total)\n",
      "\n",
      "📦 Processando lote 129/181 (6401-6450)\n",
      "✅ Lote concluído (70.7% total)\n",
      "\n",
      "📦 Processando lote 129/181 (6401-6450)\n",
      "✅ Lote concluído (71.3% total)\n",
      "\n",
      "📦 Processando lote 130/181 (6451-6500)\n",
      "✅ Lote concluído (71.3% total)\n",
      "\n",
      "📦 Processando lote 130/181 (6451-6500)\n",
      "✅ Lote concluído (71.8% total)\n",
      "\n",
      "📦 Processando lote 131/181 (6501-6550)\n",
      "✅ Lote concluído (71.8% total)\n",
      "\n",
      "📦 Processando lote 131/181 (6501-6550)\n",
      "✅ Lote concluído (72.4% total)\n",
      "\n",
      "📦 Processando lote 132/181 (6551-6600)\n",
      "✅ Lote concluído (72.4% total)\n",
      "\n",
      "📦 Processando lote 132/181 (6551-6600)\n",
      "✅ Lote concluído (72.9% total)\n",
      "\n",
      "📦 Processando lote 133/181 (6601-6650)\n",
      "✅ Lote concluído (73.5% total)\n",
      "\n",
      "📦 Processando lote 134/181 (6651-6700)\n",
      "✅ Lote concluído (72.9% total)\n",
      "\n",
      "📦 Processando lote 133/181 (6601-6650)\n",
      "✅ Lote concluído (73.5% total)\n",
      "\n",
      "📦 Processando lote 134/181 (6651-6700)\n",
      "✅ Lote concluído (74.0% total)\n",
      "\n",
      "📦 Processando lote 135/181 (6701-6750)\n",
      "✅ Lote concluído (74.0% total)\n",
      "\n",
      "📦 Processando lote 135/181 (6701-6750)\n",
      "✅ Lote concluído (74.6% total)\n",
      "\n",
      "📦 Processando lote 136/181 (6751-6800)\n",
      "✅ Lote concluído (74.6% total)\n",
      "\n",
      "📦 Processando lote 136/181 (6751-6800)\n",
      "✅ Lote concluído (75.1% total)\n",
      "\n",
      "📦 Processando lote 137/181 (6801-6850)\n",
      "✅ Lote concluído (75.1% total)\n",
      "\n",
      "📦 Processando lote 137/181 (6801-6850)\n",
      "✅ Lote concluído (75.7% total)\n",
      "\n",
      "📦 Processando lote 138/181 (6851-6900)\n",
      "✅ Lote concluído (75.7% total)\n",
      "\n",
      "📦 Processando lote 138/181 (6851-6900)\n",
      "✅ Lote concluído (76.2% total)\n",
      "\n",
      "📦 Processando lote 139/181 (6901-6950)\n",
      "✅ Lote concluído (76.2% total)\n",
      "\n",
      "📦 Processando lote 139/181 (6901-6950)\n",
      "✅ Lote concluído (76.8% total)\n",
      "\n",
      "📦 Processando lote 140/181 (6951-7000)\n",
      "✅ Lote concluído (76.8% total)\n",
      "\n",
      "📦 Processando lote 140/181 (6951-7000)\n",
      "✅ Lote concluído (77.3% total)\n",
      "\n",
      "📦 Processando lote 141/181 (7001-7050)\n",
      "✅ Lote concluído (77.3% total)\n",
      "\n",
      "📦 Processando lote 141/181 (7001-7050)\n",
      "✅ Lote concluído (77.9% total)\n",
      "\n",
      "📦 Processando lote 142/181 (7051-7100)\n",
      "✅ Lote concluído (77.9% total)\n",
      "\n",
      "📦 Processando lote 142/181 (7051-7100)\n",
      "✅ Lote concluído (78.5% total)\n",
      "\n",
      "📦 Processando lote 143/181 (7101-7150)\n",
      "✅ Lote concluído (78.5% total)\n",
      "\n",
      "📦 Processando lote 143/181 (7101-7150)\n",
      "✅ Lote concluído (79.0% total)\n",
      "\n",
      "📦 Processando lote 144/181 (7151-7200)\n",
      "✅ Lote concluído (79.0% total)\n",
      "\n",
      "📦 Processando lote 144/181 (7151-7200)\n",
      "✅ Lote concluído (79.6% total)\n",
      "\n",
      "📦 Processando lote 145/181 (7201-7250)\n",
      "✅ Lote concluído (79.6% total)\n",
      "\n",
      "📦 Processando lote 145/181 (7201-7250)\n",
      "✅ Lote concluído (80.1% total)\n",
      "\n",
      "📦 Processando lote 146/181 (7251-7300)\n",
      "✅ Lote concluído (80.1% total)\n",
      "\n",
      "📦 Processando lote 146/181 (7251-7300)\n",
      "✅ Lote concluído (80.7% total)\n",
      "\n",
      "📦 Processando lote 147/181 (7301-7350)\n",
      "✅ Lote concluído (81.2% total)\n",
      "\n",
      "📦 Processando lote 148/181 (7351-7400)\n",
      "✅ Lote concluído (80.7% total)\n",
      "\n",
      "📦 Processando lote 147/181 (7301-7350)\n",
      "✅ Lote concluído (81.2% total)\n",
      "\n",
      "📦 Processando lote 148/181 (7351-7400)\n",
      "✅ Lote concluído (81.8% total)\n",
      "\n",
      "📦 Processando lote 149/181 (7401-7450)\n",
      "✅ Lote concluído (81.8% total)\n",
      "\n",
      "📦 Processando lote 149/181 (7401-7450)\n",
      "✅ Lote concluído (82.3% total)\n",
      "\n",
      "📦 Processando lote 150/181 (7451-7500)\n",
      "✅ Lote concluído (82.9% total)\n",
      "\n",
      "📦 Processando lote 151/181 (7501-7550)\n",
      "✅ Lote concluído (82.3% total)\n",
      "\n",
      "📦 Processando lote 150/181 (7451-7500)\n",
      "✅ Lote concluído (82.9% total)\n",
      "\n",
      "📦 Processando lote 151/181 (7501-7550)\n",
      "✅ Lote concluído (83.4% total)\n",
      "\n",
      "📦 Processando lote 152/181 (7551-7600)\n",
      "✅ Lote concluído (83.4% total)\n",
      "\n",
      "📦 Processando lote 152/181 (7551-7600)\n",
      "✅ Lote concluído (84.0% total)\n",
      "\n",
      "📦 Processando lote 153/181 (7601-7650)\n",
      "✅ Lote concluído (84.0% total)\n",
      "\n",
      "📦 Processando lote 153/181 (7601-7650)\n",
      "✅ Lote concluído (84.5% total)\n",
      "\n",
      "📦 Processando lote 154/181 (7651-7700)\n",
      "✅ Lote concluído (84.5% total)\n",
      "\n",
      "📦 Processando lote 154/181 (7651-7700)\n",
      "✅ Lote concluído (85.1% total)\n",
      "\n",
      "📦 Processando lote 155/181 (7701-7750)\n",
      "✅ Lote concluído (85.6% total)\n",
      "\n",
      "📦 Processando lote 156/181 (7751-7800)\n",
      "✅ Lote concluído (85.1% total)\n",
      "\n",
      "📦 Processando lote 155/181 (7701-7750)\n",
      "✅ Lote concluído (85.6% total)\n",
      "\n",
      "📦 Processando lote 156/181 (7751-7800)\n",
      "✅ Lote concluído (86.2% total)\n",
      "\n",
      "📦 Processando lote 157/181 (7801-7850)\n",
      "✅ Lote concluído (86.7% total)\n",
      "\n",
      "📦 Processando lote 158/181 (7851-7900)\n",
      "✅ Lote concluído (86.2% total)\n",
      "\n",
      "📦 Processando lote 157/181 (7801-7850)\n",
      "✅ Lote concluído (86.7% total)\n",
      "\n",
      "📦 Processando lote 158/181 (7851-7900)\n",
      "✅ Lote concluído (87.3% total)\n",
      "\n",
      "📦 Processando lote 159/181 (7901-7950)\n",
      "✅ Lote concluído (87.3% total)\n",
      "\n",
      "📦 Processando lote 159/181 (7901-7950)\n",
      "✅ Lote concluído (87.8% total)\n",
      "\n",
      "📦 Processando lote 160/181 (7951-8000)\n",
      "✅ Lote concluído (87.8% total)\n",
      "\n",
      "📦 Processando lote 160/181 (7951-8000)\n",
      "✅ Lote concluído (88.4% total)\n",
      "\n",
      "📦 Processando lote 161/181 (8001-8050)\n",
      "✅ Lote concluído (88.4% total)\n",
      "\n",
      "📦 Processando lote 161/181 (8001-8050)\n",
      "✅ Lote concluído (89.0% total)\n",
      "\n",
      "📦 Processando lote 162/181 (8051-8100)\n",
      "✅ Lote concluído (89.0% total)\n",
      "\n",
      "📦 Processando lote 162/181 (8051-8100)\n",
      "✅ Lote concluído (89.5% total)\n",
      "\n",
      "📦 Processando lote 163/181 (8101-8150)\n",
      "✅ Lote concluído (89.5% total)\n",
      "\n",
      "📦 Processando lote 163/181 (8101-8150)\n",
      "✅ Lote concluído (90.1% total)\n",
      "\n",
      "📦 Processando lote 164/181 (8151-8200)\n",
      "✅ Lote concluído (90.1% total)\n",
      "\n",
      "📦 Processando lote 164/181 (8151-8200)\n",
      "✅ Lote concluído (90.6% total)\n",
      "\n",
      "📦 Processando lote 165/181 (8201-8250)\n",
      "✅ Lote concluído (90.6% total)\n",
      "\n",
      "📦 Processando lote 165/181 (8201-8250)\n",
      "✅ Lote concluído (91.2% total)\n",
      "\n",
      "📦 Processando lote 166/181 (8251-8300)\n",
      "✅ Lote concluído (91.2% total)\n",
      "\n",
      "📦 Processando lote 166/181 (8251-8300)\n",
      "✅ Lote concluído (91.7% total)\n",
      "\n",
      "📦 Processando lote 167/181 (8301-8350)\n",
      "✅ Lote concluído (91.7% total)\n",
      "\n",
      "📦 Processando lote 167/181 (8301-8350)\n",
      "✅ Lote concluído (92.3% total)\n",
      "\n",
      "📦 Processando lote 168/181 (8351-8400)\n",
      "✅ Lote concluído (92.3% total)\n",
      "\n",
      "📦 Processando lote 168/181 (8351-8400)\n",
      "✅ Lote concluído (92.8% total)\n",
      "\n",
      "📦 Processando lote 169/181 (8401-8450)\n",
      "✅ Lote concluído (92.8% total)\n",
      "\n",
      "📦 Processando lote 169/181 (8401-8450)\n",
      "✅ Lote concluído (93.4% total)\n",
      "\n",
      "📦 Processando lote 170/181 (8451-8500)\n",
      "✅ Lote concluído (93.4% total)\n",
      "\n",
      "📦 Processando lote 170/181 (8451-8500)\n",
      "✅ Lote concluído (93.9% total)\n",
      "\n",
      "📦 Processando lote 171/181 (8501-8550)\n",
      "✅ Lote concluído (93.9% total)\n",
      "\n",
      "📦 Processando lote 171/181 (8501-8550)\n",
      "✅ Lote concluído (94.5% total)\n",
      "\n",
      "📦 Processando lote 172/181 (8551-8600)\n",
      "✅ Lote concluído (94.5% total)\n",
      "\n",
      "📦 Processando lote 172/181 (8551-8600)\n",
      "✅ Lote concluído (95.0% total)\n",
      "\n",
      "📦 Processando lote 173/181 (8601-8650)\n",
      "✅ Lote concluído (95.0% total)\n",
      "\n",
      "📦 Processando lote 173/181 (8601-8650)\n",
      "✅ Lote concluído (95.6% total)\n",
      "\n",
      "📦 Processando lote 174/181 (8651-8700)\n",
      "✅ Lote concluído (95.6% total)\n",
      "\n",
      "📦 Processando lote 174/181 (8651-8700)\n",
      "✅ Lote concluído (96.1% total)\n",
      "\n",
      "📦 Processando lote 175/181 (8701-8750)\n",
      "✅ Lote concluído (96.1% total)\n",
      "\n",
      "📦 Processando lote 175/181 (8701-8750)\n",
      "✅ Lote concluído (96.7% total)\n",
      "\n",
      "📦 Processando lote 176/181 (8751-8800)\n",
      "✅ Lote concluído (96.7% total)\n",
      "\n",
      "📦 Processando lote 176/181 (8751-8800)\n",
      "✅ Lote concluído (97.2% total)\n",
      "\n",
      "📦 Processando lote 177/181 (8801-8850)\n",
      "✅ Lote concluído (97.2% total)\n",
      "\n",
      "📦 Processando lote 177/181 (8801-8850)\n",
      "✅ Lote concluído (97.8% total)\n",
      "\n",
      "📦 Processando lote 178/181 (8851-8900)\n",
      "✅ Lote concluído (97.8% total)\n",
      "\n",
      "📦 Processando lote 178/181 (8851-8900)\n",
      "✅ Lote concluído (98.3% total)\n",
      "\n",
      "📦 Processando lote 179/181 (8901-8950)\n",
      "✅ Lote concluído (98.9% total)\n",
      "\n",
      "📦 Processando lote 180/181 (8951-9000)\n",
      "✅ Lote concluído (98.3% total)\n",
      "\n",
      "📦 Processando lote 179/181 (8901-8950)\n",
      "✅ Lote concluído (98.9% total)\n",
      "\n",
      "📦 Processando lote 180/181 (8951-9000)\n",
      "✅ Lote concluído (99.4% total)\n",
      "\n",
      "📦 Processando lote 181/181 (9001-9046)\n",
      "✅ Lote concluído (100.0% total)\n",
      "\n",
      "🎉 ANÁLISE REGEX CONCLUÍDA\n",
      "📊 Registros processados: 9,046\n",
      "✅ Análise regex concluída\n",
      "✅ Lote concluído (99.4% total)\n",
      "\n",
      "📦 Processando lote 181/181 (9001-9046)\n",
      "✅ Lote concluído (100.0% total)\n",
      "\n",
      "🎉 ANÁLISE REGEX CONCLUÍDA\n",
      "📊 Registros processados: 9,046\n",
      "✅ Análise regex concluída\n"
     ]
    }
   ],
   "source": [
    "# 🔄 Análise em lote dos dados\n",
    "\n",
    "def analyze_dataset_regex(df: pd.DataFrame, analyzer: RegexAnalyzer, \n",
    "                         batch_size: int = 100) -> pd.DataFrame:\n",
    "    \"\"\"Analisa todo o dataset usando regex em lotes para melhor performance\"\"\"\n",
    "    \n",
    "    print(\"🔄 INICIANDO ANÁLISE REGEX EM LOTE\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    total_records = len(df)\n",
    "    print(f\"📊 Total de registros: {total_records:,}\")\n",
    "    print(f\"📦 Tamanho do lote: {batch_size}\")\n",
    "    \n",
    "    # Preparar DataFrame resultado\n",
    "    df_result = df.copy()\n",
    "    \n",
    "    # Colunas para armazenar resultados\n",
    "    resultado_colunas = [\n",
    "        'nano_relevance_score', 'functional_relevance_score', 'total_matches',\n",
    "        'nanomateriais_encontrados', 'propriedades_funcionais', 'metodos_sintese',\n",
    "        'medidas_encontradas', 'analise_completa'\n",
    "    ]\n",
    "    \n",
    "    for col in resultado_colunas:\n",
    "        df_result[col] = None\n",
    "    \n",
    "    # Processar em lotes\n",
    "    total_batches = (total_records + batch_size - 1) // batch_size\n",
    "    \n",
    "    for batch_idx in range(total_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min(start_idx + batch_size, total_records)\n",
    "        \n",
    "        print(f\"\\n📦 Processando lote {batch_idx + 1}/{total_batches} ({start_idx+1}-{end_idx})\")\n",
    "        \n",
    "        # Processar registros do lote\n",
    "        for idx in range(start_idx, end_idx):\n",
    "            # Combinar título e abstract para análise\n",
    "            titulo = str(df_result.iloc[idx].get('Article Title', ''))\n",
    "            abstract = str(df_result.iloc[idx].get('Abstract', ''))\n",
    "            texto_completo = f\"{titulo} {abstract}\".strip()\n",
    "            \n",
    "            if not texto_completo or texto_completo == 'nan nan':\n",
    "                continue\n",
    "            \n",
    "            # Analisar texto\n",
    "            analise = analyzer.analyze_text_comprehensive(texto_completo)\n",
    "            \n",
    "            # Extrair informações específicas\n",
    "            patterns_found = analise.get('patterns_found', {})\n",
    "            \n",
    "            # Coletar nanomateriais\n",
    "            nanomateriais = []\n",
    "            for pattern_name, matches in patterns_found.items():\n",
    "                if pattern_name.startswith('nano_'):\n",
    "                    nanomateriais.extend(matches)\n",
    "            \n",
    "            # Coletar propriedades funcionais\n",
    "            propriedades = []\n",
    "            for pattern_name, matches in patterns_found.items():\n",
    "                if pattern_name.startswith('func_'):\n",
    "                    propriedades.extend(matches)\n",
    "            \n",
    "            # Armazenar resultados\n",
    "            df_result.iloc[idx, df_result.columns.get_loc('nano_relevance_score')] = analise.get('nano_relevance_score', 0)\n",
    "            df_result.iloc[idx, df_result.columns.get_loc('functional_relevance_score')] = analise.get('functional_relevance_score', 0)\n",
    "            df_result.iloc[idx, df_result.columns.get_loc('total_matches')] = analise.get('total_matches', 0)\n",
    "            df_result.iloc[idx, df_result.columns.get_loc('nanomateriais_encontrados')] = json.dumps(list(set(nanomateriais)), ensure_ascii=False)\n",
    "            df_result.iloc[idx, df_result.columns.get_loc('propriedades_funcionais')] = json.dumps(list(set(propriedades)), ensure_ascii=False)\n",
    "            df_result.iloc[idx, df_result.columns.get_loc('metodos_sintese')] = json.dumps(patterns_found.get('sintese', []), ensure_ascii=False)\n",
    "            df_result.iloc[idx, df_result.columns.get_loc('medidas_encontradas')] = json.dumps(patterns_found.get('medidas', []), ensure_ascii=False)\n",
    "            df_result.iloc[idx, df_result.columns.get_loc('analise_completa')] = json.dumps(analise, ensure_ascii=False)\n",
    "        \n",
    "        # Progresso\n",
    "        progress = ((batch_idx + 1) / total_batches) * 100\n",
    "        print(f\"✅ Lote concluído ({progress:.1f}% total)\")\n",
    "    \n",
    "    print(f\"\\n🎉 ANÁLISE REGEX CONCLUÍDA\")\n",
    "    print(f\"📊 Registros processados: {total_records:,}\")\n",
    "    \n",
    "    return df_result\n",
    "\n",
    "# Executar análise em lote usando dados filtrados por Open Access\n",
    "print(\"🚀 Iniciando análise regex do dataset filtrado...\")\n",
    "# Usar df_clean que contém os dados filtrados por Open Access\n",
    "df_analyzed = analyze_dataset_regex(df_clean, regex_analyzer, batch_size=50)\n",
    "print(\"✅ Análise regex concluída\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6e56cc",
   "metadata": {},
   "source": [
    "## 📈 Estatísticas e Análise dos Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1dd3ffc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 GERANDO ESTATÍSTICAS DA ANÁLISE REGEX\n",
      "=============================================\n",
      "📊 Registros com nanomateriais: 4,235 (46.8%)\n",
      "📊 Registros com propriedades funcionais: 8,071 (89.2%)\n",
      "\\n🔍 Analisando frequências...\n",
      "\n",
      "🏆 TOP 10 NANOMATERIAIS:\n",
      "   tio2: 550 menções\n",
      "   al2o3: 446 menções\n",
      "   sio2: 383 menções\n",
      "   silica: 382 menções\n",
      "   graphene: 334 menções\n",
      "   alumina: 314 menções\n",
      "   nanocomposite: 296 menções\n",
      "   zno: 264 menções\n",
      "   silver: 259 menções\n",
      "   zirconia: 256 menções\n",
      "\n",
      "🏆 TOP 10 PROPRIEDADES FUNCIONAIS:\n",
      "   coating: 6,062 menções\n",
      "   coatings: 3,628 menções\n",
      "   thin films: 602 menções\n",
      "   sol-gel: 424 menções\n",
      "   thermal barrier: 311 menções\n",
      "   thin film: 296 menções\n",
      "   biomedical: 274 menções\n",
      "   spin coating: 244 menções\n",
      "   superhydrophobic: 233 menções\n",
      "   magnetic: 232 menções\n",
      "\n",
      "🏆 TOP 10 MÉTODOS DE SÍNTESE:\n",
      "   electrochemical: 862 menções\n",
      "   sol-gel: 424 menções\n",
      "   chemical vapor deposition: 164 menções\n",
      "   hydrothermal: 134 menções\n",
      "   precipitation: 127 menções\n",
      "   pvd: 107 menções\n",
      "   cvd: 105 menções\n",
      "   physical vapor deposition: 80 menções\n",
      "   co-precipitation: 36 menções\n",
      "   ball milling: 31 menções\n",
      "\n",
      "✅ Estatísticas geradas com sucesso\n",
      "\n",
      "🏆 TOP 10 NANOMATERIAIS:\n",
      "   tio2: 550 menções\n",
      "   al2o3: 446 menções\n",
      "   sio2: 383 menções\n",
      "   silica: 382 menções\n",
      "   graphene: 334 menções\n",
      "   alumina: 314 menções\n",
      "   nanocomposite: 296 menções\n",
      "   zno: 264 menções\n",
      "   silver: 259 menções\n",
      "   zirconia: 256 menções\n",
      "\n",
      "🏆 TOP 10 PROPRIEDADES FUNCIONAIS:\n",
      "   coating: 6,062 menções\n",
      "   coatings: 3,628 menções\n",
      "   thin films: 602 menções\n",
      "   sol-gel: 424 menções\n",
      "   thermal barrier: 311 menções\n",
      "   thin film: 296 menções\n",
      "   biomedical: 274 menções\n",
      "   spin coating: 244 menções\n",
      "   superhydrophobic: 233 menções\n",
      "   magnetic: 232 menções\n",
      "\n",
      "🏆 TOP 10 MÉTODOS DE SÍNTESE:\n",
      "   electrochemical: 862 menções\n",
      "   sol-gel: 424 menções\n",
      "   chemical vapor deposition: 164 menções\n",
      "   hydrothermal: 134 menções\n",
      "   precipitation: 127 menções\n",
      "   pvd: 107 menções\n",
      "   cvd: 105 menções\n",
      "   physical vapor deposition: 80 menções\n",
      "   co-precipitation: 36 menções\n",
      "   ball milling: 31 menções\n",
      "\n",
      "✅ Estatísticas geradas com sucesso\n"
     ]
    }
   ],
   "source": [
    "# 📈 Estatísticas e análise dos resultados\n",
    "\n",
    "def generate_regex_statistics(df: pd.DataFrame) -> Dict[str, any]:\n",
    "    \"\"\"Gera estatísticas abrangentes da análise regex\"\"\"\n",
    "    \n",
    "    print(\"📈 GERANDO ESTATÍSTICAS DA ANÁLISE REGEX\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    stats = {\n",
    "        'total_registros': len(df),\n",
    "        'registros_com_nano': 0,\n",
    "        'registros_com_func': 0,\n",
    "        'scores': {},\n",
    "        'nanomateriais_frequencia': Counter(),\n",
    "        'propriedades_frequencia': Counter(),\n",
    "        'metodos_sintese_frequencia': Counter(),\n",
    "        'coocorrencias': defaultdict(int)\n",
    "    }\n",
    "    \n",
    "    # Análise de scores\n",
    "    if 'nano_relevance_score' in df.columns:\n",
    "        nano_scores = pd.to_numeric(df['nano_relevance_score'], errors='coerce').fillna(0)\n",
    "        stats['registros_com_nano'] = (nano_scores > 0).sum()\n",
    "        stats['scores']['nano'] = {\n",
    "            'mean': float(nano_scores.mean()),\n",
    "            'median': float(nano_scores.median()),\n",
    "            'max': float(nano_scores.max()),\n",
    "            'std': float(nano_scores.std())\n",
    "        }\n",
    "    \n",
    "    if 'functional_relevance_score' in df.columns:\n",
    "        func_scores = pd.to_numeric(df['functional_relevance_score'], errors='coerce').fillna(0)\n",
    "        stats['registros_com_func'] = (func_scores > 0).sum()\n",
    "        stats['scores']['functional'] = {\n",
    "            'mean': float(func_scores.mean()),\n",
    "            'median': float(func_scores.median()),\n",
    "            'max': float(func_scores.max()),\n",
    "            'std': float(func_scores.std())\n",
    "        }\n",
    "    \n",
    "    print(f\"📊 Registros com nanomateriais: {stats['registros_com_nano']:,} ({stats['registros_com_nano']/len(df)*100:.1f}%)\")\n",
    "    print(f\"📊 Registros com propriedades funcionais: {stats['registros_com_func']:,} ({stats['registros_com_func']/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Análise de frequências\n",
    "    print(f\"\\\\n🔍 Analisando frequências...\")\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        # Nanomateriais\n",
    "        if pd.notna(row.get('nanomateriais_encontrados')):\n",
    "            try:\n",
    "                nanomateriais = json.loads(row['nanomateriais_encontrados'])\n",
    "                for nano in nanomateriais:\n",
    "                    stats['nanomateriais_frequencia'][nano] += 1\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Propriedades funcionais\n",
    "        if pd.notna(row.get('propriedades_funcionais')):\n",
    "            try:\n",
    "                propriedades = json.loads(row['propriedades_funcionais'])\n",
    "                for prop in propriedades:\n",
    "                    stats['propriedades_frequencia'][prop] += 1\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Métodos de síntese\n",
    "        if pd.notna(row.get('metodos_sintese')):\n",
    "            try:\n",
    "                metodos = json.loads(row['metodos_sintese'])\n",
    "                for metodo in metodos:\n",
    "                    stats['metodos_sintese_frequencia'][metodo] += 1\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # Top 10 de cada categoria\n",
    "    print(f\"\\n🏆 TOP 10 NANOMATERIAIS:\")\n",
    "    for material, freq in stats['nanomateriais_frequencia'].most_common(10):\n",
    "        print(f\"   {material}: {freq:,} menções\")\n",
    "    \n",
    "    print(f\"\\n🏆 TOP 10 PROPRIEDADES FUNCIONAIS:\")\n",
    "    for prop, freq in stats['propriedades_frequencia'].most_common(10):\n",
    "        print(f\"   {prop}: {freq:,} menções\")\n",
    "    \n",
    "    print(f\"\\n🏆 TOP 10 MÉTODOS DE SÍNTESE:\")\n",
    "    for metodo, freq in stats['metodos_sintese_frequencia'].most_common(10):\n",
    "        print(f\"   {metodo}: {freq:,} menções\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Gerar estatísticas\n",
    "regex_stats = generate_regex_statistics(df_analyzed)\n",
    "print(\"\\n✅ Estatísticas geradas com sucesso\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec41d71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 ANALISANDO CO-OCORRÊNCIAS\n",
      "==============================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏆 TOP 20 CO-OCORRÊNCIAS:\n",
      "   tio2 + coating: 387 registros\n",
      "   al2o3 + coating: 324 registros\n",
      "   silica + coating: 282 registros\n",
      "   sio2 + coating: 279 registros\n",
      "   graphene + coating: 256 registros\n",
      "   tio2 + coatings: 239 registros\n",
      "   al2o3 + coatings: 238 registros\n",
      "   alumina + coating: 216 registros\n",
      "   nanocomposite + coating: 213 registros\n",
      "   zno + coating: 203 registros\n",
      "   zirconia + coating: 182 registros\n",
      "   epoxy + coating: 176 registros\n",
      "   silver + coating: 174 registros\n",
      "   sio2 + coatings: 166 registros\n",
      "   zirconia + coatings: 164 registros\n",
      "   alumina + coatings: 160 registros\n",
      "   silica + coatings: 158 registros\n",
      "   hydroxyapatite + coating: 149 registros\n",
      "   nanocomposite + coatings: 143 registros\n",
      "   tio2 + photocatalytic: 142 registros\n",
      "🎯 IDENTIFICANDO REGISTROS DE ALTA RELEVÂNCIA\n",
      "==================================================\n",
      "📊 Registros com alta relevância nano (≥10.0): 1,168\n",
      "📊 Registros com alta relevância funcional (≥5.0): 6,529\n",
      "🎯 Registros com AMBAS as relevâncias altas: 1,006\n",
      "\n",
      "📈 DISTRIBUIÇÃO POR CATEGORIA:\n",
      "   high_functional: 5,523 (61.1%)\n",
      "   low: 2,355 (26.0%)\n",
      "   high_both: 1,006 (11.1%)\n",
      "   high_nano: 162 (1.8%)\n",
      "\n",
      "✅ Identificação de registros de alta relevância concluída\n",
      "\n",
      "✅ Identificação de registros de alta relevância concluída\n"
     ]
    }
   ],
   "source": [
    "def analyze_cooccurrences(df: pd.DataFrame) -> Dict[str, any]:\n",
    "    \"\"\"Analisa co-ocorrências entre nanomateriais e propriedades funcionais\"\"\"\n",
    "    \n",
    "    print(\"🔗 ANALISANDO CO-OCORRÊNCIAS\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    cooccurrences = defaultdict(int)\n",
    "    material_property_pairs = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        # Extrair nanomateriais e propriedades\n",
    "        nanomateriais = []\n",
    "        propriedades = []\n",
    "        \n",
    "        if pd.notna(row.get('nanomateriais_encontrados')):\n",
    "            try:\n",
    "                nanomateriais = json.loads(row['nanomateriais_encontrados'])\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        if pd.notna(row.get('propriedades_funcionais')):\n",
    "            try:\n",
    "                propriedades = json.loads(row['propriedades_funcionais'])\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Calcular co-ocorrências\n",
    "        for nano in nanomateriais:\n",
    "            for prop in propriedades:\n",
    "                pair = f\"{nano} + {prop}\"\n",
    "                cooccurrences[pair] += 1\n",
    "                material_property_pairs.append((nano, prop))\n",
    "    \n",
    "    # Top 20 co-ocorrências\n",
    "    print(f\"\\n🏆 TOP 20 CO-OCORRÊNCIAS:\")\n",
    "    for pair, count in Counter(cooccurrences).most_common(20):\n",
    "        print(f\"   {pair}: {count} registros\")\n",
    "    \n",
    "    return {\n",
    "        'cooccurrences': dict(cooccurrences),\n",
    "        'total_pairs': len(material_property_pairs),\n",
    "        'unique_pairs': len(set(material_property_pairs))\n",
    "    }\n",
    "\n",
    "def identify_high_relevance_records(df: pd.DataFrame, \n",
    "                                  nano_threshold: float = 10.0,\n",
    "                                  func_threshold: float = 5.0) -> pd.DataFrame:\n",
    "    \"\"\"Identifica registros com alta relevância para nanotecnologia de tintas\"\"\"\n",
    "    \n",
    "    print(f\"🎯 IDENTIFICANDO REGISTROS DE ALTA RELEVÂNCIA\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Filtros de relevância\n",
    "    nano_scores = pd.to_numeric(df.get('nano_relevance_score', 0), errors='coerce').fillna(0)\n",
    "    func_scores = pd.to_numeric(df.get('functional_relevance_score', 0), errors='coerce').fillna(0)\n",
    "    \n",
    "    # Diferentes níveis de relevância\n",
    "    high_nano = df[nano_scores >= nano_threshold]\n",
    "    high_func = df[func_scores >= func_threshold]\n",
    "    high_both = df[(nano_scores >= nano_threshold) & (func_scores >= func_threshold)]\n",
    "    \n",
    "    print(f\"📊 Registros com alta relevância nano (≥{nano_threshold}): {len(high_nano):,}\")\n",
    "    print(f\"📊 Registros com alta relevância funcional (≥{func_threshold}): {len(high_func):,}\")\n",
    "    print(f\"🎯 Registros com AMBAS as relevâncias altas: {len(high_both):,}\")\n",
    "    \n",
    "    # Adicionar classificação de relevância\n",
    "    df_result = df.copy()\n",
    "    df_result['relevance_category'] = 'low'\n",
    "    \n",
    "    df_result.loc[nano_scores >= nano_threshold, 'relevance_category'] = 'high_nano'\n",
    "    df_result.loc[func_scores >= func_threshold, 'relevance_category'] = 'high_functional'\n",
    "    df_result.loc[(nano_scores >= nano_threshold) & (func_scores >= func_threshold), 'relevance_category'] = 'high_both'\n",
    "    \n",
    "    # Estatísticas por categoria\n",
    "    category_counts = df_result['relevance_category'].value_counts()\n",
    "    print(\"\\n📈 DISTRIBUIÇÃO POR CATEGORIA:\")\n",
    "    for category, count in category_counts.items():\n",
    "        percentage = (count / len(df_result)) * 100\n",
    "        print(f\"   {category}: {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    return df_result\n",
    "\n",
    "# Analisar co-ocorrências\n",
    "cooccurrence_analysis = analyze_cooccurrences(df_analyzed)\n",
    "\n",
    "# Identificar registros de alta relevância\n",
    "df_with_relevance = identify_high_relevance_records(df_analyzed)\n",
    "print(\"\\n✅ Identificação de registros de alta relevância concluída\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90e73ce",
   "metadata": {},
   "source": [
    "## 💾 Salvamento dos Resultados da Análise Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9c413ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 SALVANDO RESULTADOS DA ANÁLISE REGEX\n",
      "========================================\n",
      "✅ Dataset salvo: /home/delon/Modelos/modeloCenanoInk/data/processed/dataset_with_regex_analysis.csv\n",
      "✅ Dataset salvo: /home/delon/Modelos/modeloCenanoInk/data/processed/dataset_with_regex_analysis.csv\n",
      "✅ Registros de alta relevância salvos: /home/delon/Modelos/modeloCenanoInk/data/processed/high_relevance_records.csv\n",
      "✅ Estatísticas salvas: /home/delon/Modelos/modeloCenanoInk/data/processed/regex_analysis_statistics.json\n",
      "✅ Co-ocorrências salvas: /home/delon/Modelos/modeloCenanoInk/data/processed/cooccurrence_analysis.json\n",
      "✅ Resumo salvo: /home/delon/Modelos/modeloCenanoInk/data/processed/regex_analysis_summary.json\n",
      "\n",
      "🎉 ANÁLISE REGEX CONCLUÍDA COM SUCESSO!\n",
      "📊 Total de registros processados: 9,046\n",
      "🎯 Registros de alta relevância: 6,691\n",
      "💾 Arquivos gerados: 5\n",
      "\n",
      "📁 Arquivos salvos:\n",
      "   dataset: /home/delon/Modelos/modeloCenanoInk/data/processed/dataset_with_regex_analysis.csv\n",
      "   high_relevance: /home/delon/Modelos/modeloCenanoInk/data/processed/high_relevance_records.csv\n",
      "   statistics: /home/delon/Modelos/modeloCenanoInk/data/processed/regex_analysis_statistics.json\n",
      "   cooccurrences: /home/delon/Modelos/modeloCenanoInk/data/processed/cooccurrence_analysis.json\n",
      "   summary: /home/delon/Modelos/modeloCenanoInk/data/processed/regex_analysis_summary.json\n",
      "✅ Registros de alta relevância salvos: /home/delon/Modelos/modeloCenanoInk/data/processed/high_relevance_records.csv\n",
      "✅ Estatísticas salvas: /home/delon/Modelos/modeloCenanoInk/data/processed/regex_analysis_statistics.json\n",
      "✅ Co-ocorrências salvas: /home/delon/Modelos/modeloCenanoInk/data/processed/cooccurrence_analysis.json\n",
      "✅ Resumo salvo: /home/delon/Modelos/modeloCenanoInk/data/processed/regex_analysis_summary.json\n",
      "\n",
      "🎉 ANÁLISE REGEX CONCLUÍDA COM SUCESSO!\n",
      "📊 Total de registros processados: 9,046\n",
      "🎯 Registros de alta relevância: 6,691\n",
      "💾 Arquivos gerados: 5\n",
      "\n",
      "📁 Arquivos salvos:\n",
      "   dataset: /home/delon/Modelos/modeloCenanoInk/data/processed/dataset_with_regex_analysis.csv\n",
      "   high_relevance: /home/delon/Modelos/modeloCenanoInk/data/processed/high_relevance_records.csv\n",
      "   statistics: /home/delon/Modelos/modeloCenanoInk/data/processed/regex_analysis_statistics.json\n",
      "   cooccurrences: /home/delon/Modelos/modeloCenanoInk/data/processed/cooccurrence_analysis.json\n",
      "   summary: /home/delon/Modelos/modeloCenanoInk/data/processed/regex_analysis_summary.json\n"
     ]
    }
   ],
   "source": [
    "def save_regex_results(df: pd.DataFrame, stats: Dict, cooccurrences: Dict) -> Dict[str, str]:\n",
    "    \"\"\"Salva todos os resultados da análise regex\"\"\"\n",
    "    \n",
    "    print(\"💾 SALVANDO RESULTADOS DA ANÁLISE REGEX\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Criar diretório se não existir\n",
    "    processed_dir = PATHS.get('processed', './processed/')\n",
    "    os.makedirs(processed_dir, exist_ok=True)\n",
    "    \n",
    "    # Caminhos dos arquivos\n",
    "    files_saved = {}\n",
    "    \n",
    "    # 1. Dataset com análise regex\n",
    "    dataset_path = os.path.join(processed_dir, 'dataset_with_regex_analysis.csv')\n",
    "    df.to_csv(dataset_path, index=False, encoding='utf-8-sig')\n",
    "    files_saved['dataset'] = dataset_path\n",
    "    print(f\"✅ Dataset salvo: {dataset_path}\")\n",
    "    \n",
    "    # 2. Registros de alta relevância\n",
    "    high_relevance = df[df['relevance_category'].isin(['high_nano', 'high_functional', 'high_both'])]\n",
    "    high_relevance_path = os.path.join(processed_dir, 'high_relevance_records.csv')\n",
    "    high_relevance.to_csv(high_relevance_path, index=False, encoding='utf-8-sig')\n",
    "    files_saved['high_relevance'] = high_relevance_path\n",
    "    print(f\"✅ Registros de alta relevância salvos: {high_relevance_path}\")\n",
    "    \n",
    "    # 3. Estatísticas da análise\n",
    "    stats_path = os.path.join(processed_dir, 'regex_analysis_statistics.json')\n",
    "    with open(stats_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(stats, f, ensure_ascii=False, indent=2, default=str)\n",
    "    files_saved['statistics'] = stats_path\n",
    "    print(f\"✅ Estatísticas salvas: {stats_path}\")\n",
    "    \n",
    "    # 4. Análise de co-ocorrências\n",
    "    cooccurrence_path = os.path.join(processed_dir, 'cooccurrence_analysis.json')\n",
    "    with open(cooccurrence_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(cooccurrences, f, ensure_ascii=False, indent=2, default=str)\n",
    "    files_saved['cooccurrences'] = cooccurrence_path\n",
    "    print(f\"✅ Co-ocorrências salvas: {cooccurrence_path}\")\n",
    "    \n",
    "    # 5. Resumo da análise\n",
    "    summary = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'notebook': '03_analise_regex.ipynb',\n",
    "        'total_records': len(df),\n",
    "        'high_relevance_records': len(high_relevance),\n",
    "        'nanomaterials_found': len(stats['nanomateriais_frequencia']),\n",
    "        'functional_properties_found': len(stats['propriedades_frequencia']),\n",
    "        'synthesis_methods_found': len(stats['metodos_sintese_frequencia']),\n",
    "        'files_generated': files_saved,\n",
    "        'next_notebook': '04_analise_gemini.ipynb'\n",
    "    }\n",
    "    \n",
    "    summary_path = os.path.join(processed_dir, 'regex_analysis_summary.json')\n",
    "    with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(summary, f, ensure_ascii=False, indent=2)\n",
    "    files_saved['summary'] = summary_path\n",
    "    print(f\"✅ Resumo salvo: {summary_path}\")\n",
    "    \n",
    "    return files_saved\n",
    "\n",
    "# Salvar todos os resultados\n",
    "saved_files = save_regex_results(df_with_relevance, regex_stats, cooccurrence_analysis)\n",
    "\n",
    "print(f\"\\n🎉 ANÁLISE REGEX CONCLUÍDA COM SUCESSO!\")\n",
    "print(f\"📊 Total de registros processados: {len(df_with_relevance):,}\")\n",
    "print(f\"🎯 Registros de alta relevância: {len(df_with_relevance[df_with_relevance['relevance_category'] != 'low']):,}\")\n",
    "print(f\"💾 Arquivos gerados: {len(saved_files)}\")\n",
    "print(f\"\\n📁 Arquivos salvos:\")\n",
    "for desc, path in saved_files.items():\n",
    "    print(f\"   {desc}: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57a7435a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Relatório da Análise Regex:\n",
      "  Total de artigos analisados: 9,046\n",
      "  Distribuição por categoria de relevância:\n",
      "    high_functional: 5,523 artigos\n",
      "    low: 2,355 artigos\n",
      "    high_both: 1,006 artigos\n",
      "    high_nano: 162 artigos\n",
      "  Estatísticas de nanomateriais:\n",
      "    Artigos com nanomateriais: 4,235\n",
      "    Score médio: 3.94\n",
      "  Top nanomateriais encontrados:\n",
      "    tio2: 550\n",
      "    al2o3: 446\n",
      "    sio2: 383\n",
      "    silica: 382\n",
      "    graphene: 334\n",
      "💾 Relatório salvo em '/home/delon/Modelos/modeloCenanoInk/data/processed/relatorio_analise_regex.json'\n"
     ]
    }
   ],
   "source": [
    "def gerar_relatorio_regex(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Gera relatório detalhado da análise regex.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame com análise regex\n",
    "    \n",
    "    Returns:\n",
    "        Dicionário com estatísticas\n",
    "    \"\"\"\n",
    "    def convert_numpy_types(obj):\n",
    "        \"\"\"Converte tipos NumPy para tipos Python nativos para serialização JSON\"\"\"\n",
    "        if hasattr(obj, 'item'):\n",
    "            return obj.item()\n",
    "        elif hasattr(obj, 'tolist'):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, dict):\n",
    "            return {k: convert_numpy_types(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [convert_numpy_types(v) for v in obj]\n",
    "        else:\n",
    "            return obj\n",
    "    \n",
    "    if df.empty:\n",
    "        return {}\n",
    "    \n",
    "    # Usar as colunas corretas baseadas na análise atual\n",
    "    relatorio = {\n",
    "        'total_artigos_analisados': int(len(df)),\n",
    "        'distribuicao_escopo': convert_numpy_types(df['relevance_category'].value_counts().to_dict() if 'relevance_category' in df.columns else {}),\n",
    "        'estatisticas_nanomateriais': {\n",
    "            'media_score': float(df['nano_relevance_score'].mean() if 'nano_relevance_score' in df.columns else 0),\n",
    "            'mediana_score': float(df['nano_relevance_score'].median() if 'nano_relevance_score' in df.columns else 0),\n",
    "            'max_score': float(df['nano_relevance_score'].max() if 'nano_relevance_score' in df.columns else 0),\n",
    "            'artigos_com_nanomateriais': int((df['nano_relevance_score'] > 0).sum() if 'nano_relevance_score' in df.columns else 0)\n",
    "        },\n",
    "        'estatisticas_funcionais': {\n",
    "            'media_score': float(df['functional_relevance_score'].mean() if 'functional_relevance_score' in df.columns else 0),\n",
    "            'mediana_score': float(df['functional_relevance_score'].median() if 'functional_relevance_score' in df.columns else 0),\n",
    "            'max_score': float(df['functional_relevance_score'].max() if 'functional_relevance_score' in df.columns else 0),\n",
    "            'artigos_com_funcionais': int((df['functional_relevance_score'] > 0).sum() if 'functional_relevance_score' in df.columns else 0)\n",
    "        },\n",
    "        'top_nanomateriais': {},\n",
    "        'top_propriedades_funcionais': {}\n",
    "    }\n",
    "    \n",
    "    # Extrair nanomateriais mais frequentes\n",
    "    if 'nanomateriais_encontrados' in df.columns:\n",
    "        todos_nanomateriais = []\n",
    "        for nanomateriais_json in df['nanomateriais_encontrados'].dropna():\n",
    "            try:\n",
    "                nanomateriais = json.loads(nanomateriais_json)\n",
    "                todos_nanomateriais.extend(nanomateriais)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        if todos_nanomateriais:\n",
    "            contador_nanomateriais = Counter(todos_nanomateriais)\n",
    "            relatorio['top_nanomateriais'] = dict(contador_nanomateriais.most_common(10))\n",
    "    \n",
    "    # Extrair propriedades funcionais mais frequentes\n",
    "    if 'propriedades_funcionais' in df.columns:\n",
    "        todas_propriedades = []\n",
    "        for propriedades_json in df['propriedades_funcionais'].dropna():\n",
    "            try:\n",
    "                propriedades = json.loads(propriedades_json)\n",
    "                todas_propriedades.extend(propriedades)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        if todas_propriedades:\n",
    "            contador_propriedades = Counter(todas_propriedades)\n",
    "            relatorio['top_propriedades_funcionais'] = dict(contador_propriedades.most_common(10))\n",
    "    \n",
    "    return relatorio\n",
    "\n",
    "# Gerar relatório usando os dados corretos\n",
    "if not df_with_relevance.empty:\n",
    "    relatorio_regex = gerar_relatorio_regex(df_with_relevance)\n",
    "    \n",
    "    # Criar diretório se não existir\n",
    "    processed_dir = PATHS.get('processed', './processed/')\n",
    "    os.makedirs(processed_dir, exist_ok=True)\n",
    "    \n",
    "    # Salvar relatório\n",
    "    relatorio_path = os.path.join(processed_dir, 'relatorio_analise_regex.json')\n",
    "    with open(relatorio_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(relatorio_regex, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(\"📊 Relatório da Análise Regex:\")\n",
    "    print(f\"  Total de artigos analisados: {relatorio_regex['total_artigos_analisados']:,}\")\n",
    "    print(\"  Distribuição por categoria de relevância:\")\n",
    "    for categoria, count in relatorio_regex['distribuicao_escopo'].items():\n",
    "        print(f\"    {categoria}: {count:,} artigos\")\n",
    "    \n",
    "    print(\"  Estatísticas de nanomateriais:\")\n",
    "    stats_nano = relatorio_regex['estatisticas_nanomateriais']\n",
    "    print(f\"    Artigos com nanomateriais: {stats_nano['artigos_com_nanomateriais']:,}\")\n",
    "    print(f\"    Score médio: {stats_nano['media_score']:.2f}\")\n",
    "    \n",
    "    print(\"  Top nanomateriais encontrados:\")\n",
    "    for nanomaterial, count in list(relatorio_regex['top_nanomateriais'].items())[:5]:\n",
    "        print(f\"    {nanomaterial}: {count:,}\")\n",
    "    \n",
    "    print(f\"💾 Relatório salvo em '{relatorio_path}'\")\n",
    "else:\n",
    "    print(\"⚠️  Dados não disponíveis para relatório\")\n",
    "    relatorio_regex = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b320ff",
   "metadata": {},
   "source": [
    "## 🔄 Preparação para Próxima Etapa (Análise Gemini)\n",
    "\n",
    "# Verificar se o sistema está pronto para a próxima etapa\n",
    "ready_for_gemini = True\n",
    "ready_checks = []\n",
    "\n",
    "# Verificar arquivos necessários\n",
    "required_files = [\n",
    "    'dataset_with_regex_analysis.csv',\n",
    "    'high_relevance_records.csv', \n",
    "    'regex_analysis_statistics.json'\n",
    "]\n",
    "\n",
    "processed_dir = PATHS.get('processed', './processed/')\n",
    "for filename in required_files:\n",
    "    filepath = os.path.join(processed_dir, filename)\n",
    "    if os.path.exists(filepath):\n",
    "        ready_checks.append(f\"✅ {filename}\")\n",
    "    else:\n",
    "        ready_checks.append(f\"❌ {filename}\")\n",
    "        ready_for_gemini = False\n",
    "\n",
    "# Verificar configuração da API Gemini\n",
    "if CONFIG.get('gemini', {}).get('api_key'):\n",
    "    ready_checks.append(\"✅ API Gemini configurada\")\n",
    "else:\n",
    "    ready_checks.append(\"❌ API Gemini não configurada\")\n",
    "    ready_for_gemini = False\n",
    "\n",
    "# Atualizar configuração do sistema\n",
    "config_data['regex_analysis'] = {\n",
    "    'completed': True,\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'records_processed': len(df_with_relevance),\n",
    "    'high_relevance_records': len(df_with_relevance[df_with_relevance['relevance_category'] != 'low']),\n",
    "    'files_generated': list(saved_files.keys()),\n",
    "    'ready_for_gemini': ready_for_gemini\n",
    "}\n",
    "\n",
    "# Salvar configuração atualizada\n",
    "with open('config_sistema.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(config_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"🔄 PREPARAÇÃO PARA PRÓXIMA ETAPA\")\n",
    "print(\"=\" * 35)\n",
    "print(\"\\n📋 Verificações:\")\n",
    "for check in ready_checks:\n",
    "    print(f\"   {check}\")\n",
    "\n",
    "if ready_for_gemini:\n",
    "    print(\"\\n✅ SISTEMA PRONTO PARA ANÁLISE GEMINI!\")\n",
    "    print(\"🚀 Próximo notebook: 04_analise_gemini.ipynb\")\n",
    "else:\n",
    "    print(\"\\n⚠️ Sistema não está completamente pronto para análise Gemini\")\n",
    "    print(\"💡 Verifique as configurações e arquivos faltantes acima\")\n",
    "\n",
    "print(f\"\\n📊 RESUMO FINAL:\")\n",
    "print(f\"   📁 Registros processados: {len(df_with_relevance):,}\")\n",
    "print(f\"   🎯 Alta relevância nano: {len(df_with_relevance[df_with_relevance['relevance_category'].isin(['high_nano', 'high_both'])]):,}\")\n",
    "print(f\"   🔧 Alta relevância funcional: {len(df_with_relevance[df_with_relevance['relevance_category'].isin(['high_functional', 'high_both'])]):,}\")\n",
    "print(f\"   ⭐ Ambas relevâncias altas: {len(df_with_relevance[df_with_relevance['relevance_category'] == 'high_both']):,}\")\n",
    "print(f\"   🧪 Nanomateriais únicos: {len(regex_stats['nanomateriais_frequencia'])}\")\n",
    "print(f\"   ⚙️ Propriedades funcionais únicas: {len(regex_stats['propriedades_frequencia'])}\")\n",
    "\n",
    "print(\"\\n🎉 NOTEBOOK 03 - ANÁLISE REGEX CONCLUÍDO!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b18867a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Notebook 03 - Análise Regex concluído\n",
      "📊 9,046 artigos processados com análise regex\n",
      "🔄 Próxima etapa: 04_analise_gemini.ipynb\n",
      "💾 Metadados salvos em '/home/delon/Modelos/modeloCenanoInk/data/processed/metadados_analise_regex.json'\n",
      "\n",
      "🎯 RESUMO FINAL DA ANÁLISE:\n",
      "   📁 Total de registros: 9,046\n",
      "   high_functional: 5,523 registros\n",
      "   low: 2,355 registros\n",
      "   high_both: 1,006 registros\n",
      "   high_nano: 162 registros\n",
      "   🧪 Nanomateriais únicos: 92\n",
      "   ⚙️ Propriedades funcionais únicas: 50\n"
     ]
    }
   ],
   "source": [
    "# Atualizar metadados para próxima etapa\n",
    "metadados_regex = {\n",
    "    'notebook': '03_analise_regex',\n",
    "    'timestamp': pd.Timestamp.now().isoformat(),\n",
    "    'dados_processados': os.path.join(PATHS.get('processed', './processed/'), 'dataset_with_regex_analysis.csv'),\n",
    "    'relatorio': os.path.join(PATHS.get('processed', './processed/'), 'relatorio_analise_regex.json'),\n",
    "    'total_artigos': len(df_with_relevance) if 'df_with_relevance' in locals() and not df_with_relevance.empty else 0,\n",
    "    'colunas_adicionadas': [\n",
    "        'nano_relevance_score',\n",
    "        'functional_relevance_score',\n",
    "        'total_matches',\n",
    "        'nanomateriais_encontrados',\n",
    "        'propriedades_funcionais',\n",
    "        'metodos_sintese',\n",
    "        'medidas_encontradas',\n",
    "        'analise_completa',\n",
    "        'relevance_category'\n",
    "    ],\n",
    "    'proxima_etapa': '04_analise_gemini.ipynb'\n",
    "}\n",
    "\n",
    "# Salvar metadados\n",
    "metadados_path = os.path.join(PATHS.get('processed', './processed/'), 'metadados_analise_regex.json')\n",
    "with open(metadados_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadados_regex, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"✅ Notebook 03 - Análise Regex concluído\")\n",
    "print(f\"📊 {metadados_regex['total_artigos']:,} artigos processados com análise regex\")\n",
    "print(\"🔄 Próxima etapa: 04_analise_gemini.ipynb\")\n",
    "print(f\"💾 Metadados salvos em '{metadados_path}'\")\n",
    "\n",
    "# Exibir resumo final\n",
    "if 'df_with_relevance' in locals() and not df_with_relevance.empty:\n",
    "    print(\"\\n🎯 RESUMO FINAL DA ANÁLISE:\")\n",
    "    print(f\"   📁 Total de registros: {len(df_with_relevance):,}\")\n",
    "    if 'relevance_category' in df_with_relevance.columns:\n",
    "        category_counts = df_with_relevance['relevance_category'].value_counts()\n",
    "        for category, count in category_counts.items():\n",
    "            print(f\"   {category}: {count:,} registros\")\n",
    "    print(f\"   🧪 Nanomateriais únicos: {len(regex_stats['nanomateriais_frequencia']) if 'regex_stats' in locals() else 0}\")\n",
    "    print(f\"   ⚙️ Propriedades funcionais únicas: {len(regex_stats['propriedades_frequencia']) if 'regex_stats' in locals() else 0}\")\n",
    "else:\n",
    "    print(\"\\n⚠️ Dados não disponíveis para resumo final\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
