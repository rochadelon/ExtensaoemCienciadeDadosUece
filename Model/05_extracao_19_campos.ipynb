{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75a26091",
   "metadata": {},
   "source": [
    "# Notebook 05: Extração Detalhada de 19 Campos com Gemini\n",
    "\n",
    "Este notebook foca em realizar uma extração de dados altamente específica e estruturada a partir dos textos processados (resumos e títulos de artigos/patentes). Utilizaremos o Google Gemini com um prompt detalhado para preencher 19 campos pré-definidos, visando criar um dataset rico para análises futuras e para o sistema de recomendação de formulações de tintas nanotecnológicas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9a5ff3",
   "metadata": {},
   "source": [
    "## 1. Configuração Inicial e Carregamento de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bdb4ad5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e48a6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caminho do arquivo da API Key: api_key.txt\n",
      "Caminho dos dados processados: ../data/processed\n",
      "Arquivo de entrada (análise regex anterior): ../data/processed/dataset_with_regex_analysis.csv\n",
      "Arquivo de saída (19 campos extraídos): ../data/processed/dataset_with_19_fields.csv\n",
      "Arquivo de sumário da extração (19 campos): ../data/processed/extraction_19_fields_summary.json\n"
     ]
    }
   ],
   "source": [
    "# Carregar configurações do sistema\n",
    "config_path = Path('config_sistema.json')\n",
    "if config_path.exists():\n",
    "    with open(config_path, 'r', encoding='utf-8') as f:\n",
    "        config_sistema = json.load(f)\n",
    "else:\n",
    "    print(\"Arquivo de configuração 'config_sistema.json' não encontrado. Execute o notebook 01 primeiro.\")\n",
    "    config_sistema = {}\n",
    "\n",
    "# Caminhos (baseados no config_sistema.json)\n",
    "API_KEY_FILE = Path(config_sistema.get('API_KEY_PATH', 'api_key.txt'))\n",
    "PROCESSED_DATA_PATH = Path(config_sistema.get('PROCESSED_DATA_PATH', '../data/processed/'))\n",
    "\n",
    "# Arquivo de entrada (saída do notebook 03 - análise regex)\n",
    "GEMINI_ANALYSIS_INPUT_FILE = PROCESSED_DATA_PATH / 'dataset_with_regex_analysis.csv'\n",
    "\n",
    "# Saídas deste notebook\n",
    "EXTRACTED_19_FIELDS_OUTPUT_FILE = PROCESSED_DATA_PATH / 'dataset_with_19_fields.csv'\n",
    "EXTRACTED_19_FIELDS_SUMMARY_FILE = PROCESSED_DATA_PATH / 'extraction_19_fields_summary.json'\n",
    "\n",
    "# Criar diretório de dados processados se não existir\n",
    "PROCESSED_DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Caminho do arquivo da API Key: {API_KEY_FILE}\")\n",
    "print(f\"Caminho dos dados processados: {PROCESSED_DATA_PATH}\")\n",
    "print(f\"Arquivo de entrada (análise regex anterior): {GEMINI_ANALYSIS_INPUT_FILE}\")\n",
    "print(f\"Arquivo de saída (19 campos extraídos): {EXTRACTED_19_FIELDS_OUTPUT_FILE}\")\n",
    "print(f\"Arquivo de sumário da extração (19 campos): {EXTRACTED_19_FIELDS_SUMMARY_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e0f45ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key do Google Gemini configurada com sucesso.\n"
     ]
    }
   ],
   "source": [
    "# Configurar API Key do Google Gemini\n",
    "try:\n",
    "    with open(API_KEY_FILE, 'r') as f:\n",
    "        api_key = f.read().strip()\n",
    "    genai.configure(api_key=api_key)\n",
    "    print(\"API Key do Google Gemini configurada com sucesso.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Arquivo da API Key '{API_KEY_FILE}' não encontrado. Verifique o caminho e o arquivo.\")\n",
    "    api_key = None\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao configurar a API Key: {e}\")\n",
    "    api_key = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e9c4cf05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset da análise Gemini anterior carregado: 9046 registros.\n",
      "Colunas disponíveis: ['Publication Type', 'Authors', 'Book Authors', 'Book Editors', 'Book Group Authors', 'Author Full Names', 'Book Author Full Names', 'Group Authors', 'Article Title', 'Source Title', 'Book Series Title', 'Book Series Subtitle', 'Language', 'Document Type', 'Conference Title', 'Conference Date', 'Conference Location', 'Conference Sponsor', 'Conference Host', 'Author Keywords', 'Keywords Plus', 'Abstract', 'Addresses', 'Affiliations', 'Reprint Addresses', 'Email Addresses', 'Researcher Ids', 'ORCIDs', 'Funding Orgs', 'Funding Name Preferred', 'Funding Text', 'Cited References', 'Cited Reference Count', 'Times Cited, WoS Core', 'Times Cited, All Databases', '180 Day Usage Count', 'Since 2013 Usage Count', 'Publisher', 'Publisher City', 'Publisher Address', 'ISSN', 'eISSN', 'ISBN', 'Journal Abbreviation', 'Journal ISO Abbreviation', 'Publication Date', 'Publication Year', 'Volume', 'Issue', 'Part Number', 'Supplement', 'Special Issue', 'Meeting Abstract', 'Start Page', 'End Page', 'Article Number', 'DOI', 'DOI Link', 'Book DOI', 'Early Access Date', 'Number of Pages', 'WoS Categories', 'Web of Science Index', 'Research Areas', 'IDS Number', 'Pubmed Id', 'Open Access Designations', 'Highly Cited Status', 'Hot Paper Status', 'Date of Export', 'UT (Unique WOS ID)', 'Web of Science Record', 'arquivo_origem', 'batch_numero', 'data_carregamento', 'filtrado_open_access', 'nano_relevance_score', 'functional_relevance_score', 'total_matches', 'nanomateriais_encontrados', 'propriedades_funcionais', 'metodos_sintese', 'medidas_encontradas', 'analise_completa', 'relevance_category']\n",
      "  Publication Type                                            Authors  \\\n",
      "0                J  Meng, XC; Huang, YX; Liu, S; Xie, YM; Li, JC; ...   \n",
      "1                J  Mogilevsky, P; Boakye, EE; Key, TS; Parthasara...   \n",
      "2                J  Monteiro, ED; Kasal, RB; Moraes, NC; de Melo, ...   \n",
      "3                J               Muhire, E; Yang, J; Huo, XJ; Gao, MZ   \n",
      "4                J           Naser, AA; Al-Mubarak, AS; Al-Sawaad, HZ   \n",
      "\n",
      "   Book Authors  Book Editors  Book Group Authors  \\\n",
      "0           NaN           NaN                 NaN   \n",
      "1           NaN           NaN                 NaN   \n",
      "2           NaN           NaN                 NaN   \n",
      "3           NaN           NaN                 NaN   \n",
      "4           NaN           NaN                 NaN   \n",
      "\n",
      "                                   Author Full Names  Book Author Full Names  \\\n",
      "0  Meng, Xiangchen; Huang, Yongxian; Liu, Shuai; ...                     NaN   \n",
      "1  Mogilevsky, Pavel; Boakye, Emmanuel E.; Key, T...                     NaN   \n",
      "2  Monteiro, Emilia dos Santos; Kasal, Raphael Ba...                     NaN   \n",
      "3  Muhire, Elisee; Yang, Jiao; Huo, Xuejian; Gao,...                     NaN   \n",
      "4  Naser, Ali A.; Al-Mubarak, Alaa S.; Al-Sawaad,...                     NaN   \n",
      "\n",
      "  Group Authors                                      Article Title  \\\n",
      "0           NaN  Functionally Gradient Coating of Aluminum Allo...   \n",
      "1           NaN  In situ Y2Si2O7 coatings on SiC fibers: Thermo...   \n",
      "2           NaN  Nanoparticles of Ni1-xZnxFe2O4 used as Microwa...   \n",
      "3           NaN  Dependence of Electrical and Optical Propertie...   \n",
      "4           NaN  Synthesis, characterization and evaluation of ...   \n",
      "\n",
      "                                        Source Title  ...  \\\n",
      "0                     ADVANCED ENGINEERING MATERIALS  ...   \n",
      "1            JOURNAL OF THE AMERICAN CERAMIC SOCIETY  ...   \n",
      "2  MATERIALS RESEARCH-IBERO-AMERICAN JOURNAL OF M...  ...   \n",
      "3                     MATERIALS SCIENCE-MEDZIAGOTYRA  ...   \n",
      "4  INTERNATIONAL JOURNAL OF CORROSION AND SCALE I...  ...   \n",
      "\n",
      "   filtrado_open_access  nano_relevance_score functional_relevance_score  \\\n",
      "0                  True              0.000000                  15.544041   \n",
      "1                  True              0.000000                  14.084507   \n",
      "2                  True              0.000000                   6.329114   \n",
      "3                  True              0.000000                  22.222222   \n",
      "4                  True              4.424779                   0.000000   \n",
      "\n",
      "  total_matches  nanomateriais_encontrados  \\\n",
      "0             3                         []   \n",
      "1             2                         []   \n",
      "2             2                         []   \n",
      "3             4                         []   \n",
      "4             5               [\"graphene\"]   \n",
      "\n",
      "                  propriedades_funcionais      metodos_sintese  \\\n",
      "0  [\"automotive\", \"coating\", \"aerospace\"]                   []   \n",
      "1                 [\"coating\", \"coatings\"]                   []   \n",
      "2                               [\"paint\"]                   []   \n",
      "3    [\"sol-gel\", \"thin films\", \"coating\"]          [\"sol-gel\"]   \n",
      "4                                      []  [\"electrochemical\"]   \n",
      "\n",
      "           medidas_encontradas  \\\n",
      "0                           []   \n",
      "1                           []   \n",
      "2                    [\"81 nm\"]   \n",
      "3                           []   \n",
      "4  [\"0.1 m\", \"298 k\", \"328 k\"]   \n",
      "\n",
      "                                    analise_completa relevance_category  \n",
      "0  {\"text_length\": 1330, \"word_count\": 193, \"patt...    high_functional  \n",
      "1  {\"text_length\": 1013, \"word_count\": 142, \"patt...    high_functional  \n",
      "2  {\"text_length\": 1056, \"word_count\": 158, \"patt...    high_functional  \n",
      "3  {\"text_length\": 942, \"word_count\": 135, \"patte...    high_functional  \n",
      "4  {\"text_length\": 1498, \"word_count\": 226, \"patt...                low  \n",
      "\n",
      "[5 rows x 85 columns]\n",
      "ALERTA: Coluna 'Abstract_to_analyze' não encontrada. Esta coluna é essencial para a extração.\n",
      "Coluna 'Abstract_to_analyze' criada a partir da concatenação de 'Article Title' e 'Abstract'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_122208/2015955678.py:3: DtypeWarning: Columns (7,47,50,53,54) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_input = pd.read_csv(GEMINI_ANALYSIS_INPUT_FILE)\n"
     ]
    }
   ],
   "source": [
    "# Carregar dados da análise Gemini anterior (saída do notebook 04)\n",
    "if GEMINI_ANALYSIS_INPUT_FILE.exists():\n",
    "    df_input = pd.read_csv(GEMINI_ANALYSIS_INPUT_FILE)\n",
    "    print(f\"Dataset da análise Gemini anterior carregado: {len(df_input)} registros.\")\n",
    "    print(\"Colunas disponíveis:\", df_input.columns.tolist())\n",
    "    print(df_input.head())\n",
    "    if 'Abstract_to_analyze' not in df_input.columns:\n",
    "        print(\"ALERTA: Coluna 'Abstract_to_analyze' não encontrada. Esta coluna é essencial para a extração.\" )       # Tentar criar a coluna 'Abstract_to_analyze' usando os nomes corretos das colunas\n",
    "        if 'Article Title' in df_input.columns and 'Abstract' in df_input.columns:\n",
    "            df_input['Abstract_to_analyze'] = df_input['Article Title'].fillna('') + \" \" + df_input['Abstract'].fillna('')\n",
    "            print(\"Coluna 'Abstract_to_analyze' criada a partir da concatenação de 'Article Title' e 'Abstract'.\")\n",
    "        elif 'TI' in df_input.columns and 'AB' in df_input.columns:\n",
    "            df_input['Abstract_to_analyze'] = df_input['TI'].fillna('') + \" \" + df_input['AB'].fillna('')\n",
    "            print(\"Coluna 'Abstract_to_analyze' criada a partir da concatenação de 'TI' e 'AB'.\")\n",
    "        else:\n",
    "            print(\"Não foi possível criar 'Abstract_to_analyze'. A extração pode falhar ou ser de baixa qualidade.\")\n",
    "            df_input['Abstract_to_analyze'] = \"\" # Coluna vazia para evitar erros, mas resultará em má extração\n",
    "else:\n",
    "    print(f\"Arquivo '{GEMINI_ANALYSIS_INPUT_FILE}' não encontrado. Execute o notebook 04 primeiro.\")\n",
    "    df_input = pd.DataFrame() # DataFrame vazio para evitar erros subsequentes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629d83fd",
   "metadata": {},
   "source": [
    "## 2. Definição dos 19 Campos para Extração e Configuração da API Gemini\n",
    "\n",
    "Os 19 campos a serem extraídos são:\n",
    "1.  `id_documento`: Identificador único do documento (ex: DOI, número de patente, ou ID interno).\n",
    "2.  `titulo`: Título completo do documento.\n",
    "3.  `ano`: Ano de publicação.\n",
    "4.  `tipo_documento`: Tipo de documento (ex: Artigo Científico, Patente, Revisão, Conferência).\n",
    "5.  `autores`: Lista dos nomes dos autores.\n",
    "6.  `afiliacoes_autores`: Lista das afiliações dos autores.\n",
    "7.  `revista_patente_info`: Nome da revista, conferência ou informações da patente (escritório, número).\n",
    "8.  `pais_origem`: País de publicação ou origem da patente.\n",
    "9.  `palavras_chave_documento`: Lista das palavras-chave fornecidas pelo documento.\n",
    "10. `nanomateriais_citados`: Lista dos nanomateriais específicos investigados ou desenvolvidos.\n",
    "11. `metodo_sintese_fabricacao_detalhado`: Descrição detalhada dos métodos de síntese dos nanomateriais ou fabricação do produto/tinta.\n",
    "12. `precursores_reagentes`: Lista dos principais precursores, reagentes ou matérias-primas utilizadas.\n",
    "13. `parametros_processo`: Parâmetros críticos do processo (ex: temperatura, tempo, pressão, concentração, pH).\n",
    "14. `tecnicas_caracterizacao`: Lista das técnicas de caracterização empregadas para analisar os materiais ou propriedades.\n",
    "15. `propriedades_resultados_especificos`: Propriedades chave investigadas e resultados quantitativos específicos (ex: \"eficiência de 95%\", \"tamanho da partícula de 20-50 nm\", \"condutividade de 150 S/cm\").\n",
    "16. `aplicacao_primaria`: Principal aplicação visada ou testada para o nanomaterial ou produto.\n",
    "17. `mecanismos_acao_explicados`: Breve descrição de quaisquer mecanismos de ação, funcionamento ou fenômenos explicados.\n",
    "18. `vantagens_inovacao_declarada`: Vantagens ou inovações destacadas no estudo.\n",
    "19. `desafios_limitacoes_futuro`: Desafios, limitações identificadas ou trabalhos futuros sugeridos.\n",
    "\n",
    "Se uma informação não estiver explicitamente presente no texto, o campo correspondente no JSON deve ser `null`, uma string vazia (`\"\"`), ou uma lista vazia (`[]`), conforme apropriado para o tipo de dado do campo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6da6a002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo Gemini para extração dos 19 campos (gemini-2.0-pro) inicializado.\n"
     ]
    }
   ],
   "source": [
    "# Parâmetros da API Gemini\n",
    "GEMINI_MODEL_NAME = config_sistema.get('GEMINI_MODEL_NAME_DETAILED_EXTRACTION', 'gemini-2.0-pro')\n",
    "MAX_RETRIES = config_sistema.get('MAX_RETRIES_DETAILED_EXTRACTION', 3)\n",
    "RETRY_DELAY = config_sistema.get('RETRY_DELAY_DETAILED_EXTRACTION', 10)  # segundos, maior para prompts complexos\n",
    "REQUEST_TIMEOUT = config_sistema.get('REQUEST_TIMEOUT_DETAILED_EXTRACTION', 300) # segundos, maior para prompts complexos\n",
    "RATE_LIMIT_DELAY = config_sistema.get('RATE_LIMIT_DELAY_DETAILED_EXTRACTION', 2) # segundos entre requisições\n",
    "\n",
    "generation_config = {\n",
    "    \"temperature\": config_sistema.get('GEMINI_TEMPERATURE_DETAILED_EXTRACTION', 0.5),\n",
    "    \"top_p\": config_sistema.get('GEMINI_TOP_P_DETAILED_EXTRACTION', 0.95),\n",
    "    \"top_k\": config_sistema.get('GEMINI_TOP_K_DETAILED_EXTRACTION', 40),\n",
    "    \"max_output_tokens\": config_sistema.get('GEMINI_MAX_OUTPUT_TOKENS_DETAILED_EXTRACTION', 4096), # Aumentado para JSON complexo\n",
    "    \"response_mime_type\": \"application/json\",\n",
    "}\n",
    "\n",
    "safety_settings = [\n",
    "    {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
    "    {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
    "    {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
    "    {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
    "]\n",
    "\n",
    "if api_key:\n",
    "    model_19_fields = genai.GenerativeModel(\n",
    "        model_name=GEMINI_MODEL_NAME,\n",
    "        generation_config=generation_config,\n",
    "        safety_settings=safety_settings\n",
    "    )\n",
    "    print(f\"Modelo Gemini para extração dos 19 campos ({GEMINI_MODEL_NAME}) inicializado.\")\n",
    "else:\n",
    "    model_19_fields = None\n",
    "    print(\"Modelo Gemini para extração dos 19 campos não pôde ser inicializado (API Key ausente).\")\n",
    "\n",
    "# Lista dos 19 campos para referência no código\n",
    "FIELDS_TO_EXTRACT = [\n",
    "    \"id_documento\", \"titulo\", \"ano\", \"tipo_documento\", \"autores\", \n",
    "    \"afiliacoes_autores\", \"revista_patente_info\", \"pais_origem\", \n",
    "    \"palavras_chave_documento\", \"nanomateriais_citados\", \n",
    "    \"metodo_sintese_fabricacao_detalhado\", \"precursores_reagentes\", \n",
    "    \"parametros_processo\", \"tecnicas_caracterizacao\", \n",
    "    \"propriedades_resultados_especificos\", \"aplicacao_primaria\", \n",
    "    \"mecanismos_acao_explicados\", \"vantagens_inovacao_declarada\", \n",
    "    \"desafios_limitacoes_futuro\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8c960598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_extraction_prompt(text_content):\n",
    "    \"\"\"\n",
    "    Gera o prompt para a API Gemini solicitando a extração dos 19 campos.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Analise o seguinte texto (que pode ser um resumo, título ou combinação de partes de um artigo científico ou patente) sobre nanotecnologia, nanomateriais ou tintas funcionais.\n",
    "    Sua tarefa é extrair informações detalhadas e estruturadas correspondentes aos 19 campos listados abaixo.\n",
    "    Retorne a sua resposta EXCLUSIVAMENTE como um único objeto JSON. Não inclua explicações antes ou depois do JSON.\n",
    "    O objeto JSON deve conter as seguintes chaves (exatamente como escritas):\n",
    "    1.  `id_documento`: (String) Identificador único do documento (ex: DOI, número de patente, ID interno se mencionado no texto). Se não encontrado, use null.\n",
    "    2.  `titulo`: (String) Título completo do documento, conforme presente no texto. Se não encontrado, use null.\n",
    "    3.  `ano`: (Integer or String) Ano de publicação. Extraia apenas o ano. Se não encontrado, use null.\n",
    "    4.  `tipo_documento`: (String) Tipo de documento (ex: \"Artigo Científico\", \"Patente\", \"Revisão\", \"Artigo de Conferência\"). Inferir do contexto se possível. Se não encontrado, use null.\n",
    "    5.  `autores`: (List of Strings) Lista dos nomes dos autores. Ex: [\"Silva, J.\", \"Santos, M.\"]. Se não encontrado, use [].\n",
    "    6.  `afiliacoes_autores`: (List of Strings) Lista das afiliações dos autores. Ex: [\"Universidade X, Brasil\", \"Instituto Y, EUA\"]. Se não encontrado, use [].\n",
    "    7.  `revista_patente_info`: (String) Nome da revista, anais de conferência, ou informações da patente (ex: \"Journal of Nanomaterials\", \"US Patent Office 1234567\"). Se não encontrado, use null.\n",
    "    8.  `pais_origem`: (String) País de publicação ou de origem da patente, se mencionado. Se não encontrado, use null.\n",
    "    9.  `palavras_chave_documento`: (List of Strings) Lista das palavras-chave fornecidas no documento. Se não encontrado, use [].\n",
    "    10. `nanomateriais_citados`: (List of Strings) Lista dos nanomateriais específicos investigados ou desenvolvidos (ex: [\"nanopartículas de TiO2\", \"nanotubos de carbono\", \"grafeno\"]). Se não encontrado, use [].\n",
    "    11. `metodo_sintese_fabricacao_detalhado`: (String) Descrição concisa, mas informativa, dos métodos de síntese dos nanomateriais ou fabricação do produto/tinta. Se não encontrado, use null.\n",
    "    12. `precursores_reagentes`: (List of Strings) Lista dos principais precursores, reagentes ou matérias-primas utilizadas. Se não encontrado, use [].\n",
    "    13. `parametros_processo`: (List of Strings) Parâmetros críticos do processo mencionados (ex: [\"Temperatura: 150°C\", \"Tempo de reação: 24h\", \"pH: 7.0\"]). Se não encontrado, use [].\n",
    "    14. `tecnicas_caracterizacao`: (List of Strings) Lista das técnicas de caracterização empregadas (ex: [\"MEV\", \"TEM\", \"DRX\", \"FTIR\", \"UV-Vis\"]). Se não encontrado, use [].\n",
    "    15. `propriedades_resultados_especificos`: (List of Strings) Propriedades chave investigadas e resultados quantitativos específicos (ex: [\"Eficiência de remoção de poluente: 95%\", \"Tamanho médio de partícula: 30 nm\", \"Resistividade: 0.01 Ohm.cm\"]). Se não encontrado, use [].\n",
    "    16. `aplicacao_primaria`: (String) Principal aplicação visada, testada ou proposta para o nanomaterial ou produto/tinta. Se não encontrado, use null.\n",
    "    17. `mecanismos_acao_explicados`: (String) Breve descrição de quaisquer mecanismos de ação, funcionamento ou fenômenos científicos explicados no texto. Se não encontrado, use null.\n",
    "    18. `vantagens_inovacao_declarada`: (String) Vantagens, benefícios ou inovações destacadas no estudo em relação a trabalhos anteriores ou tecnologias existentes. Se não encontrado, use null.\n",
    "    19. `desafios_limitacoes_futuro`: (String) Desafios, limitações identificadas no estudo, ou sugestões para trabalhos futuros. Se não encontrado, use null.\n",
    "\n",
    "    Texto para análise:\n",
    "    ---------------------\n",
    "    {text_content}\n",
    "    ---------------------\n",
    "\n",
    "    Retorne APENAS o objeto JSON. Certifique-se de que o JSON é válido.\n",
    "    Se um campo não puder ser preenchido com base no texto, use `null` para strings/inteiros, ou uma lista vazia `[]` para listas.\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "def extract_19_fields_with_gemini(text_content, retries=MAX_RETRIES, delay=RETRY_DELAY):\n",
    "    if not model_19_fields:\n",
    "        return {\"error\": \"Modelo Gemini não inicializado.\"}\n",
    "    if not text_content or pd.isna(text_content) or text_content.strip() == \"\":\n",
    "        return {\"error\": \"Conteúdo de texto de entrada vazio ou inválido.\"}\n",
    "\n",
    "    prompt = generate_extraction_prompt(text_content)\n",
    "    \n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = model_19_fields.generate_content(\n",
    "                prompt,\n",
    "                request_options={'timeout': REQUEST_TIMEOUT}\n",
    "            )\n",
    "            time.sleep(RATE_LIMIT_DELAY + random.uniform(0, 0.5)) # Respeitar limites da API\n",
    "\n",
    "            if response.parts:\n",
    "                content = response.text\n",
    "                # Gemini pode retornar o JSON dentro de ```json ... ``` ou diretamente\n",
    "                if content.startswith(\"```json\"):\n",
    "                    content = content[7:-3].strip()\n",
    "                elif content.startswith(\"```JSON\"):\n",
    "                    content = content[7:-3].strip()\n",
    "                elif content.startswith(\"```\") and content.endswith(\"```\"):\n",
    "                     content = content[3:-3].strip()\n",
    "                \n",
    "                try:\n",
    "                    parsed_json = json.loads(content)\n",
    "                    # Validar se todos os 19 campos esperados estão presentes (mesmo que null/vazio)\n",
    "                    missing_keys = [key for key in FIELDS_TO_EXTRACT if key not in parsed_json]\n",
    "                    if missing_keys:\n",
    "                        # Adicionar chaves ausentes com valor None para manter a estrutura consistente\n",
    "                        for key in missing_keys:\n",
    "                            parsed_json[key] = None # Ou [] se for uma lista, mas None é mais simples aqui\n",
    "                        # print(f\"Aviso: Chaves ausentes na resposta JSON foram preenchidas com None: {missing_keys}\")\n",
    "                    return parsed_json\n",
    "                except json.JSONDecodeError as e:\n",
    "                    error_msg = f\"Erro ao decodificar JSON (tentativa {attempt + 1}/{retries}): {e}. Resposta: {content[:500]}...\"\n",
    "                    print(error_msg)\n",
    "                    if attempt == retries - 1:\n",
    "                        return {\"error\": error_msg, \"raw_response\": content}\n",
    "                except Exception as e:\n",
    "                    error_msg = f\"Erro inesperado ao processar resposta (tentativa {attempt + 1}/{retries}): {e}. Resposta: {content[:500]}...\"\n",
    "                    print(error_msg)\n",
    "                    if attempt == retries - 1:\n",
    "                        return {\"error\": error_msg, \"raw_response\": content}\n",
    "            else: # Resposta vazia ou bloqueada\n",
    "                block_reason = response.prompt_feedback.block_reason if response.prompt_feedback else \"Não especificado\"\n",
    "                error_msg = f\"Resposta vazia ou bloqueada (tentativa {attempt + 1}/{retries}). Motivo: {block_reason}\"\n",
    "                print(error_msg)\n",
    "                if attempt == retries - 1:\n",
    "                    return {\"error\": error_msg}\n",
    "\n",
    "        except genai.types.generation_types.BlockedPromptException as e:\n",
    "            error_msg = f\"Prompt bloqueado pela API (tentativa {attempt + 1}/{retries}): {e}\"\n",
    "            print(error_msg)\n",
    "            if attempt == retries - 1:\n",
    "                return {\"error\": error_msg}\n",
    "        except Exception as e: # Outros erros (timeout, API key, rate limit)\n",
    "            error_msg = f\"Erro na chamada da API (tentativa {attempt + 1}/{retries}): {e}\"\n",
    "            print(error_msg)\n",
    "            if \"API key not valid\" in str(e):\n",
    "                 return {\"error\": \"API Key inválida. Verifique sua chave.\"}\n",
    "            if \"429\" in str(e) or \"rate limit\" in str(e).lower():\n",
    "                print(f\"Rate limit atingido. Aguardando {delay * 2}s.\")\n",
    "                time.sleep(delay * 2)\n",
    "                delay *= 1.5 \n",
    "            elif attempt == retries - 1:\n",
    "                return {\"error\": error_msg}\n",
    "        \n",
    "        time.sleep(delay)\n",
    "    \n",
    "    return {\"error\": f\"Falha ao obter extração da API Gemini após {retries} tentativas.\"}\n",
    "\n",
    "# Teste rápido da função de prompt (opcional)\n",
    "# sample_text = \"O título é 'Super Nanotintas' por Dr. X em 2023. Usa TiO2 para UV block. Publicado no Journal of Coatings.\"\n",
    "# print(generate_extraction_prompt(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d695d9",
   "metadata": {},
   "source": [
    "## 3. Processamento em Lote para Extração dos 19 Campos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a085d426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando todos os 9046 registros para extração dos 19 campos.\n",
      "Iniciando extração dos 19 campos para 9046 registros...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fd5e24232564c91aaecf56424a07ef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9046 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro na chamada da API (tentativa 1/3): 404 models/gemini-2.0-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Erro na chamada da API (tentativa 2/3): 404 models/gemini-2.0-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Erro na chamada da API (tentativa 2/3): 404 models/gemini-2.0-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Erro na chamada da API (tentativa 3/3): 404 models/gemini-2.0-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Erro na chamada da API (tentativa 3/3): 404 models/gemini-2.0-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Erro na chamada da API (tentativa 1/3): 404 models/gemini-2.0-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Erro na chamada da API (tentativa 1/3): 404 models/gemini-2.0-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Erro na chamada da API (tentativa 2/3): 404 models/gemini-2.0-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Erro na chamada da API (tentativa 2/3): 404 models/gemini-2.0-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Erro na chamada da API (tentativa 3/3): 404 models/gemini-2.0-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Erro na chamada da API (tentativa 3/3): 404 models/gemini-2.0-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Erro na chamada da API (tentativa 1/3): 404 models/gemini-2.0-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Erro na chamada da API (tentativa 1/3): 404 models/gemini-2.0-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Erro na chamada da API (tentativa 2/3): 404 models/gemini-2.0-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Erro na chamada da API (tentativa 2/3): 404 models/gemini-2.0-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Erro na chamada da API (tentativa 3/3): 404 models/gemini-2.0-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Erro na chamada da API (tentativa 3/3): 404 models/gemini-2.0-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Erro na chamada da API (tentativa 1/3): 404 models/gemini-2.0-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "Erro na chamada da API (tentativa 1/3): 404 models/gemini-2.0-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIniciando extração dos 19 campos para \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df_to_process_19_fields)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m registros...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Usar a coluna 'Abstract_to_analyze' que deve ter sido preparada no notebook 04 ou no início deste.\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Se essa coluna estiver vazia ou for de má qualidade, a extração será ruim.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m results_19_fields = \u001b[43mdf_to_process_19_fields\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mAbstract_to_analyze\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprogress_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextract_19_fields_with_gemini\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Converter os resultados (que são dicts ou dicts de erro) para um DataFrame\u001b[39;00m\n\u001b[32m     41\u001b[39m df_results_19_fields = pd.json_normalize(results_19_fields)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Modelos/modeloCenanoInk/venv/lib/python3.12/site-packages/tqdm/std.py:917\u001b[39m, in \u001b[36mtqdm.pandas.<locals>.inner_generator.<locals>.inner\u001b[39m\u001b[34m(df, func, *args, **kwargs)\u001b[39m\n\u001b[32m    914\u001b[39m \u001b[38;5;66;03m# Apply the provided function (in **kwargs)\u001b[39;00m\n\u001b[32m    915\u001b[39m \u001b[38;5;66;03m# on the df using our wrapper (which provides bar updating)\u001b[39;00m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m917\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_function\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    919\u001b[39m     t.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Modelos/modeloCenanoInk/venv/lib/python3.12/site-packages/pandas/core/series.py:4935\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4800\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4801\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4802\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4807\u001b[39m     **kwargs,\n\u001b[32m   4808\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4809\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4810\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4811\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4926\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4927\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4928\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4929\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4930\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4931\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4932\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4933\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4934\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4935\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Modelos/modeloCenanoInk/venv/lib/python3.12/site-packages/pandas/core/apply.py:1422\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1421\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Modelos/modeloCenanoInk/venv/lib/python3.12/site-packages/pandas/core/apply.py:1502\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1499\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1500\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1501\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1502\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1507\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1508\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Modelos/modeloCenanoInk/venv/lib/python3.12/site-packages/pandas/core/base.py:925\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Modelos/modeloCenanoInk/venv/lib/python3.12/site-packages/pandas/core/algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/lib.pyx:2999\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Modelos/modeloCenanoInk/venv/lib/python3.12/site-packages/tqdm/std.py:912\u001b[39m, in \u001b[36mtqdm.pandas.<locals>.inner_generator.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    906\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    907\u001b[39m     \u001b[38;5;66;03m# update tbar correctly\u001b[39;00m\n\u001b[32m    908\u001b[39m     \u001b[38;5;66;03m# it seems `pandas apply` calls `func` twice\u001b[39;00m\n\u001b[32m    909\u001b[39m     \u001b[38;5;66;03m# on the first column/row to decide whether it can\u001b[39;00m\n\u001b[32m    910\u001b[39m     \u001b[38;5;66;03m# take a fast or slow code path; so stop when t.total==t.n\u001b[39;00m\n\u001b[32m    911\u001b[39m     t.update(n=\u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t.total \u001b[38;5;129;01mor\u001b[39;00m t.n < t.total \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m912\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 110\u001b[39m, in \u001b[36mextract_19_fields_with_gemini\u001b[39m\u001b[34m(text_content, retries, delay)\u001b[39m\n\u001b[32m    107\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m attempt == retries - \u001b[32m1\u001b[39m:\n\u001b[32m    108\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33merror\u001b[39m\u001b[33m\"\u001b[39m: error_msg}\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33merror\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFalha ao obter extração da API Gemini após \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mretries\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m tentativas.\u001b[39m\u001b[33m\"\u001b[39m}\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Limitar o número de registros para teste inicial, se desejado\n",
    "NUM_SAMPLES_TO_PROCESS_19_FIELDS = config_sistema.get('NUM_SAMPLES_19_FIELDS_EXTRACTION', len(df_input)) # Processar todos por padrão\n",
    "\n",
    "if df_input.empty or not api_key or not model_19_fields:\n",
    "    print(\"Pré-requisitos não atendidos: DataFrame de entrada vazio, API Key não configurada ou modelo Gemini não inicializado. Pulando extração.\")\n",
    "    df_extracted_19_fields = pd.DataFrame() # DataFrame vazio\n",
    "    # Criar colunas vazias para consistência do schema se o processamento for pulado\n",
    "    empty_data = {key: [] for key in FIELDS_TO_EXTRACT}\n",
    "    empty_data['error'] = []\n",
    "    df_results_19_fields = pd.DataFrame(empty_data)\n",
    "\n",
    "else:\n",
    "    if 'Abstract_to_analyze' not in df_input.columns:\n",
    "        print(\"Coluna 'Abstract_to_analyze' é crucial e não foi encontrada ou criada. A extração será de baixa qualidade.\")\n",
    "        # Tentar criar a coluna usando os nomes corretos das colunas\n",
    "        if 'Article Title' in df_input.columns and 'Abstract' in df_input.columns:\n",
    "            df_input['Abstract_to_analyze'] = df_input['Article Title'].fillna('') + \" \" + df_input['Abstract'].fillna('')\n",
    "            print(\"Coluna 'Abstract_to_analyze' criada a partir da concatenação de 'Article Title' e 'Abstract'.\")\n",
    "        elif 'TI' in df_input.columns and 'AB' in df_input.columns:\n",
    "            df_input['Abstract_to_analyze'] = df_input['TI'].fillna('') + \" \" + df_input['AB'].fillna('')\n",
    "            print(\"Coluna 'Abstract_to_analyze' criada a partir da concatenação de 'TI' e 'AB'.\")\n",
    "        else:\n",
    "            df_input['Abstract_to_analyze'] = \"\"\n",
    "\n",
    "    df_to_process_19_fields = df_input.copy()\n",
    "    if NUM_SAMPLES_TO_PROCESS_19_FIELDS < len(df_input):\n",
    "        print(f\"Processando uma amostra de {NUM_SAMPLES_TO_PROCESS_19_FIELDS} registros para extração dos 19 campos.\")\n",
    "        df_to_process_19_fields = df_input.sample(n=NUM_SAMPLES_TO_PROCESS_19_FIELDS, random_state=config_sistema.get('RANDOM_SEED', 42))\n",
    "    else:\n",
    "        print(f\"Processando todos os {len(df_input)} registros para extração dos 19 campos.\")\n",
    "        # df_to_process_19_fields já é df_input.copy()\n",
    "\n",
    "    # Aplicar a função de extração\n",
    "    print(f\"Iniciando extração dos 19 campos para {len(df_to_process_19_fields)} registros...\")\n",
    "    \n",
    "    # Usar a coluna 'Abstract_to_analyze' que deve ter sido preparada no notebook 04 ou no início deste.\n",
    "    # Se essa coluna estiver vazia ou for de má qualidade, a extração será ruim.\n",
    "    results_19_fields = df_to_process_19_fields['Abstract_to_analyze'].progress_apply(extract_19_fields_with_gemini)\n",
    "    \n",
    "    # Converter os resultados (que são dicts ou dicts de erro) para um DataFrame\n",
    "    df_results_19_fields = pd.json_normalize(results_19_fields)\n",
    "    \n",
    "    # Garantir que todas as 19 colunas de campos + coluna de erro existam, preenchendo com None se ausentes\n",
    "    for col in FIELDS_TO_EXTRACT + ['error', 'raw_response']: # Adiciona raw_response para debug\n",
    "        if col not in df_results_19_fields.columns:\n",
    "            df_results_19_fields[col] = None\n",
    "            \n",
    "    # Juntar os resultados da extração com o DataFrame original (ou subset processado)\n",
    "    # Certifique-se de que os índices estão alinhados para a junção\n",
    "    df_extracted_19_fields = df_to_process_19_fields.join(df_results_19_fields.set_index(df_to_process_19_fields.index))\n",
    "    \n",
    "    print(\"\\nExtração dos 19 campos concluída.\")\n",
    "    print(f\"Colunas no DataFrame resultante: {df_extracted_19_fields.columns.tolist()}\")\n",
    "    print(df_extracted_19_fields.head())\n",
    "\n",
    "    # Verificar a contagem de erros\n",
    "    if 'error' in df_extracted_19_fields.columns:\n",
    "        num_errors = df_extracted_19_fields['error'].notna().sum()\n",
    "        print(f\"Número de erros durante a extração: {num_errors} de {len(df_extracted_19_fields)}\")\n",
    "        if num_errors > 0:\n",
    "            print(\"Distribuição dos erros:\")\n",
    "            print(df_extracted_19_fields['error'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9fbe6d",
   "metadata": {},
   "source": [
    "## 4. Salvamento dos Resultados Detalhados e Sumarização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173f24d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset com 19 campos extraídos salvo em: ../data/processed/dataset_with_19_fields.csv\n",
      "Sumário da extração dos 19 campos salvo em: ../data/processed/extraction_19_fields_summary.json\n",
      "\n",
      "Sumário da Extração Detalhada:\n",
      "{\n",
      "  \"total_records_attempted_extraction\": 9046,\n",
      "  \"successful_full_json_extractions\": 0,\n",
      "  \"failed_full_json_extractions\": 9046,\n",
      "  \"error_distribution\": {\n",
      "    \"Conteúdo de texto de entrada vazio ou inválido.\": 9046\n",
      "  },\n",
      "  \"field_fill_statistics\": {\n",
      "    \"id_documento\": {\n",
      "      \"filled\": 0,\n",
      "      \"total\": 9046,\n",
      "      \"fill_rate_percent\": 0.0\n",
      "    },\n",
      "    \"titulo\": {\n",
      "      \"filled\": 0,\n",
      "      \"total\": 9046,\n",
      "      \"fill_rate_percent\": 0.0\n",
      "    },\n",
      "    \"ano\": {\n",
      "      \"filled\": 0,\n",
      "      \"total\": 9046,\n",
      "      \"fill_rate_percent\": 0.0\n",
      "    },\n",
      "    \"tipo_documento\": {\n",
      "      \"filled\": 0,\n",
      "      \"total\": 9046,\n",
      "      \"fill_rate_percent\": 0.0\n",
      "    },\n",
      "    \"autores\": {\n",
      "      \"filled\": 0,\n",
      "      \"total\": 9046,\n",
      "      \"fill_rate_percent\": 0.0\n",
      "    },\n",
      "    \"afiliacoes_autores\": {\n",
      "      \"filled\": 0,\n",
      "      \"total\": 9046,\n",
      "      \"fill_rate_percent\": 0.0\n",
      "    },\n",
      "    \"revista_patente_info\": {\n",
      "      \"filled\": 0,\n",
      "      \"total\": 9046,\n",
      "      \"fill_rate_percent\": 0.0\n",
      "    },\n",
      "    \"pais_origem\": {\n",
      "      \"filled\": 0,\n",
      "      \"total\": 9046,\n",
      "      \"fill_rate_percent\": 0.0\n",
      "    },\n",
      "    \"palavras_chave_documento\": {\n",
      "      \"filled\": 0,\n",
      "      \"total\": 9046,\n",
      "      \"fill_rate_percent\": 0.0\n",
      "    },\n",
      "    \"nanomateriais_citados\": {\n",
      "      \"filled\": 0,\n",
      "      \"total\": 9046,\n",
      "      \"fill_rate_percent\": 0.0\n",
      "    },\n",
      "    \"metodo_sintese_fabricacao_detalhado\": {\n",
      "      \"filled\": 0,\n",
      "      \"total\": 9046,\n",
      "      \"fill_rate_percent\": 0.0\n",
      "    },\n",
      "    \"precursores_reagentes\": {\n",
      "      \"filled\": 0,\n",
      "      \"total\": 9046,\n",
      "      \"fill_rate_percent\": 0.0\n",
      "    },\n",
      "    \"parametros_processo\": {\n",
      "      \"filled\": 0,\n",
      "      \"total\": 9046,\n",
      "      \"fill_rate_percent\": 0.0\n",
      "    },\n",
      "    \"tecnicas_caracterizacao\": {\n",
      "      \"filled\": 0,\n",
      "      \"total\": 9046,\n",
      "      \"fill_rate_percent\": 0.0\n",
      "    },\n",
      "    \"propriedades_resultados_especificos\": {\n",
      "      \"filled\": 0,\n",
      "      \"total\": 9046,\n",
      "      \"fill_rate_percent\": 0.0\n",
      "    },\n",
      "    \"aplicacao_primaria\": {\n",
      "      \"filled\": 0,\n",
      "      \"total\": 9046,\n",
      "      \"fill_rate_percent\": 0.0\n",
      "    },\n",
      "    \"mecanismos_acao_explicados\": {\n",
      "      \"filled\": 0,\n",
      "      \"total\": 9046,\n",
      "      \"fill_rate_percent\": 0.0\n",
      "    },\n",
      "    \"vantagens_inovacao_declarada\": {\n",
      "      \"filled\": 0,\n",
      "      \"total\": 9046,\n",
      "      \"fill_rate_percent\": 0.0\n",
      "    },\n",
      "    \"desafios_limitacoes_futuro\": {\n",
      "      \"filled\": 0,\n",
      "      \"total\": 9046,\n",
      "      \"fill_rate_percent\": 0.0\n",
      "    }\n",
      "  },\n",
      "  \"output_file_19_fields\": \"../data/processed/dataset_with_19_fields.csv\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "extraction_summary_19_fields = {}\n",
    "\n",
    "if not df_extracted_19_fields.empty:\n",
    "    # Salvar o DataFrame com os 19 campos extraídos\n",
    "    df_extracted_19_fields.to_csv(EXTRACTED_19_FIELDS_OUTPUT_FILE, index=False, encoding='utf-8')\n",
    "    print(f\"Dataset com 19 campos extraídos salvo em: {EXTRACTED_19_FIELDS_OUTPUT_FILE}\")\n",
    "\n",
    "    # Gerar estatísticas/sumário da extração\n",
    "    total_records_attempted = len(df_extracted_19_fields)\n",
    "    successful_extractions = 0\n",
    "    failed_extractions = 0\n",
    "    error_messages_distribution = {}\n",
    "\n",
    "    if 'error' in df_extracted_19_fields.columns:\n",
    "        successful_extractions = df_extracted_19_fields['error'].isna().sum()\n",
    "        failed_extractions = df_extracted_19_fields['error'].notna().sum()\n",
    "        if failed_extractions > 0:\n",
    "            error_messages_distribution = df_extracted_19_fields.loc[df_extracted_19_fields['error'].notna(), 'error'].value_counts().to_dict()\n",
    "    else: # Caso a coluna 'error' não exista (improvável com a lógica acima, mas para segurança)\n",
    "        if api_key and model_19_fields and not df_input.empty : # Se a extração deveria ter rodado\n",
    "             successful_extractions = total_records_attempted \n",
    "    \n",
    "    # Contagem de preenchimento para cada um dos 19 campos\n",
    "    field_fill_counts = {}\n",
    "    for field in FIELDS_TO_EXTRACT:\n",
    "        if field in df_extracted_19_fields.columns:\n",
    "            # Conta não nulos e não vazios (para listas/strings)\n",
    "            non_empty_count = df_extracted_19_fields[field].apply(lambda x: False if (x is None or (isinstance(x, (str, list)) and not x)) else True).sum()\n",
    "            field_fill_counts[field] = {\n",
    "                \"filled\": int(non_empty_count),\n",
    "                \"total\": int(total_records_attempted),\n",
    "                \"fill_rate_percent\": round((non_empty_count / total_records_attempted) * 100, 2) if total_records_attempted > 0 else 0\n",
    "            }\n",
    "        else:\n",
    "            field_fill_counts[field] = {\"filled\": 0, \"total\": int(total_records_attempted), \"fill_rate_percent\": 0}\n",
    "\n",
    "\n",
    "    extraction_summary_19_fields = {\n",
    "        \"total_records_attempted_extraction\": total_records_attempted,\n",
    "        \"successful_full_json_extractions\": int(successful_extractions), # JSON decodificado sem erro fatal\n",
    "        \"failed_full_json_extractions\": int(failed_extractions),\n",
    "        \"error_distribution\": error_messages_distribution,\n",
    "        \"field_fill_statistics\": field_fill_counts,\n",
    "        \"output_file_19_fields\": str(EXTRACTED_19_FIELDS_OUTPUT_FILE)\n",
    "    }\n",
    "\n",
    "    with open(EXTRACTED_19_FIELDS_SUMMARY_FILE, 'w', encoding='utf-8') as f:\n",
    "        json.dump(extraction_summary_19_fields, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"Sumário da extração dos 19 campos salvo em: {EXTRACTED_19_FIELDS_SUMMARY_FILE}\")\n",
    "    print(\"\\nSumário da Extração Detalhada:\")\n",
    "    print(json.dumps(extraction_summary_19_fields, indent=2, ensure_ascii=False))\n",
    "else:\n",
    "    print(\"Nenhum resultado da extração dos 19 campos para salvar ou sumarizar.\")\n",
    "    extraction_summary_19_fields = {\n",
    "        \"total_records_attempted_extraction\": 0,\n",
    "        \"successful_full_json_extractions\": 0,\n",
    "        \"failed_full_json_extractions\": 0,\n",
    "        \"error_distribution\": {},\n",
    "        \"field_fill_statistics\": {field: {\"filled\": 0, \"total\": 0, \"fill_rate_percent\": 0} for field in FIELDS_TO_EXTRACT},\n",
    "        \"message\": \"Extração dos 19 campos não executada ou não produziu resultados.\"\n",
    "    }\n",
    "    with open(EXTRACTED_19_FIELDS_SUMMARY_FILE, 'w', encoding='utf-8') as f:\n",
    "        json.dump(extraction_summary_19_fields, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"Arquivo de sumário da extração (vazio) salvo em: {EXTRACTED_19_FIELDS_SUMMARY_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f435bb",
   "metadata": {},
   "source": [
    "## 5. Atualização do Arquivo de Configuração do Sistema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0042b1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo de configuração 'config_sistema.json' atualizado com o status da extração dos 19 campos.\n"
     ]
    }
   ],
   "source": [
    "if config_path.exists():\n",
    "    with open(config_path, 'r', encoding='utf-8') as f:\n",
    "        config_sistema = json.load(f)\n",
    "else:\n",
    "    config_sistema = {} # Deveria existir, mas como fallback\n",
    "\n",
    "config_sistema['EXTRACTION_19_FIELDS_COMPLETED'] = True\n",
    "config_sistema['EXTRACTION_19_FIELDS_OUTPUT_FILE'] = str(EXTRACTED_19_FIELDS_OUTPUT_FILE)\n",
    "config_sistema['EXTRACTION_19_FIELDS_SUMMARY_FILE'] = str(EXTRACTED_19_FIELDS_SUMMARY_FILE)\n",
    "config_sistema['TIMESTAMP_EXTRACTION_19_FIELDS'] = time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Adicionar alguns parâmetros usados nesta etapa para rastreabilidade\n",
    "config_sistema['PARAMS_19_FIELDS_EXTRACTION'] = {\n",
    "    'GEMINI_MODEL_NAME': GEMINI_MODEL_NAME,\n",
    "    'MAX_RETRIES': MAX_RETRIES,\n",
    "    'RETRY_DELAY': RETRY_DELAY, # O valor inicial, pode aumentar dinamicamente\n",
    "    'REQUEST_TIMEOUT': REQUEST_TIMEOUT,\n",
    "    'RATE_LIMIT_DELAY': RATE_LIMIT_DELAY,\n",
    "    'NUM_SAMPLES_PROCESSED': NUM_SAMPLES_TO_PROCESS_19_FIELDS if not df_extracted_19_fields.empty else 0,\n",
    "    'FIELDS_TARGETED': FIELDS_TO_EXTRACT\n",
    "}\n",
    "\n",
    "\n",
    "with open(config_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(config_sistema, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Arquivo de configuração '{config_path}' atualizado com o status da extração dos 19 campos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378a18ca",
   "metadata": {},
   "source": [
    "## Próximos Passos\n",
    "\n",
    "Com a extração detalhada dos 19 campos concluída, o dataset está significativamente enriquecido. Os próximos passos podem incluir:\n",
    "\n",
    "1.  **Validação e Limpeza dos Dados Extraídos:** Uma análise mais aprofundada da qualidade dos dados extraídos, seguida de limpeza e normalização, se necessário.\n",
    "2.  **Análise Exploratória dos Dados (EDA):** Investigar as distribuições, correlações e padrões nos 19 campos.\n",
    "3.  **Engenharia de Features:** Criação de novas features a partir dos campos extraídos para modelos de machine learning.\n",
    "4.  **Desenvolvimento de Modelos Preditivos:** Utilizar os dados para treinar modelos que possam prever, por exemplo, propriedades de tintas ou adequação de nanomateriais.\n",
    "5.  **Construção de um Sistema de Busca e Recomendação:** Usar os dados estruturados para permitir buscas complexas e recomendar artigos/patentes ou formulações.\n",
    "6.  **Visualização e Dashboarding:** Criar dashboards para explorar os insights gerados."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
