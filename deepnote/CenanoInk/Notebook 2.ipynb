{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "bd158450",
        "execution_start": 1747673257074,
        "execution_millis": 324,
        "execution_context_id": "2b499af1-f72b-43e4-b5e8-d3bcb92acc6c",
        "cell_id": "2f66a52c14ac44a4b552345976ecbb3d",
        "deepnote_cell_type": "code"
      },
      "source": "# -*- coding: utf-8 -*-\nimport pandas as pd\nimport requests\nimport os\nimport time\n# import re # Descomente se precisar de regex mais complexas no futuro\n\n# --- Configurações do Usuário ---\n\n# Opção 1: Definir a API Key diretamente (recomendado se não estiver usando Colab/Drive)\nAPI_KEY = \"XfCz461qd3BMH8NDt0GRWLjgUycbOs5k\" # SUBSTITUA PELA SUA CHAVE DE API REAL\n\n# Opção 2: Ler a API Key do Google Drive (descomente as linhas abaixo se estiver usando Colab)\n# from google.colab import drive\n# drive.mount('/content/drive')\n# API_KEY_FILE_PATH = 'drive/MyDrive/apikey.txt' # Ajuste o caminho conforme necessário\n# try:\n#     with open(API_KEY_FILE_PATH) as keyfile:\n#         API_KEY = keyfile.read().strip()\n#     if not API_KEY:\n#         print(f\"ERRO: Arquivo de API Key '{API_KEY_FILE_PATH}' está vazio.\")\n#         exit()\n#     print(\"API Key carregada do Google Drive.\")\n# except FileNotFoundError:\n#     print(f\"ERRO: Arquivo de API Key '{API_KEY_FILE_PATH}' não encontrado. Defina a API_KEY diretamente no script.\")\n#     exit()\n\nif API_KEY == \"SUA_API_KEY_AQUI\" or not API_KEY:\n    print(\"ERRO CRÍTICO: A API_KEY não foi definida. \"\n          \"Por favor, edite o script para adicionar sua chave ou configure o caminho para o arquivo no Google Drive.\")\n    exit()\n\n# Caminho para sua planilha CSV de entrada\nPLANILHA_ENTRADA = \"/work/CenaNoink/planilha30_31_32(Sheet1).csv\" # AJUSTE SE NECESSÁRIO\n\n# Nomes das colunas na planilha de entrada\nCOLUNA_DOI = \"DOI\"\nCOLUNA_TERMO_BUSCA_FULLTEXT = \"TERMO_BUSCA_FULLTEXT\" # IMPORTANTE: Adicione esta coluna à sua planilha\n\n# Arquivo CSV de saída para os resultados\nARQUIVO_SAIDA_RESULTADOS = \"resultados_busca_fulltext_core_v2_ref.csv\"\n\n# Configurações para o retry em caso de rate limit (erro 429)\nMAX_RETRIES_API = 3\nINITIAL_BACKOFF_SECONDS_API = 65 # Levemente aumentado\nMAX_BACKOFF_SECONDS_API = 360 # Levemente aumentado\n\n# Delay base entre o processamento de cada linha do CSV (em segundos)\nDELAY_ENTRE_LINHAS_CSV = 8\n\ndef executar_query_api_core(base_url, query_params, headers, max_retries, initial_backoff, max_backoff):\n    \"\"\"\n    Executa uma chamada à API CORE com tratamento de erro, incluindo rate limits (429) e retries.\n    Retorna o objeto de resposta da API ou None em caso de falha persistente.\n    \"\"\"\n    current_retry = 0\n    backoff_time = initial_backoff\n\n    while current_retry <= max_retries:\n        try:\n            if current_retry > 0:\n                print(f\"Nova tentativa ({current_retry}/{max_retries}) para URL: {base_url} com params: {query_params.get('q')}\")\n            \n            response = requests.get(base_url, headers=headers, params=query_params, timeout=45)\n            \n            if response.status_code == 429: # Too Many Requests\n                print(f\"Erro 429 (Too Many Requests) para URL: {response.url}.\")\n                retry_after_header = response.headers.get(\"Retry-After\")\n                wait_time = backoff_time\n                if retry_after_header:\n                    try:\n                        wait_time = int(retry_after_header)\n                        print(f\"Cabeçalho Retry-After: {wait_time}s.\")\n                    except ValueError:\n                        print(f\"Retry-After ('{retry_after_header}') inválido. Usando backoff: {wait_time}s.\")\n                else:\n                    print(f\"Sem Retry-After. Usando backoff: {wait_time}s.\")\n                \n                wait_time = min(max(wait_time, 1), max_backoff) # Garante espera mínima e não excede máxima\n                \n                if current_retry < max_retries:\n                    print(f\"Aguardando {wait_time} segundos antes da próxima tentativa...\")\n                    time.sleep(wait_time)\n                    current_retry += 1\n                    backoff_time = min(backoff_time * 1.5, max_backoff) # Aumento para próxima tentativa\n                    continue # Tenta novamente\n                else:\n                    print(f\"Máximo de retries ({max_retries}) atingido após erro 429 para {response.url}.\")\n                    return response # Retorna a última resposta de erro 429\n\n            response.raise_for_status() # Levanta HTTPError para outros códigos 4xx/5xx\n            return response # Retorna a resposta bem-sucedida\n\n        except requests.exceptions.HTTPError as e:\n            print(f\"Erro HTTP {e.response.status_code} para URL {e.response.url}: {e.response.text[:300]}\")\n            # Para erros como 400, 401, 403, 404 (que não sejam 429), geralmente não adianta tentar novamente\n            # A menos que seja um erro de servidor (5xx) que pode ser temporário\n            if e.response.status_code >= 500 and current_retry < max_retries :\n                 print(f\"Erro de servidor detectado. Aguardando {backoff_time}s antes de tentar novamente...\")\n                 time.sleep(backoff_time)\n                 current_retry +=1\n                 backoff_time = min(backoff_time * 1.5, max_backoff)\n                 continue\n            return e.response # Retorna a resposta de erro\n        \n        except requests.exceptions.RequestException as e: # Erros de conexão, timeout, etc.\n            print(f\"Erro de requisição para URL {base_url} (Tentativa {current_retry + 1}): {e}\")\n            if current_retry < max_retries:\n                print(f\"Aguardando {backoff_time} segundos antes de tentar novamente...\")\n                time.sleep(backoff_time)\n                current_retry += 1\n                backoff_time = min(backoff_time * 1.5, max_backoff)\n            else:\n                print(f\"Máximo de retries ({max_retries}) atingido após erro de requisição.\")\n                return None # Falha persistente na requisição\n    \n    print(f\"Falha ao obter resposta da API para {base_url} após {max_retries +1} tentativas.\")\n    return None\n\n\ndef buscar_combinado_doi_fulltext(doi, termo_busca, entity_type=\"outputs\"):\n    \"\"\"\n    Busca na API CORE combinando DOI e um termo no fullText para uma entidade específica.\n    Retorna (True, entity_id, None) se encontrado, (False, None, error_message) caso contrário.\n    \"\"\"\n    if not doi or not termo_busca:\n        return False, None, \"DOI ou termo de busca está vazio.\"\n\n    termo_busca_query = termo_busca if (termo_busca.startswith('\"') and termo_busca.endswith('\"')) else f'\"{termo_busca}\"'\n    doi_query = doi if (doi.startswith('\"') and doi.endswith('\"')) else f'\"{doi}\"'\n    \n    query = f'doi:{doi_query} AND fullText:{termo_busca_query}'\n    base_search_url = f\"https://api.core.ac.uk/v3/search/{entity_type}\"\n    api_headers = {\"Authorization\": f\"Bearer {API_KEY}\"}\n    api_params = {\"q\": query, \"limit\": 1}\n\n    print(f\"Buscando em '{entity_type}' para DOI: {doi}, Termo FullText: {termo_busca} (Query: {query})\")\n    \n    response = executar_query_api_core(\n        base_search_url, \n        api_params, \n        api_headers, \n        MAX_RETRIES_API, \n        INITIAL_BACKOFF_SECONDS_API, \n        MAX_BACKOFF_SECONDS_API\n    )\n\n    if response is None: # Falha persistente na requisição\n        return False, None, \"Falha na comunicação com a API após múltiplas tentativas.\"\n    \n    if response.status_code == 200:\n        try:\n            data = response.json()\n            if data.get(\"total_hits\", 0) > 0 and data.get(\"results\") and len(data[\"results\"]) > 0:\n                entity_id = data[\"results\"][0].get(\"id\")\n                print(f\"Termo '{termo_busca}' ENCONTRADO no fullText de '{entity_type}' para DOI {doi}. ID: {entity_id}\")\n                return True, str(entity_id), None\n            else:\n                msg = f\"Termo '{termo_busca}' NÃO encontrado no fullText de '{entity_type}' para DOI {doi} (hits: {data.get('total_hits', 0)}).\"\n                return False, None, msg\n        except ValueError: # Erro ao decodificar JSON\n            msg = f\"Resposta da API não é JSON válido para {entity_type} DOI {doi}. Conteúdo: {response.text[:200]}\"\n            print(msg)\n            return False, None, msg\n    else: # Se a resposta não foi 200 (e não None)\n        msg = f\"Erro da API {response.status_code} ao buscar {entity_type} para DOI {doi}. Resposta: {response.text[:200]}\"\n        print(msg)\n        return False, None, msg\n\n\nif __name__ == \"__main__\":\n    try:\n        df = None\n        codificacoes = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']\n        for codificacao in codificacoes:\n            try:\n                df = pd.read_csv(PLANILHA_ENTRADA, encoding=codificacao)\n                print(f\"Arquivo CSV '{PLANILHA_ENTRADA}' lido com codificação: {codificacao}\")\n                break\n            except UnicodeDecodeError:\n                print(f\"Falha ao ler CSV com codificação: {codificacao}. Tentando próxima...\")\n            except FileNotFoundError:\n                print(f\"ERRO CRÍTICO: Arquivo de entrada não encontrado em: {PLANILHA_ENTRADA}\")\n                exit()\n\n        if df is None:\n            raise Exception(f\"Não foi possível decodificar o arquivo CSV: {PLANILHA_ENTRADA} com as codificações testadas.\")\n\n        if COLUNA_DOI not in df.columns:\n            print(f\"ERRO CRÍTICO: Coluna DOI '{COLUNA_DOI}' não encontrada na planilha.\")\n            exit()\n        if COLUNA_TERMO_BUSCA_FULLTEXT not in df.columns:\n            print(f\"ERRO CRÍTICO: Coluna de termo de busca '{COLUNA_TERMO_BUSCA_FULLTEXT}' não encontrada.\")\n            exit()\n\n        resultados_finais = []\n        total_linhas = len(df)\n        print(f\"\\nIniciando processamento de {total_linhas} linhas da planilha...\")\n\n        for index, row in df.iterrows():\n            doi_original = row[COLUNA_DOI]\n            termo_busca_original = row[COLUNA_TERMO_BUSCA_FULLTEXT]\n            \n            print(f\"\\n--- Processando Linha {index + 1}/{total_linhas} ---\")\n            print(f\"DOI: {doi_original}, Termo de Busca: {termo_busca_original}\")\n\n            resultado_linha = {\n                'doi': doi_original,\n                'termo_busca': termo_busca_original,\n                'encontrado_em_outputs': False,\n                'output_id_encontrado': None,\n                'erro_outputs': None,\n                'encontrado_em_works': False,\n                'work_id_encontrado': None,\n                'erro_works': None,\n                'status_geral': \"Não processado\"\n            }\n\n            if pd.isna(doi_original) or str(doi_original).strip() == \"\":\n                print(\"DOI vazio ou NaN. Pulando.\")\n                resultado_linha['status_geral'] = \"Falha: DOI vazio\"\n                resultado_linha['erro_outputs'] = \"DOI vazio\"\n                # Não precisa definir erro_works se o DOI já é inválido\n                resultados_finais.append(resultado_linha)\n                continue\n            \n            if pd.isna(termo_busca_original) or str(termo_busca_original).strip() == \"\":\n                print(\"Termo de busca vazio ou NaN. Pulando.\")\n                resultado_linha['status_geral'] = \"Falha: Termo de busca vazio\"\n                resultado_linha['erro_outputs'] = \"Termo de busca vazio\"\n                resultados_finais.append(resultado_linha)\n                continue\n\n            doi_str = str(doi_original).strip()\n            termo_busca_str = str(termo_busca_original).strip()\n\n            # Busca em Outputs\n            encontrado_output, output_id, erro_output = buscar_combinado_doi_fulltext(doi_str, termo_busca_str, \"outputs\")\n            resultado_linha['encontrado_em_outputs'] = encontrado_output\n            resultado_linha['output_id_encontrado'] = output_id\n            resultado_linha['erro_outputs'] = erro_output if not encontrado_output else None\n            \n            # Busca em Works\n            encontrado_work, work_id, erro_work = buscar_combinado_doi_fulltext(doi_str, termo_busca_str, \"works\")\n            resultado_linha['encontrado_em_works'] = encontrado_work\n            resultado_linha['work_id_encontrado'] = work_id\n            resultado_linha['erro_works'] = erro_work if not encontrado_work else None\n\n            # Define o status geral com base nos resultados das buscas\n            if encontrado_output and encontrado_work:\n                resultado_linha['status_geral'] = \"Encontrado em Outputs e Works\"\n            elif encontrado_output:\n                resultado_linha['status_geral'] = \"Encontrado em Outputs\"\n            elif encontrado_work:\n                resultado_linha['status_geral'] = \"Encontrado em Works\"\n            elif (erro_output and \"Falha na comunicação\" in erro_output) or \\\n                 (erro_work and \"Falha na comunicação\" in erro_work):\n                resultado_linha['status_geral'] = \"Erro: Falha de comunicação com API\"\n            elif (erro_output and \"API 429\" in erro_output) or \\\n                 (erro_work and \"API 429\" in erro_work) or \\\n                 (erro_output and \"Too Many Requests\" in erro_output) or \\\n                 (erro_work and \"Too Many Requests\" in erro_work) : # Verifica se a msg de erro contém indicação de rate limit\n                resultado_linha['status_geral'] = \"Erro: API Rate Limit Atingido\"\n            elif erro_output or erro_work: # Outros erros ou não encontrado\n                 resultado_linha['status_geral'] = \"Não encontrado ou Erro na busca\"\n            else: # Não encontrado em nenhum, sem erros fatais de API\n                resultado_linha['status_geral'] = \"Não encontrado\"\n\n            resultados_finais.append(resultado_linha)\n            \n            print(f\"Aguardando {DELAY_ENTRE_LINHAS_CSV} segundos antes da próxima linha...\")\n            time.sleep(DELAY_ENTRE_LINHAS_CSV)\n\n        if resultados_finais:\n            resultados_df = pd.DataFrame(resultados_finais)\n            colunas_ordenadas = [\n                'doi', 'termo_busca', \n                'encontrado_em_outputs', 'output_id_encontrado', 'erro_outputs',\n                'encontrado_em_works', 'work_id_encontrado', 'erro_works',\n                'status_geral'\n            ]\n            for col in colunas_ordenadas: # Garante que todas as colunas existam\n                if col not in resultados_df.columns:\n                    resultados_df[col] = None\n            resultados_df = resultados_df[colunas_ordenadas]\n\n            resultados_df.to_csv(ARQUIVO_SAIDA_RESULTADOS, index=False, encoding='utf-8')\n            print(f\"\\nResultados do processamento foram salvos em: {ARQUIVO_SAIDA_RESULTADOS}\")\n        else:\n            print(\"\\nNenhuma linha processada para salvar resultados.\")\n\n    except FileNotFoundError:\n        pass # Já tratado\n    except Exception as e:\n        print(f\"Ocorreu um erro geral e fatal no script: {e}\")\n        import traceback\n        traceback.print_exc()\n\n",
      "block_group": "2f66a52c14ac44a4b552345976ecbb3d",
      "execution_count": 1,
      "outputs": [
        {
          "name": "stdout",
          "text": "Falha ao ler CSV com codificação: utf-8. Tentando próxima...\nArquivo CSV '/work/CenaNoink/planilha30_31_32(Sheet1).csv' lido com codificação: latin-1\nERRO CRÍTICO: Coluna de termo de busca 'TERMO_BUSCA_FULLTEXT' não encontrada.\n\nIniciando processamento de 503 linhas da planilha...\nOcorreu um erro geral e fatal no script: 'TERMO_BUSCA_FULLTEXT'\nTraceback (most recent call last):\n  File \"/root/venv/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 3791, in get_loc\n    return self._engine.get_loc(casted_key)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"index.pyx\", line 152, in pandas._libs.index.IndexEngine.get_loc\n  File \"index.pyx\", line 181, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'TERMO_BUSCA_FULLTEXT'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_1055/3058179666.py\", line 202, in <module>\n    termo_busca_original = row[COLUNA_TERMO_BUSCA_FULLTEXT]\n                           ~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/venv/lib/python3.11/site-packages/pandas/core/series.py\", line 1040, in __getitem__\n    return self._get_value(key)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/root/venv/lib/python3.11/site-packages/pandas/core/series.py\", line 1156, in _get_value\n    loc = self.index.get_loc(label)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/venv/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 3798, in get_loc\n    raise KeyError(key) from err\nKeyError: 'TERMO_BUSCA_FULLTEXT'\n",
          "output_type": "stream"
        }
      ],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=d35fdc8b-8543-45dc-ae25-3ba609dd01b9' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "deepnote_notebook_id": "1ff648ab049b4924a11f444316284132"
  }
}