{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "8a69cc3e",
        "execution_start": 1748385554700,
        "execution_millis": 1,
        "execution_context_id": "3ed297e7-a555-43b2-887a-b28769104494",
        "cell_id": "af50352e6b94413fa6e6f6a0e24885a7",
        "deepnote_cell_type": "code"
      },
      "source": "# An√°lise de Artigos Cient√≠ficos com Gemini AI\n\n## Escopo: Nanorevestimentos e Tintas (Nanocoatings and Paints)\n\nEste notebook realiza an√°lise automatizada de artigos cient√≠ficos para identificar trabalhos relevantes ao escopo de nanorevestimentos e tintas usando a API do Google Gemini.\n\n### Funcionalidades:\n1. **Instala√ß√£o de depend√™ncias**\n2. **Configura√ß√£o da API Gemini**\n3. **Defini√ß√£o de palavras-chave espec√≠ficas**\n4. **Busca e filtro de artigos**\n5. **An√°lise individual de abstracts**\n6. **Processamento em lote com rate limiting**\n7. **Sistema de checkpoint para recupera√ß√£o**\n\n\n\n",
      "block_group": "dad07e73d3d94655ae63ca34efbf7b40",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (3739530835.py, line 5)",
          "traceback": [
            "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mEste notebook realiza an√°lise automatizada de artigos cient√≠ficos para identificar trabalhos relevantes ao escopo de nanorevestimentos e tintas usando a API do Google Gemini.\u001b[39m\n         ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
          ]
        }
      ],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "8b8c3a14",
        "execution_start": 1748385259622,
        "execution_millis": 8686,
        "execution_context_id": "3ed297e7-a555-43b2-887a-b28769104494",
        "deepnote_to_be_reexecuted": true,
        "cell_id": "12cee1f6511a485cb4027b62ad40d337",
        "deepnote_cell_type": "code"
      },
      "source": "!pip install google.generativeai",
      "block_group": "76137c9771f24cfd859b4d28171ea726",
      "execution_count": 7,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting google.generativeai\n  Downloading google_generativeai-0.8.5-py3-none-any.whl.metadata (3.9 kB)\nCollecting google-ai-generativelanguage==0.6.15 (from google.generativeai)\n  Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\nRequirement already satisfied: google-api-core in /tmp/python3.11/kernel-libs/lib/python3.11/site-packages (from google.generativeai) (2.19.2)\nCollecting google-api-python-client (from google.generativeai)\n  Downloading google_api_python_client-2.170.0-py3-none-any.whl.metadata (6.7 kB)\nRequirement already satisfied: google-auth>=2.15.0 in /root/venv/lib/python3.11/site-packages (from google.generativeai) (2.40.0)\nRequirement already satisfied: protobuf in /root/venv/lib/python3.11/site-packages (from google.generativeai) (4.25.7)\nRequirement already satisfied: pydantic in /root/venv/lib/python3.11/site-packages (from google.generativeai) (2.11.4)\nRequirement already satisfied: tqdm in /root/venv/lib/python3.11/site-packages (from google.generativeai) (4.67.1)\nRequirement already satisfied: typing-extensions in /root/venv/lib/python3.11/site-packages (from google.generativeai) (4.13.2)\nRequirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /tmp/python3.11/kernel-libs/lib/python3.11/site-packages (from google-ai-generativelanguage==0.6.15->google.generativeai) (1.24.0)\nRequirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /tmp/python3.11/kernel-libs/lib/python3.11/site-packages (from google-api-core->google.generativeai) (1.65.0)\nRequirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /root/venv/lib/python3.11/site-packages (from google-api-core->google.generativeai) (2.32.3)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /root/venv/lib/python3.11/site-packages (from google-auth>=2.15.0->google.generativeai) (5.5.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /root/venv/lib/python3.11/site-packages (from google-auth>=2.15.0->google.generativeai) (0.4.2)\nRequirement already satisfied: rsa<5,>=3.1.4 in /root/venv/lib/python3.11/site-packages (from google-auth>=2.15.0->google.generativeai) (4.9.1)\nCollecting httplib2<1.0.0,>=0.19.0 (from google-api-python-client->google.generativeai)\n  Downloading httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\nCollecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client->google.generativeai)\n  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\nCollecting uritemplate<5,>=3.0.1 (from google-api-python-client->google.generativeai)\n  Downloading uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\nRequirement already satisfied: annotated-types>=0.6.0 in /root/venv/lib/python3.11/site-packages (from pydantic->google.generativeai) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /root/venv/lib/python3.11/site-packages (from pydantic->google.generativeai) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /root/venv/lib/python3.11/site-packages (from pydantic->google.generativeai) (0.4.0)\nRequirement already satisfied: grpcio<2.0dev,>=1.33.2 in /root/venv/lib/python3.11/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google.generativeai) (1.71.0)\nRequirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /tmp/python3.11/kernel-libs/lib/python3.11/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google.generativeai) (1.62.3)\nRequirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /root/venv/lib/python3.11/site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google.generativeai) (3.2.3)\nRequirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /root/venv/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google.generativeai) (0.6.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /root/venv/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google.generativeai) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /root/venv/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google.generativeai) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /root/venv/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google.generativeai) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /root/venv/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google.generativeai) (2025.4.26)\nDownloading google_generativeai-0.8.5-py3-none-any.whl (155 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m155.4/155.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m109.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading google_api_python_client-2.170.0-py3-none-any.whl (13.5 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\nDownloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m96.9/96.9 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\nInstalling collected packages: uritemplate, httplib2, google-auth-httplib2, google-api-python-client, google-ai-generativelanguage, google.generativeai\nSuccessfully installed google-ai-generativelanguage-0.6.15 google-api-python-client-2.170.0 google-auth-httplib2-0.2.0 google.generativeai-0.8.5 httplib2-0.22.0 uritemplate-4.1.1\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
          "output_type": "stream"
        }
      ],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "4c28bc3d",
        "execution_start": 1748385576443,
        "execution_millis": 5834,
        "execution_context_id": "3ed297e7-a555-43b2-887a-b28769104494",
        "cell_id": "27e77a47bffd4a1fbfc0da18e5bfd692",
        "deepnote_cell_type": "code"
      },
      "source": "# Instala√ß√£o das depend√™ncias necess√°rias\n!pip install google-generativeai pandas",
      "block_group": "5f0a84a5eea24314ab4e2d2ce93ac137",
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: google-generativeai in /root/venv/lib/python3.11/site-packages (0.8.5)\nRequirement already satisfied: pandas in /root/venv/lib/python3.11/site-packages (2.1.4)\nRequirement already satisfied: google-ai-generativelanguage==0.6.15 in /root/venv/lib/python3.11/site-packages (from google-generativeai) (0.6.15)\nRequirement already satisfied: google-api-core in /tmp/python3.11/kernel-libs/lib/python3.11/site-packages (from google-generativeai) (2.19.2)\nRequirement already satisfied: google-api-python-client in /root/venv/lib/python3.11/site-packages (from google-generativeai) (2.170.0)\nRequirement already satisfied: google-auth>=2.15.0 in /root/venv/lib/python3.11/site-packages (from google-generativeai) (2.40.0)\nRequirement already satisfied: protobuf in /root/venv/lib/python3.11/site-packages (from google-generativeai) (4.25.7)\nRequirement already satisfied: pydantic in /root/venv/lib/python3.11/site-packages (from google-generativeai) (2.11.4)\nRequirement already satisfied: tqdm in /root/venv/lib/python3.11/site-packages (from google-generativeai) (4.67.1)\nRequirement already satisfied: typing-extensions in /root/venv/lib/python3.11/site-packages (from google-generativeai) (4.13.2)\nRequirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /tmp/python3.11/kernel-libs/lib/python3.11/site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.24.0)\nRequirement already satisfied: numpy<2,>=1.23.2 in /root/venv/lib/python3.11/site-packages (from pandas) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /root/venv/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /root/venv/lib/python3.11/site-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.1 in /root/venv/lib/python3.11/site-packages (from pandas) (2025.2)\nRequirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /tmp/python3.11/kernel-libs/lib/python3.11/site-packages (from google-api-core->google-generativeai) (1.65.0)\nRequirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /root/venv/lib/python3.11/site-packages (from google-api-core->google-generativeai) (2.32.3)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /root/venv/lib/python3.11/site-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /root/venv/lib/python3.11/site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\nRequirement already satisfied: rsa<5,>=3.1.4 in /root/venv/lib/python3.11/site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\nRequirement already satisfied: six>=1.5 in /root/venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: httplib2<1.0.0,>=0.19.0 in /root/venv/lib/python3.11/site-packages (from google-api-python-client->google-generativeai) (0.22.0)\nRequirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /root/venv/lib/python3.11/site-packages (from google-api-python-client->google-generativeai) (0.2.0)\nRequirement already satisfied: uritemplate<5,>=3.0.1 in /root/venv/lib/python3.11/site-packages (from google-api-python-client->google-generativeai) (4.1.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /root/venv/lib/python3.11/site-packages (from pydantic->google-generativeai) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /root/venv/lib/python3.11/site-packages (from pydantic->google-generativeai) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /root/venv/lib/python3.11/site-packages (from pydantic->google-generativeai) (0.4.0)\nRequirement already satisfied: grpcio<2.0dev,>=1.33.2 in /root/venv/lib/python3.11/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\nRequirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /tmp/python3.11/kernel-libs/lib/python3.11/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.62.3)\nRequirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /root/venv/lib/python3.11/site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\nRequirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /root/venv/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /root/venv/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /root/venv/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /root/venv/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /root/venv/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2025.4.26)\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
          "output_type": "stream"
        }
      ],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "f3a90055",
        "execution_start": 1748385584342,
        "execution_millis": 496,
        "execution_context_id": "3ed297e7-a555-43b2-887a-b28769104494",
        "cell_id": "048864eacdc74d8c8ad8c3f1dbbe69a9",
        "deepnote_cell_type": "code"
      },
      "source": "# Imports necess√°rios\nimport pandas as pd\nimport google.generativeai as genai\nimport time\nimport random\nfrom typing import List, Optional\n\n# Configura√ß√£o da API do Gemini\ngenai.configure(api_key='AIzaSyAcQicBEj8s2NvvCBORlQCgPUrPchR-vCQ')\n\nprint(\"‚úÖ Depend√™ncias carregadas e API configurada\")",
      "block_group": "3ae905ad99674f56a78b5b88eb61e67f",
      "execution_count": 7,
      "outputs": [
        {
          "name": "stdout",
          "text": "‚úÖ Depend√™ncias carregadas e API configurada\n/root/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n",
          "output_type": "stream"
        }
      ],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "3b8a85f1f6e04c60a738891c8432bff0",
        "deepnote_cell_type": "markdown"
      },
      "source": "## 1. Configura√ß√£o de Par√¢metros\n\nDefini√ß√£o dos par√¢metros principais para an√°lise e rate limiting da API.",
      "block_group": "b81950f9b4ca46b1a3517aa32588056a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "f56ec2a6",
        "execution_start": 1748385591762,
        "execution_millis": 6,
        "execution_context_id": "3ed297e7-a555-43b2-887a-b28769104494",
        "cell_id": "6a2e3802c2d443919f091e25de67b8ce",
        "deepnote_cell_type": "code"
      },
      "source": "# Configura√ß√µes principais\nCONFIG = {\n    'modelo_gemini': 'gemini-2.0-flash-exp',  # Flash-Lite para rate limits melhores\n    'max_requests_per_minute': 30,              # Limite de requisi√ß√µes por minuto\n    'max_requests_per_day': 1400,               # Limite di√°rio recomendado\n    'delay_between_requests': 2.1,              # Delay entre requisi√ß√µes (segundos)\n    'arquivo_entrada': '/home/delon/Modelos/cenanoink/Projeto-CENanoInk/Alan Delon.csv',\n    'checkpoint_file': 'analise_escopo_checkpoint.csv',\n    'arquivo_saida': 'df_com_analise_escopo_completo.csv'\n}\n\nprint(\"üìã Configura√ß√µes carregadas:\")\nfor key, value in CONFIG.items():\n    print(f\"  {key}: {value}\")",
      "block_group": "4bb4d6da8af54405b63b83516479f60a",
      "execution_count": 10,
      "outputs": [
        {
          "name": "stdout",
          "text": "üìã Configura√ß√µes carregadas:\n  modelo_gemini: gemini-2.0-flash-exp\n  max_requests_per_minute: 30\n  max_requests_per_day: 1400\n  delay_between_requests: 2.1\n  arquivo_entrada: /home/delon/Modelos/cenanoink/Projeto-CENanoInk/Alan Delon.csv\n  checkpoint_file: analise_escopo_checkpoint.csv\n  arquivo_saida: df_com_analise_escopo_completo.csv\n",
          "output_type": "stream"
        }
      ],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "cebb84cd9b644cd9bc9d9a8a14abd116",
        "deepnote_cell_type": "markdown"
      },
      "source": "## 2. Defini√ß√£o de Palavras-Chave\n\nConjunto abrangente de termos relacionados ao escopo de nanorevestimentos e tintas.",
      "block_group": "d54335acc51e43399f9c8cacb1622412"
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "87d37ae8",
        "execution_start": 1748385597449,
        "execution_millis": 6,
        "execution_context_id": "3ed297e7-a555-43b2-887a-b28769104494",
        "cell_id": "d5abb85b98574eeab0e3784dabc2652b",
        "deepnote_cell_type": "code"
      },
      "source": "# Palavras-chave organizadas por categoria\nPALAVRAS_CHAVE = {\n    'termos_principais': [\n        \"nanocoating\", \"nanocoatings\", \"nano coating\", \"nano coatings\",\n        \"nanorevestimento\", \"nanorevestimentos\", \"nanocomposite coating\"\n    ],\n    \n    'materiais_nano': [\n        \"TiO2\", \"titanium dioxide\", \"zinc oxide\", \"ZnO\", \n        \"silica nanoparticle\", \"alumina nanoparticle\", \"Al2O3\",\n        \"carbon nanotube\", \"CNT\", \"graphene\", \"iron oxide\", \"Fe2O3\",\n        \"cerium oxide\", \"CeO2\", \"clay nanoparticles\"\n    ],\n    \n    'propriedades_funcionais': [\n        \"anticorrosive coating\", \"anti-corrosion coating\",\n        \"antimicrobial coating\", \"antibacterial coating\",\n        \"self-cleaning coating\", \"superhydrophobic coating\",\n        \"photocatalytic coating\", \"UV resistant coating\",\n        \"scratch resistant coating\", \"wear resistant coating\"\n    ],\n    \n    'tecnicas_preparacao': [\n        \"sol-gel coating\", \"layer-by-layer\", \"dip coating\",\n        \"spin coating\", \"spray coating\", \"electrodeposition\",\n        \"CVD coating\", \"PVD coating\", \"plasma treatment\"\n    ],\n    \n    'aplicacoes': [\n        \"protective coating\", \"functional coating\",\n        \"smart coating\", \"intelligent coating\", \"automotive coating\",\n        \"marine coating\", \"architectural coating\", \"biomedical coating\"\n    ]\n}\n\n# Criar lista √∫nica de todas as palavras-chave\ntodas_palavras_chave = []\nfor categoria, palavras in PALAVRAS_CHAVE.items():\n    todas_palavras_chave.extend(palavras)\n\nprint(f\"üîç Total de palavras-chave definidas: {len(todas_palavras_chave)}\")\nprint(f\"üìÇ Categorias: {list(PALAVRAS_CHAVE.keys())}\")",
      "block_group": "bf0480bb9d21495c9c769e8b1e1188f4",
      "execution_count": 13,
      "outputs": [
        {
          "name": "stdout",
          "text": "üîç Total de palavras-chave definidas: 49\nüìÇ Categorias: ['termos_principais', 'materiais_nano', 'propriedades_funcionais', 'tecnicas_preparacao', 'aplicacoes']\n",
          "output_type": "stream"
        }
      ],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "d5bba769a0c340f99258db2f1551ca94",
        "deepnote_cell_type": "markdown"
      },
      "source": "## 3. Fun√ß√µes Utilit√°rias\n\nFun√ß√µes para filtro, busca e manipula√ß√£o de dados.",
      "block_group": "6d7303de5f854e09afb1006497b2a47e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "efcf1f3d",
        "execution_start": 1748385602618,
        "execution_millis": 1268,
        "execution_context_id": "3ed297e7-a555-43b2-887a-b28769104494",
        "cell_id": "2b63e84812314e8b93ba707eb0190ab6",
        "deepnote_cell_type": "code"
      },
      "source": "def filtrar_coluna(df, coluna, palavra, tipo=\"contem\"):\n    \"\"\"\n    Fun√ß√£o auxiliar para filtrar uma dataframe com base em uma palavra-chave espec√≠fica.\n\n    Par√¢metros:\n    - df (DataFrame): DataFrame Pandas com os dados\n    - coluna (str): Nome da coluna a ser filtrada\n    - palavra (str): Palavra-chave usada para filtrar\n    - tipo (str): M√©todo de filtro, op√ß√µes: 'contem', 'igual'. Default √© 'contem'.\n\n    Retorno:\n    - DataFrame contendo as linhas que correspondem ao filtro\n    \"\"\"\n    if tipo == \"contem\":\n        resultado = df[df[coluna].str.contains(palavra, case=False, na=False)]\n    elif tipo == \"igual\":\n        resultado = df[df[coluna].str.lower() == palavra.lower()]\n    else:\n        raise ValueError(\"Tipo de filtro n√£o suportado. Use 'contem' ou 'igual'.\")\n\n    return resultado",
      "block_group": "3a960f469dd64c0dbefb55d61dcbd875",
      "execution_count": 16,
      "outputs": [
        {
          "name": "stdout",
          "text": "Informa√ß√µes do DataFrame:\nTotal de linhas: 525\nColunas dispon√≠veis: ['Publication Type', 'Authors', 'Book Authors', 'Book Editors', 'Book Group Authors', 'Author Full Names', 'Book Author Full Names', 'Group Authors', 'Article Title', 'Source Title', 'Book Series Title', 'Book Series Subtitle', 'Language', 'Document Type', 'Conference Title', 'Conference Date', 'Conference Location', 'Conference Sponsor', 'Conference Host', 'Author Keywords', 'Keywords Plus', 'Abstract', 'Addresses', 'Affiliations', 'Reprint Addresses', 'Email Addresses', 'Researcher Ids', 'ORCIDs', 'Funding Orgs', 'Funding Name Preferred', 'Funding Text', 'Cited References', 'Cited Reference Count', 'Times Cited, WoS Core', 'Times Cited, All Databases', '180 Day Usage Count', 'Since 2013 Usage Count', 'Publisher', 'Publisher City', 'Publisher Address', 'ISSN', 'eISSN', 'ISBN', 'Journal Abbreviation', 'Journal ISO Abbreviation', 'Publication Date', 'Publication Year', 'Volume', 'Issue', 'Part Number', 'Supplement', 'Special Issue', 'Meeting Abstract', 'Start Page', 'End Page', 'Article Number', 'DOI', 'DOI Link', 'Book DOI', 'Early Access Date', 'Number of Pages', 'WoS Categories', 'Web of Science Index', 'Research Areas', 'IDS Number', 'Pubmed Id', 'Open Access Designations', 'Highly Cited Status', 'Hot Paper Status', 'Date of Export', 'UT (Unique WOS ID)', 'Web of Science Record', 'Unnamed: 72']\nBuscando por palavras-chave espec√≠ficas...\nBuscando: 'nanocoating'\n",
          "output_type": "stream"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'filtrar_coluna' is not defined",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 66\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mColunas dispon√≠veis: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(df.columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# Buscar artigos relacionados a nanorevestimentos\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m artigos_encontrados = \u001b[43mbuscar_artigos_nanorevestimentos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpalavras_chave_especificas\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(artigos_encontrados) > \u001b[32m0\u001b[39m:\n\u001b[32m     69\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== ARTIGOS ENCONTRADOS (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(artigos_encontrados)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) ===\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mbuscar_artigos_nanorevestimentos\u001b[39m\u001b[34m(df, palavras_chave, colunas)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m coluna \u001b[38;5;129;01min\u001b[39;00m colunas:\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m coluna \u001b[38;5;129;01min\u001b[39;00m df.columns:\n\u001b[32m     42\u001b[39m         \u001b[38;5;66;03m# Buscar na coluna\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m         temp_resultados = \u001b[43mfiltrar_coluna\u001b[49m(df, coluna, palavra, tipo=\u001b[33m'\u001b[39m\u001b[33mcontem\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     45\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(temp_resultados) > \u001b[32m0\u001b[39m:\n\u001b[32m     46\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Encontrados \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(temp_resultados)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m resultados em \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcoluna\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mNameError\u001b[39m: name 'filtrar_coluna' is not defined"
          ]
        }
      ],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "8a09f09331124c3c97d3c3b10b0e3aa1",
        "deepnote_cell_type": "markdown"
      },
      "source": "## 4. An√°lise com Gemini AI\n\nFun√ß√µes para an√°lise autom√°tica de abstracts usando a API do Gemini.",
      "block_group": "1f6c6e439be54d538e5a29ef8a1f1d8b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "3a44aa35",
        "execution_start": 1748385699201,
        "execution_millis": 352,
        "execution_context_id": "3ed297e7-a555-43b2-887a-b28769104494",
        "cell_id": "f72abd9edae64664809e1e5fc54ad579",
        "deepnote_cell_type": "code"
      },
      "source": "# ...existing code...\n\ndef analisar_escopo_abstract(abstract_text):\n    \"\"\"\n    Analisa um abstract individual para determinar se se adequa ao escopo de nanorevestimentos e tintas\n    \"\"\"\n    if pd.isna(abstract_text) or str(abstract_text).strip() == '':\n        return \"N√£o se adequa - Abstract vazio ou n√£o dispon√≠vel\"\n    \n    model = genai.GenerativeModel('gemini-2.0-flash')\n    \n    prompt = f\"\"\"\n    Analise o seguinte abstract cient√≠fico e determine se ele se adequa ao escopo de pesquisa sobre \"nanorevestimentos e tintas (nanocoatings and paints)\".\n\n    Abstract: {abstract_text}\n\n    Crit√©rios para adequa√ß√£o ao escopo:\n    - Estudos sobre nanomateriais aplicados em revestimentos ou tintas\n    - Propriedades funcionais de nanocoatings (anticorros√£o, antimicrobiano, autolimpante, etc.)\n    - T√©cnicas de prepara√ß√£o ou caracteriza√ß√£o de nanorevestimentos\n    - Aplica√ß√µes industriais de tintas nanoestruturadas\n    - Materiais nanoestruturados para prote√ß√£o de superf√≠cies\n\n    Responda APENAS uma das op√ß√µes:\n    1. \"Sim - [breve justificativa]\"\n    2. \"N√£o - [breve justificativa explicando por que n√£o se adequa]\"\n\n    Mantenha a justificativa concisa (m√°ximo 100 palavras).\n    \"\"\"\n    \n    try:\n        response = model.generate_content(prompt)\n        return response.text.strip()\n    except Exception as e:\n        return f\"Erro na an√°lise - {str(e)}\"\n\ndef processar_dataframe_escopo(df, coluna_abstract='Abstract', batch_size=10):\n    \"\"\"\n    Processa todo o DataFrame adicionando a coluna de adequa√ß√£o ao escopo\n    \"\"\"\n    # Verificar se a coluna Abstract existe\n    if coluna_abstract not in df.columns:\n        print(f\"Coluna '{coluna_abstract}' n√£o encontrada no DataFrame\")\n        return df\n    \n    # Criar uma c√≥pia do DataFrame\n    df_processado = df.copy()\n    \n    # Inicializar a nova coluna\n    df_processado['Se adequa ao escopo?'] = ''\n    \n    total_linhas = len(df_processado)\n    print(f\"Processando {total_linhas} abstracts...\")\n    \n    # Processar em lotes para evitar sobrecarga da API\n    for i in range(0, total_linhas, batch_size):\n        batch_end = min(i + batch_size, total_linhas)\n        print(f\"Processando linhas {i+1} a {batch_end}...\")\n        \n        for idx in range(i, batch_end):\n            abstract = df_processado.iloc[idx][coluna_abstract]\n            resultado = analisar_escopo_abstract(abstract)\n            df_processado.iloc[idx, df_processado.columns.get_loc('Se adequa ao escopo?')] = resultado\n            \n            # Mostrar progresso\n            if (idx + 1) % 5 == 0:\n                print(f\"  Processado: {idx + 1}/{total_linhas}\")\n        \n        # Pequena pausa entre lotes para n√£o sobrecarregar a API\n        import time\n        time.sleep(2)\n    \n    return df_processado\n\n# Carregar o DataFrame\ndf = pd.read_csv('/work/ExtensaoemCienciadeDadosUece/Imers√£o - CenanoInk/Alan Delon.csv')\n\nprint(\"Informa√ß√µes do DataFrame:\")\nprint(f\"Total de linhas: {len(df)}\")\nprint(f\"Colunas dispon√≠veis: {list(df.columns)}\")\n\n# Verificar se existe coluna Abstract\nif 'Abstract' in df.columns:\n    print(f\"Abstracts n√£o vazios: {df['Abstract'].notna().sum()}\")\n    \n    # Processar apenas uma amostra primeiro (para teste)\n    print(\"\\n=== PROCESSAMENTO DE TESTE (primeiras 5 linhas) ===\")\n    df_teste = df.head(5).copy()\n    df_teste_processado = processar_dataframe_escopo(df_teste, batch_size=1)\n    \n    print(\"\\nResultados do teste:\")\n    for idx, row in df_teste_processado.iterrows():\n        print(f\"\\nLinha {idx + 1}:\")\n        print(f\"T√≠tulo: {row.get('Article Title', 'N/A')[:100]}...\")\n        print(f\"Adequa√ß√£o: {row['Se adequa ao escopo?']}\")\n    \n    # Perguntar se deseja processar todo o DataFrame\n    resposta = input(\"\\nDeseja processar todo o DataFrame? (s/n): \")\n    if resposta.lower() == 's':\n        print(\"\\n=== PROCESSANDO TODO O DATAFRAME ===\")\n        df_completo = processar_dataframe_escopo(df)\n        \n        # Salvar resultado\n        arquivo_saida = 'df_com_analise_escopo.csv'\n        df_completo.to_csv(arquivo_saida, index=False)\n        print(f\"\\nResultados salvos em: {arquivo_saida}\")\n        \n        # Mostrar estat√≠sticas\n        adequados = df_completo['Se adequa ao escopo?'].str.contains('Sim', case=False, na=False).sum()\n        nao_adequados = df_completo['Se adequa ao escopo?'].str.contains('N√£o', case=False, na=False).sum()\n        \n        print(f\"\\n=== ESTAT√çSTICAS FINAIS ===\")\n        print(f\"Adequados ao escopo: {adequados}\")\n        print(f\"N√£o adequados ao escopo: {nao_adequados}\")\n        print(f\"Total processado: {adequados + nao_adequados}\")\n        \nelse:\n    print(\"Coluna 'Abstract' n√£o encontrada no DataFrame\")\n    print(\"Colunas dispon√≠veis:\", list(df.columns))",
      "block_group": "ab3dcba9461f46b980b51b4101a4dfab",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Informa√ß√µes do DataFrame:\nTotal de linhas: 525\nColunas dispon√≠veis: ['Publication Type', 'Authors', 'Book Authors', 'Book Editors', 'Book Group Authors', 'Author Full Names', 'Book Author Full Names', 'Group Authors', 'Article Title', 'Source Title', 'Book Series Title', 'Book Series Subtitle', 'Language', 'Document Type', 'Conference Title', 'Conference Date', 'Conference Location', 'Conference Sponsor', 'Conference Host', 'Author Keywords', 'Keywords Plus', 'Abstract', 'Addresses', 'Affiliations', 'Reprint Addresses', 'Email Addresses', 'Researcher Ids', 'ORCIDs', 'Funding Orgs', 'Funding Name Preferred', 'Funding Text', 'Cited References', 'Cited Reference Count', 'Times Cited, WoS Core', 'Times Cited, All Databases', '180 Day Usage Count', 'Since 2013 Usage Count', 'Publisher', 'Publisher City', 'Publisher Address', 'ISSN', 'eISSN', 'ISBN', 'Journal Abbreviation', 'Journal ISO Abbreviation', 'Publication Date', 'Publication Year', 'Volume', 'Issue', 'Part Number', 'Supplement', 'Special Issue', 'Meeting Abstract', 'Start Page', 'End Page', 'Article Number', 'DOI', 'DOI Link', 'Book DOI', 'Early Access Date', 'Number of Pages', 'WoS Categories', 'Web of Science Index', 'Research Areas', 'IDS Number', 'Pubmed Id', 'Open Access Designations', 'Highly Cited Status', 'Hot Paper Status', 'Date of Export', 'UT (Unique WOS ID)', 'Web of Science Record', 'Unnamed: 72']\nAbstracts n√£o vazios: 525\n\n=== PROCESSAMENTO DE TESTE (primeiras 5 linhas) ===\nProcessando 5 abstracts...\nProcessando linhas 1 a 1...\nProcessando linhas 2 a 2...\nProcessando linhas 3 a 3...\nProcessando linhas 4 a 4...\nProcessando linhas 5 a 5...\n  Processado: 5/5\n\nResultados do teste:\n\nLinha 1:\nT√≠tulo: Ab Initio Study of the Atomic Level Structure of the Rutile TiO2(110) -Titanium Nitride (TiN) Interf...\nAdequa√ß√£o: Sim - O abstract descreve um estudo te√≥rico (DFT) sobre a interface TiO2/TiN, que √© relevante para o entendimento e potencial otimiza√ß√£o de revestimentos protetores de TiN, com foco na forma√ß√£o de uma camada de √≥xido superficial. Apesar de ser um estudo fundamental, a compreens√£o da estrutura e propriedades da interface √© crucial para o desenvolvimento de nanorevestimentos com melhor desempenho.\n\nLinha 2:\nT√≠tulo: Polymer and ceramic nanocomposites for aerospace applications...\nAdequa√ß√£o: Sim - O abstract menciona a aplica√ß√£o de nanocomp√≥sitos polim√©ricos em revestimentos (coatings) para a ind√∫stria aeroespacial, indicando relev√¢ncia para o escopo de pesquisa sobre nanorrevestimentos. Embora n√£o se concentre exclusivamente em nanorrevestimentos, a men√ß√£o √† aplica√ß√£o nessa √°rea sugere que o artigo pode conter informa√ß√µes relevantes.\n\nLinha 3:\nT√≠tulo: Recycling of yttria-stabilized zirconia waste powders in glazes suitable for ceramic tiles...\nAdequa√ß√£o: 2.  N√£o - O abstract descreve o uso de res√≠duos de zirc√¥nia estabilizada com √≠trio para produ√ß√£o de fritas para esmaltes e cer√¢micas, focando na substitui√ß√£o de zirc√¥nia pura. N√£o h√° men√ß√£o a nanomateriais, nanorevestimentos, tintas, ou propriedades relacionadas a nanoestruturas para prote√ß√£o de superf√≠cies. O foco est√° na reciclagem de um res√≠duo cer√¢mico para uma aplica√ß√£o espec√≠fica em cer√¢mica, n√£o em nanotecnologia para revestimentos ou tintas.\n\nLinha 4:\nT√≠tulo: Properties of Polysiloxane Coated Borosilicate Lining Blocks...\nAdequa√ß√£o: 1.  \"Sim - O abstract descreve a aplica√ß√£o de uma suspens√£o de s√≠lica fumada (potencialmente nanomaterial) e polissiloxano como revestimento em um bloco de borosilicato para melhorar a resist√™ncia t√©rmica. Embora n√£o explicite a nanoescala, a s√≠lica fumada frequentemente possui caracter√≠sticas nanom√©tricas e o foco na funcionalidade (resist√™ncia t√©rmica) e prepara√ß√£o do revestimento se alinham ao escopo.\"\n\nLinha 5:\nT√≠tulo: Solution Processed Porous Fe2O3 Thin Films for Solar-Driven Water Splitting...\nAdequa√ß√£o: N√£o - O abstract descreve a prepara√ß√£o e caracteriza√ß√£o de filmes finos de hematita (alpha-Fe2O3) para aplica√ß√£o em divis√£o da √°gua impulsionada por energia solar. Embora envolva nanomateriais (filmes finos mesoporosos), o foco principal √© a aplica√ß√£o em c√©lulas fotoeletroqu√≠micas e n√£o em revestimentos ou tintas para prote√ß√£o de superf√≠cies ou funcionalidades t√≠picas de nanocoatings.\n\n=== PROCESSANDO TODO O DATAFRAME ===\nProcessando 525 abstracts...\nProcessando linhas 1 a 10...\n",
          "output_type": "stream"
        }
      ],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "9eee10dcfba1467d8b83fef58d192181",
        "deepnote_cell_type": "markdown"
      },
      "source": "## 5. Processamento em Lote\n\nFun√ß√µes para processar grandes volumes de dados respeitando rate limits.",
      "block_group": "3a4a7229dc854d46af36dd96c7738edd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "deepnote_to_be_reexecuted": true,
        "cell_id": "6e8d81de55474eb5b5c808a167db3ad4",
        "deepnote_cell_type": "code"
      },
      "source": "import pandas as pd\nimport genaiimport random\nas as pd\nimport time\nimport randomfrom typing import List\n\ndef analisar_escopo_abstract(abstract_text: str, retry_count: int = 3) -> str:lisar_escopo_abstract(abstract_text, retry_count=3):\n    \"\"\"\n    Analisa um abstract individual para adequa√ß√£o ao escopoal para determinar se se adequa ao escopo de nanorevestimentos e tintas\n    ndo Gemini 2.0 Flash-Lite\n    Args:\n        abstract_text: Texto do abstract:\n        retry_count: N√∫mero de tentativas em caso de erro    return \"N√£o se adequa - Abstract vazio ou n√£o dispon√≠vel\"\n    \n    Returns:\n        String com resultado da an√°lisemodel = genai.GenerativeModel('gemini-2.0-flash-exp')\n    \"\"\"\n    if pd.isna(abstract_text) or str(abstract_text).strip() == '':\n        return \"N√£o se adequa - Abstract vazio ou n√£o dispon√≠vel\"    Analise o seguinte abstract cient√≠fico e determine se ele se adequa ao escopo de pesquisa sobre \"nanorevestimentos e tintas (nanocoatings and paints)\".\n    \n    model = genai.GenerativeModel(CONFIG['modelo_gemini'])    Abstract: {abstract_text}\n    \n    prompt = f\"\"\"\n    Analise o seguinte abstract cient√≠fico e determine se ele se adequa ao escopo de pesquisa sobre \"nanorevestimentos e tintas (nanocoatings and paints)\".\ncrobiano, autolimpante, etc.)\n    Abstract: {abstract_text}evestimentos\n\n    Crit√©rios para adequa√ß√£o ao escopo:    - Materiais nanoestruturados para prote√ß√£o de superf√≠cies\n    - Estudos sobre nanomateriais aplicados em revestimentos ou tintas\n    - Propriedades funcionais de nanocoatings (anticorros√£o, antimicrobiano, autolimpante, etc.)\n    - T√©cnicas de prepara√ß√£o ou caracteriza√ß√£o de nanorevestimentos\n    - Aplica√ß√µes industriais de tintas nanoestruturadas    2. \"N√£o - [breve justificativa explicando por que n√£o se adequa]\"\n    - Materiais nanoestruturados para prote√ß√£o de superf√≠cies\ntenha a justificativa concisa (m√°ximo 100 palavras).\n    Responda APENAS uma das op√ß√µes:\"\"\"\n    1. \"Sim - [breve justificativa]\"\n    2. \"N√£o - [breve justificativa explicando por que n√£o se adequa]\"mpt in range(retry_count):\n\n    Mantenha a justificativa concisa (m√°ximo 100 palavras).ntent(prompt)\n    \"\"\"xt.strip()\n    \n    for attempt in range(retry_count):\n        try:in error_msg.lower():\n            response = model.generate_content(prompt)\n            return response.text.strip()ar rate limit\n        except Exception as e:\n            error_msg = str(e)- aguardando {wait_time}s...\")\n            if \"429\" in error_msg or \"quota\" in error_msg.lower():ep(wait_time)\n                if attempt < retry_count - 1:ontinue\n                    wait_time = 3 + random.randint(1, 2)\n                    print(f\"  ‚è≥ Rate limit - aguardando {wait_time}s...\")   return f\"Erro - Rate limit excedido ap√≥s {retry_count} tentativas\"\n                    time.sleep(wait_time)\n                    continue            return f\"Erro na an√°lise - {error_msg}\"\n                else:\n                    return f\"Erro - Rate limit excedido ap√≥s {retry_count} tentativas\"    return \"Erro - Falha ap√≥s m√∫ltiplas tentativas\"\n            else:\n                              coluna_abstract: str = 'Abstract') -> pd.DataFrame:         return f\"Erro na an√°lise - {error_msg}\"cessar_dataframe_escopo(df, coluna_abstract='Abstract', max_requests_per_day=1400):\n    \"\"\"\n    Processa DataFrame completo adicionando an√°lise de escopo\" de adequa√ß√£o ao escopo\n    Respeitando limites de API: 30 req/min, 1.500 req/diao limites: 30 req/min, 1.500 req/dia\n    bstract='Abstract', max_requests_per_day=1400):\n    Args:\n        df: DataFrame a ser processadoo\n        coluna_abstract: Nome da coluna com abstractsimites: 30 req/min, 1.500 req/diaoluna '{coluna_abstract}' n√£o encontrada no DataFrame\")\n    \"\"\"    return df\n    Returns:ct existe\n        DataFrame com nova coluna 'Se adequa ao escopo?' df.columns:Frame\n    \"\"\"    print(f\"Coluna '{coluna_abstract}' n√£o encontrada no DataFrame\")df_processado = df.copy()\n    if coluna_abstract not in df.columns:\n        print(f\"‚ùå Coluna '{coluna_abstract}' n√£o encontrada\")\n        return df# Criar uma c√≥pia do DataFramedf_processado['Se adequa ao escopo?'] = ''\n    \n    df_processado = df.copy()\n    df_processado['Se adequa ao escopo?'] = ''# Inicializar a nova colunaprint(f\"Processando {total_linhas} abstracts...\")\n    '\n    total_linhas = len(df_processado)\n    print(f\"üìã Processando {total_linhas} abstracts...\")\n    \n    # Verificar limite di√°rioax_requests_per_day} linhas? (s/n): \")\n    if total_linhas > CONFIG['max_requests_per_day']:\n        print(f\"‚ö†Ô∏è Dataset tem {total_linhas} linhas, limite di√°rio: {CONFIG['max_requests_per_day']}\")linhas > max_requests_per_day:otal_linhas = max_requests_per_day\n        resposta = input(f\"Processar apenas as primeiras {CONFIG['max_requests_per_day']} linhas? (s/n): \")inhas} linhas, mas limite di√°rio √© {max_requests_per_day}\")\n        if resposta.lower() == 's':a processar apenas as primeiras {max_requests_per_day} linhas? (s/n): \") cancelado.\")\n            total_linhas = CONFIG['max_requests_per_day']    if resposta.lower() == 's':        return df_processado\n        else:\n            print(\"‚ùå Processamento cancelado\")\n            return df_processado        print(\"Processamento cancelado.\")print(f\"Tempo estimado: {(total_linhas * 2.5) / 60:.1f} minutos\")\n    _processado\n    tempo_estimado = (total_linhas * CONFIG['delay_between_requests']) / 60\n    print(f\"‚è±Ô∏è Tempo estimado: {tempo_estimado:.1f} minutos\")print(f\"Rate limiting: 30 req/min, processando {total_linhas} abstracts\")start_time = time.time()\n    print(f\"üîÑ Rate limiting: {CONFIG['max_requests_per_minute']} req/min\")_linhas * 2.5) / 60:.1f} minutos\")\n    \n    requests_made = 0\n    start_time = time.time()\n    adequados_count = 0\n    \n    for idx in range(total_linhas): 30 req/min aguardando {wait_time:.1f}s...\")\n        # Controle de rate limitime() - start_timeime)\n        elapsed_time = time.time() - start_timelapsed_time < 60:\n        if requests_made >= (CONFIG['max_requests_per_minute'] - 1) and elapsed_time < 60:    wait_time = 60 - elapsed_time + 2  # +2 segundos de margem    start_time = time.time()\n            wait_time = 60 - elapsed_time + 2}s...\")\n            print(f\"\\n‚è≥ Rate limit: aguardando {wait_time:.1f}s...\")    time.sleep(wait_time)abstract = df_processado.iloc[idx][coluna_abstract]\n            time.sleep(wait_time) 0\n            requests_made = 0\n            start_time = time.time()\n        abstract = df_processado.iloc[idx][coluna_abstract]    print(f\"Processando {idx + 1}/{total_linhas} ({((idx + 1)/total_linhas)*100:.1f}%)\")\n        abstract = df_processado.iloc[idx][coluna_abstract]\n        \n        # Progressoif idx % 10 == 0 or idx == total_linhas - 1:df_processado.iloc[idx, df_processado.columns.get_loc('Se adequa ao escopo?')] = resultado\n        if idx % 10 == 0 or idx == total_linhas - 1:sando {idx + 1}/{total_linhas} ({((idx + 1)/total_linhas)*100:.1f}%)\")\n            print(f\"üìä Processando {idx + 1}/{total_linhas} ({((idx + 1)/total_linhas)*100:.1f}%)\")requests_made += 1\n        \n        resultado = analisar_escopo_abstract(abstract)entre requisi√ß√µes (2 segundos para 30 req/min)loc[idx, df_processado.columns.get_loc('Se adequa ao escopo?')] = resultado\n        df_processado.iloc[idx, df_processado.columns.get_loc('Se adequa ao escopo?')] = resultadotime.sleep(2.1)\n        \n        if 'Sim' in resultado:ada 25 processamentos\n            adequados_count += 1\n        +1].str.contains('Sim', case=False, na=False).sum()\n        requests_made += 1        print(f\"  Adequados at√© agora: {adequados_parcial}/{idx + 1}\")    \n        time.sleep(CONFIG['delay_between_requests'])samentos\n            return df_processado        if (idx + 1) % 25 == 0:\n        # Estat√≠sticas parciais\n        if (idx + 1) % 25 == 0:     print(f\"  Adequados at√© agora: {adequados_parcial}/{idx + 1}\")cessar_com_checkpoint(df, coluna_abstract='Abstract', checkpoint_file='checkpoint_analise.csv', start_from=0):\n            taxa_atual = (adequados_count / (idx + 1)) * 100\n            print(f\"  ‚úÖ Adequados at√© agora: {adequados_count}/{idx + 1} ({taxa_atual:.1f}%)\")urn df_processados√£o com checkpoint para poder retomar o processamento\n    \n    return df_processadotract='Abstract', checkpoint_file='checkpoint_analise.csv', start_from=0):\n_file and start_from == 0:\ndef processar_com_checkpoint(df: pd.DataFrame, ento\n                           coluna_abstract: str = 'Abstract') -> pd.DataFrame:\n    \"\"\"\n    Processamento com sistema de checkpoint para recupera√ß√£o\n    )\n    Args:checkpoint_file)s':\n        df: DataFrame a ser processadopo?' in df_checkpoint.columns:ckpoint\n        coluna_abstract: Nome da coluna com abstractslinhas j√° processadas\")\n                resposta = input(\"Deseja continuar do checkpoint? (s/n): \")        print(\"Nenhum checkpoint encontrado, iniciando do zero\")\n    Returns:.lower() == 's':\n        DataFrame processado\n    \"\"\"    except FileNotFoundError:df_processado = processar_dataframe_escopo(df, coluna_abstract)\n    checkpoint_file = CONFIG['checkpoint_file']um checkpoint encontrado, iniciando do zero\")\n    \n    # Verificar checkpoint existente\n    try:a_abstract)e)\n        df_checkpoint = pd.read_csv(checkpoint_file)    print(f\"Checkpoint salvo em: {checkpoint_file}\")\n        if 'Se adequa ao escopo?' in df_checkpoint.columns:\n            processed_count = df_checkpoint['Se adequa ao escopo?'].ne('').sum()    if checkpoint_file:    return df_processado\n            print(f\"üìÇ Checkpoint encontrado: {processed_count} linhas j√° processadas\")to_csv(checkpoint_file, index=False)\n            \n            resposta = input(\"Continuar do checkpoint? (s/n): \")        \"\"\"\n            if resposta.lower() == 's': espec√≠fica\n                return df_checkpoint\n    except FileNotFoundError:.DataFrame) -> str:\n        print(\"üìù Nenhum checkpoint encontrado, iniciando do zero\")    \"\"\"        df: DataFrame a ser filtrado\n    rtigos com Gemini\n    # Processar normalmente\n    df_processado = processar_dataframe_escopo(df, coluna_abstract)\n    s\n    # Salvar checkpointReturns:\n    df_processado.to_csv(checkpoint_file, index=False)\n    print(f\"üíæ Checkpoint salvo em: {checkpoint_file}\")\n    \n    return df_processadoif len(dados_filtrados) == 0:    print(f\"‚ö†Ô∏è Coluna '{coluna}' n√£o encontrada\")\naFrame()m dado dispon√≠vel para an√°lise.\"\n# Carregar o DataFrame\ndf = pd.read_csv('/home/delon/Modelos/cenanoink/Projeto-CENanoInk/Alan Delon.csv')\nstr.contains(termo, case=False, na=False)filtrados.head(10)\nprint(\"Informa√ß√µes do DataFrame:\")elif tipo == 'exato':texto_dados = dados_amostra[['Article Title', 'Abstract']].to_string()\nprint(f\"Total de linhas: {len(df)}\")\nprint(f\"Colunas dispon√≠veis: {list(df.columns)}\")delo_gemini'])\nh(termo.lower())\n# Verificar se existe coluna Abstract\nif 'Abstract' in df.columns:s e tintas:\n    abstracts_validos = df['Abstract'].notna().sum()\n    print(f\"Abstracts n√£o vazios: {abstracts_validos}\")return df[mask]{texto_dados}\n    \n    # Teste com 3 abstracts primeiroataFrame, \n    print(\"\\n=== TESTE INICIAL (3 abstracts) ===\")\n    df_teste = df.head(3).copy()['Article Title', 'Abstract']) -> pd.DataFrame:\n    \n    # Processar testeBusca artigos usando m√∫ltiplas palavras-chave em m√∫ltiplas colunas4. Aplica√ß√µes industriais identificadas\n    start_test = time.time()\n    df_teste_processado = processar_dataframe_escopo(df_teste)\n    end_test = time.time()\n    \n    print(f\"\\nTempo do teste: {end_test - start_test:.1f} segundos\")colunas: Colunas onde buscar\n    print(\"\\nResultados do teste:\")\n    for idx, row in df_teste_processado.iterrows():\n        print(f\"\\nLinha {idx + 1}:\")\n        print(f\"T√≠tulo: {row.get('Article Title', 'N/A')[:80]}...\")\n        print(f\"Adequa√ß√£o: {row['Se adequa ao escopo?']}\")ltados = pd.DataFrame()return f\"Erro na an√°lise geral: {str(e)}\"\n    \n    # Perguntar sobre processamento completo\n    print(f\"\\n=== PROCESSAMENTO COMPLETO ===\")\n    print(f\"Total de abstracts para processar: {abstracts_validos}\")\n    print(f\"Limite di√°rio recomendado: 1.400 requisi√ß√µes\")palavra in palavras_chave:nforma√ß√µes do DataFrame:\")\n    print(f\"Tempo estimado: {(abstracts_validos * 2.5) / 60:.1f} minutos\")\n    \n    resposta = input(\"\\nDeseja processar todo o DataFrame? (s/n): \")\n    if resposta.lower() == 's':oluna(df, coluna, palavra, tipo='contem')tract\n        print(\"\\nIniciando processamento completo com checkpoint...\")\n        df_completo = processar_com_checkpoint(df, checkpoint_file='analise_escopo_checkpoint.csv')            count_palavra += len(temp)racts_validos = df['Abstract'].notna().sum()\n        tados = pd.concat([resultados, temp]) vazios: {abstracts_validos}\")\n        # Salvar resultado final\n        arquivo_saida = 'df_com_analise_escopo_completo.csv'if count_palavra > 0:ste com 3 abstracts primeiro\n        df_completo.to_csv(arquivo_saida, index=False)       estatisticas[palavra] = count_palavrarint(\"\\n=== TESTE INICIAL (3 abstracts) ===\")\n        print(f\"\\nResultados finais salvos em: {arquivo_saida}\")dos\")\n        \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    print(\"Colunas dispon√≠veis:\", list(df.columns))    print(\"Coluna 'Abstract' n√£o encontrada no DataFrame\")else:                    print(f\"Taxa de adequa√ß√£o: {(adequados/(adequados + nao_adequados))*100:.1f}%\")        if adequados > 0:                print(f\"Total processado: {adequados + nao_adequados + erros}\")        print(f\"Erros: {erros}\")        print(f\"N√£o adequados ao escopo: {nao_adequados}\")        print(f\"Adequados ao escopo: {adequados}\")        print(f\"\\n=== ESTAT√çSTICAS FINAIS ===\")                erros = df_completo['Se adequa ao escopo?'].str.contains('Erro', case=False, na=False).sum()        nao_adequados = df_completo['Se adequa ao escopo?'].str.contains('N√£o', case=False, na=False).sum()        adequados = df_completo['Se adequa ao escopo?'].str.contains('Sim', case=False, na=False).sum()        # Mostrar estat√≠sticas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    print(\"Colunas dispon√≠veis:\", list(df.columns))    print(\"Coluna 'Abstract' n√£o encontrada no DataFrame\")else:                    print(f\"Taxa de adequa√ß√£o: {(adequados/(adequados + nao_adequados))*100:.1f}%\")        if adequados > 0:                print(f\"Total processado: {adequados + nao_adequados + erros}\")        print(f\"Erros: {erros}\")        print(f\"N√£o adequados ao escopo: {nao_adequados}\")        print(f\"Adequados ao escopo: {adequados}\")        print(f\"\\n=== ESTAT√çSTICAS FINAIS ===\")                erros = df_completo['Se adequa ao escopo?'].str.contains('Erro', case=False, na=False).sum()        nao_adequados = df_completo['Se adequa ao escopo?'].str.contains('N√£o', case=False, na=False).sum()        adequados = df_completo['Se adequa ao escopo?'].str.contains('Sim', case=False, na=False).sum()        # Mostrar estat√≠sticas                print(f\"\\nResultados finais salvos em: {arquivo_saida}\")        df_completo.to_csv(arquivo_saida, index=False)        arquivo_saida = 'df_com_analise_escopo_completo.csv'        # Salvar resultado final                df_completo = processar_com_checkpoint(df, checkpoint_file='analise_escopo_checkpoint.csv')        print(\"\\nIniciando processamento completo com checkpoint...\")    if resposta.lower() == 's':    resposta = input(\"\\nDeseja processar todo o DataFrame? (s/n): \")        print(f\"Tempo estimado: {(abstracts_validos * 2.5) / 60:.1f} minutos\")    print(f\"Limite di√°rio recomendado: 1.400 requisi√ß√µes\")    print(f\"Total de abstracts para processar: {abstracts_validos}\")    print(f\"\\n=== PROCESSAMENTO COMPLETO ===\")    # Perguntar sobre processamento completo            print(f\"Adequa√ß√£o: {row['Se adequa ao escopo?']}\")        print(f\"T√≠tulo: {row.get('Article Title', 'N/A')[:80]}...\")        print(f\"\\nLinha {idx + 1}:\")    for idx, row in df_teste_processado.iterrows():    print(\"\\nResultados do teste:\")    print(f\"\\nTempo do teste: {end_test - start_test:.1f} segundos\")        end_test = time.time()    df_teste_processado = processar_dataframe_escopo(df_teste)    start_test = time.time()    # Processar teste        df_teste = df.head(3).copy()    print(\"\\n=== TESTE INICIAL (3 abstracts) ===\")    # Teste com 3 abstracts primeiro        print(f\"Abstracts n√£o vazios: {abstracts_validos}\")    abstracts_validos = df['Abstract'].notna().sum()if 'Abstract' in df.columns:# Verificar se existe coluna Abstractprint(f\"Colunas dispon√≠veis: {list(df.columns)}\")print(f\"Total de linhas: {len(df)}\")print(\"Informa√ß√µes do DataFrame:\")df = pd.read_csv('/home/delon/Modelos/cenanoink/Projeto-CENanoInk/Alan Delon.csv')# Carregar o DataFrame    return resultados            print(\"\\n‚ùå Nenhum artigo encontrado\")    else:        print(f\"üìä Palavras-chave com resultados: {len(estatisticas)}\")        print(f\"\\n‚úÖ Total de artigos √∫nicos encontrados: {len(resultados)}\")        resultados = resultados.drop_duplicates()    if len(resultados) > 0:    # Remover duplicatas    # Processar teste\n    start_test = time.time()\n    df_teste_processado = processar_dataframe_escopo(df_teste)\n    end_test = time.time()\n    \n    print(f\"\\nTempo do teste: {end_test - start_test:.1f} segundos\")\n    print(\"\\nResultados do teste:\")\n    for idx, row in df_teste_processado.iterrows():\n        print(f\"\\nLinha {idx + 1}:\")\n        print(f\"T√≠tulo: {row.get('Article Title', 'N/A')[:80]}...\")\n        print(f\"Adequa√ß√£o: {row['Se adequa ao escopo?']}\")\n    \n    # Perguntar sobre processamento completo\n    print(f\"\\n=== PROCESSAMENTO COMPLETO ===\")\n    print(f\"Total de abstracts para processar: {abstracts_validos}\")\n    print(f\"Limite di√°rio recomendado: 1.400 requisi√ß√µes\")\n    print(f\"Tempo estimado: {(abstracts_validos * 2.5) / 60:.1f} minutos\")\n    \n    resposta = input(\"\\nDeseja processar todo o DataFrame? (s/n): \")\n    if resposta.lower() == 's':\n        print(\"\\nIniciando processamento completo com checkpoint...\")\n        df_completo = processar_com_checkpoint(df, checkpoint_file='analise_escopo_checkpoint.csv')\n        \n        # Salvar resultado final\n        arquivo_saida = 'df_com_analise_escopo_completo.csv'\n        df_completo.to_csv(arquivo_saida, index=False)\n        print(f\"\\nResultados finais salvos em: {arquivo_saida}\")\n        \n        # Mostrar estat√≠sticas\n        adequados = df_completo['Se adequa ao escopo?'].str.contains('Sim', case=False, na=False).sum()\n        nao_adequados = df_completo['Se adequa ao escopo?'].str.contains('N√£o', case=False, na=False).sum()\n        erros = df_completo['Se adequa ao escopo?'].str.contains('Erro', case=False, na=False).sum()\n        \n        print(f\"\\n=== ESTAT√çSTICAS FINAIS ===\")\n        print(f\"Adequados ao escopo: {adequados}\")\n        print(f\"N√£o adequados ao escopo: {nao_adequados}\")\n        print(f\"Erros: {erros}\")\n        print(f\"Total processado: {adequados + nao_adequados + erros}\")\n        \n        if adequados > 0:\n            print(f\"Taxa de adequa√ß√£o: {(adequados/(adequados + nao_adequados))*100:.1f}%\")\n        \nelse:\n    print(\"Coluna 'Abstract' n√£o encontrada no DataFrame\")\n    print(\"Colunas dispon√≠veis:\", list(df.columns))",
      "block_group": "0abc433754b44dba8763ce3e02066b69",
      "execution_count": null,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "99bbfdbfe8e7432e9241a937d2ecc8d1",
        "deepnote_cell_type": "markdown"
      },
      "source": "## 6. Execu√ß√£o Principal\n\nScript principal para carregar dados e executar an√°lises.",
      "block_group": "80483a10bb7f4b5abde919f31eaaa694"
    },
    {
      "cell_type": "code",
      "metadata": {
        "deepnote_to_be_reexecuted": true,
        "cell_id": "3fd24c99c5944d339e143f1b21acf3aa",
        "deepnote_cell_type": "code"
      },
      "source": "# Carregar e explorar os dados\nprint(\"üìÇ Carregando dados...\")\n\ntry:\n    df = pd.read_csv(CONFIG['arquivo_entrada'])\n    print(f\"‚úÖ Dados carregados com sucesso\")\n    print(f\"üìä Total de linhas: {len(df):,}\")\n    print(f\"üìä Total de colunas: {len(df.columns)}\")\n    print(f\"üìã Colunas dispon√≠veis: {list(df.columns)}\")\n    \n    # Verificar colunas essenciais\n    colunas_essenciais = ['Article Title', 'Abstract']\n    colunas_encontradas = [col for col in colunas_essenciais if col in df.columns]\n    colunas_faltantes = [col for col in colunas_essenciais if col not in df.columns]\n    \n    print(f\"\\n‚úÖ Colunas encontradas: {colunas_encontradas}\")\n    if colunas_faltantes:\n        print(f\"‚ö†Ô∏è Colunas faltantes: {colunas_faltantes}\")\n    \n    # Estat√≠sticas dos abstracts\n    if 'Abstract' in df.columns:\n        abstracts_validos = df['Abstract'].notna().sum()\n        abstracts_vazios = df['Abstract'].isna().sum()\n        print(f\"\\nüìÑ Abstracts dispon√≠veis: {abstracts_validos:,}\")\n        print(f\"üìÑ Abstracts vazios: {abstracts_vazios:,}\")\n        print(f\"üìÑ Taxa de completude: {(abstracts_validos/len(df))*100:.1f}%\")\n    \nexcept FileNotFoundError:\n    print(f\"‚ùå Arquivo n√£o encontrado: {CONFIG['arquivo_entrada']}\")\n    df = None\nexcept Exception as e:\n    print(f\"‚ùå Erro ao carregar dados: {e}\")\n    df = None",
      "block_group": "c9d85e9fcf374491a8a22b116c00ba31",
      "execution_count": null,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "deepnote_to_be_reexecuted": true,
        "cell_id": "e258f6ef43e04dfa994d4ea4f2329c12",
        "deepnote_cell_type": "code"
      },
      "source": "# Buscar artigos relevantes por palavras-chave\nif df is not None:\n    print(\"\\n\" + \"=\"*50)\n    print(\"üîç BUSCA POR PALAVRAS-CHAVE\")\n    print(\"=\"*50)\n    \n    # Buscar artigos usando todas as palavras-chave\n    artigos_encontrados = buscar_artigos_por_palavras_chave(df, todas_palavras_chave)\n    \n    if len(artigos_encontrados) > 0:\n        print(f\"\\nüìã RESULTADOS DA BUSCA:\")\n        print(f\"  Total de artigos encontrados: {len(artigos_encontrados):,}\")\n        print(f\"  Percentual do dataset: {(len(artigos_encontrados)/len(df))*100:.2f}%\")\n        \n        # Mostrar amostra dos t√≠tulos\n        print(f\"\\nüìë Primeiros 10 t√≠tulos encontrados:\")\n        for i, titulo in enumerate(artigos_encontrados['Article Title'].head(10), 1):\n            print(f\"  {i:2d}. {titulo[:80]}{'...' if len(titulo) > 80 else ''}\")\n        \n        # Salvar artigos encontrados\n        arquivo_filtrados = 'artigos_nanorevestimentos_encontrados.csv'\n        artigos_encontrados.to_csv(arquivo_filtrados, index=False)\n        print(f\"\\nüíæ Artigos filtrados salvos em: {arquivo_filtrados}\")\n        \n        # An√°lise geral com Gemini\n        print(f\"\\nü§ñ Realizando an√°lise geral com Gemini...\")\n        analise_geral = analisar_com_gemini_geral(artigos_encontrados)\n        print(\"\\n\" + \"=\"*50)\n        print(\"üìä AN√ÅLISE GERAL - GEMINI AI\")\n        print(\"=\"*50)\n        print(analise_geral)\n        \n    else:\n        print(\"\\n‚ùå Nenhum artigo encontrado com as palavras-chave especificadas\")\n        print(\"üí° Sugest√£o: Verificar se o dataset cont√©m artigos da √°rea\")\nelse:\n    print(\"‚ùå N√£o foi poss√≠vel carregar os dados\")",
      "block_group": "067684ac8e464eee9758df17b12bcbcd",
      "execution_count": null,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "deepnote_to_be_reexecuted": true,
        "cell_id": "623e8051c39544feac26910eb7d09406",
        "deepnote_cell_type": "code"
      },
      "source": "# An√°lise detalhada de escopo (opcional)\nif df is not None and len(artigos_encontrados) > 0:\n    print(\"\\n\" + \"=\"*50)\n    print(\"üî¨ AN√ÅLISE DETALHADA DE ESCOPO\")\n    print(\"=\"*50)\n    \n    if 'Abstract' in artigos_encontrados.columns:\n        abstracts_disponiveis = artigos_encontrados['Abstract'].notna().sum()\n        print(f\"üìÑ Artigos com abstract: {abstracts_disponiveis:,}\")\n        \n        if abstracts_disponiveis > 0:\n            print(f\"‚ö†Ô∏è ATEN√á√ÉO: Esta an√°lise usa a API do Gemini\")\n            print(f\"   Limite: {CONFIG['max_requests_per_day']} requisi√ß√µes/dia\")\n            print(f\"   Tempo estimado: {(abstracts_disponiveis * CONFIG['delay_between_requests'])/60:.1f} minutos\")\n            \n            resposta = input(\"\\nRealizar an√°lise detalhada de todos os abstracts? (s/n): \")\n            \n            if resposta.lower() == 's':\n                print(\"\\nüöÄ Iniciando an√°lise detalhada com checkpoint...\")\n                \n                # Usar apenas artigos encontrados para an√°lise\n                df_analisado = processar_com_checkpoint(artigos_encontrados)\n                \n                # Salvar resultado final\n                df_analisado.to_csv(CONFIG['arquivo_saida'], index=False)\n                print(f\"\\nüíæ An√°lise completa salva em: {CONFIG['arquivo_saida']}\")\n                \n                # Estat√≠sticas finais\n                print(\"\\n\" + \"=\"*50)\n                print(\"üìä ESTAT√çSTICAS FINAIS\")\n                print(\"=\"*50)\n                \n                adequados = df_analisado['Se adequa ao escopo?'].str.contains('Sim', case=False, na=False).sum()\n                nao_adequados = df_analisado['Se adequa ao escopo?'].str.contains('N√£o', case=False, na=False).sum()\n                erros = df_analisado['Se adequa ao escopo?'].str.contains('Erro', case=False, na=False).sum()\n                total_analisado = adequados + nao_adequados + erros\n                \n                print(f\"‚úÖ Adequados ao escopo: {adequados:,}\")\n                print(f\"‚ùå N√£o adequados: {nao_adequados:,}\")\n                print(f\"‚ö†Ô∏è Erros: {erros:,}\")\n                print(f\"üìä Total analisado: {total_analisado:,}\")\n                \n                if adequados + nao_adequados > 0:\n                    taxa_adequacao = (adequados / (adequados + nao_adequados)) * 100\n                    print(f\"üìà Taxa de adequa√ß√£o: {taxa_adequacao:.1f}%\")\n                \n                # Mostrar alguns exemplos\n                if adequados > 0:\n                    print(f\"\\nüìë Exemplos de artigos adequados:\")\n                    adequados_df = df_analisado[df_analisado['Se adequa ao escopo?'].str.contains('Sim', case=False, na=False)]\n                    for i, row in adequados_df.head(3).iterrows():\n                        print(f\"\\n{i+1}. {row['Article Title'][:80]}...\")\n                        print(f\"   Adequa√ß√£o: {row['Se adequa ao escopo?']}\")\n            \n            else:\n                print(\"\\n‚è≠Ô∏è An√°lise detalhada pulada\")\n                \n                # Fazer teste com 3 abstracts\n                print(\"\\nüß™ Realizando teste com 3 abstracts...\")\n                amostra_teste = artigos_encontrados.head(3)\n                df_teste = processar_dataframe_escopo(amostra_teste)\n                \n                print(\"\\nüìã Resultados do teste:\")\n                for idx, row in df_teste.iterrows():\n                    print(f\"\\n{idx+1}. {row['Article Title'][:60]}...\")\n                    print(f\"   Adequa√ß√£o: {row['Se adequa ao escopo?']}\")\n        else:\n            print(\"‚ùå Nenhum abstract dispon√≠vel para an√°lise\")\n    else:\n        print(\"‚ùå Coluna 'Abstract' n√£o encontrada\")",
      "block_group": "554fe6e7cd474ea5b608553debd06248",
      "execution_count": null,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "bd6f7c35177049a0a2f878501e039b53",
        "deepnote_cell_type": "markdown"
      },
      "source": "## 7. Conclus√£o\n\n### Funcionalidades implementadas:\n\n1. ‚úÖ **Busca por palavras-chave**: Sistema robusto de filtro com m√∫ltiplas categorias\n2. ‚úÖ **An√°lise com Gemini AI**: An√°lise autom√°tica de relev√¢ncia dos abstracts\n3. ‚úÖ **Rate limiting**: Respeita limites da API (30 req/min, 1.500 req/dia)\n4. ‚úÖ **Sistema de checkpoint**: Permite retomar an√°lises interrompidas\n5. ‚úÖ **Estat√≠sticas detalhadas**: M√©tricas de adequa√ß√£o e performance\n6. ‚úÖ **Exporta√ß√£o de dados**: Salva resultados em formato CSV\n\n### Arquivos gerados:\n- `artigos_nanorevestimentos_encontrados.csv`: Artigos filtrados por palavras-chave\n- `analise_escopo_checkpoint.csv`: Checkpoint do processamento\n- `df_com_analise_escopo_completo.csv`: An√°lise completa com Gemini\n\n### Pr√≥ximos passos:\n- Refinar palavras-chave com base nos resultados\n- Implementar an√°lise de trends temporais\n- Adicionar an√°lise de colabora√ß√µes e redes\n- Desenvolver dashboard interativo",
      "block_group": "b72e5ebeb21345c19e82e19c1d0c9a2b"
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=d35fdc8b-8543-45dc-ae25-3ba609dd01b9' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "deepnote_notebook_id": "848a9951670d45caa2f76f00e5ba2236"
  }
}